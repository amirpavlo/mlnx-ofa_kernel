From: Yevgeny Petrilin <yevgenyp@mellanox.com>
Subject: [PATCH] BACKPORT-mlx5

Change-Id: I033ec1c2f78c597b8fa2cceb07e3fb83dc08fec2
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
---
 drivers/infiniband/hw/mlx5/main.c                  |  20 ++
 drivers/infiniband/hw/mlx5/mr.c                    |  15 +
 drivers/net/ethernet/mellanox/mlx5/core/cmd.c      |  52 +++
 drivers/net/ethernet/mellanox/mlx5/core/en.h       | 109 +++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_arfs.c  |  25 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_clock.c |  52 ++-
 .../net/ethernet/mellanox/mlx5/core/en_dcb_nl.c    |  18 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_diag.c  |   2 +
 .../net/ethernet/mellanox/mlx5/core/en_ethtool.c   | 234 ++++++++++++
 drivers/net/ethernet/mellanox/mlx5/core/en_fs.c    |  48 ++-
 drivers/net/ethernet/mellanox/mlx5/core/en_main.c  | 391 ++++++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_rx.c    |  97 ++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_stats.h |  10 +
 drivers/net/ethernet/mellanox/mlx5/core/en_sysfs.c |   9 +
 drivers/net/ethernet/mellanox/mlx5/core/en_tx.c    |  47 ++-
 drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c  |  20 ++
 drivers/net/ethernet/mellanox/mlx5/core/eswitch.c  |   7 +
 drivers/net/ethernet/mellanox/mlx5/core/eswitch.h  |  22 +-
 drivers/net/ethernet/mellanox/mlx5/core/main.c     |  43 ++-
 .../net/ethernet/mellanox/mlx5/core/mlx5_core.h    |   4 +
 drivers/net/ethernet/mellanox/mlx5/core/mst_dump.c |   3 +-
 drivers/net/ethernet/mellanox/mlx5/core/sriov.c    |  13 +-
 drivers/net/ethernet/mellanox/mlx5/core/uar.c      |   5 +
 include/linux/mlx5/driver.h                        |  16 +-
 24 files changed, 1236 insertions(+), 26 deletions(-)

--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -40,6 +40,7 @@
 #include <linux/sched.h>
 #include <linux/highmem.h>
 #include <linux/spinlock.h>
+#include <linux/etherdevice.h>
 #include <rdma/ib_user_verbs.h>
 #include <rdma/ib_user_verbs_exp.h>
 #include <rdma/ib_verbs_exp.h>
@@ -1023,6 +1024,7 @@ static int get_pg_order(unsigned long of
 	return get_arg(offset);
 }
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 static void  mlx5_ib_vma_open(struct vm_area_struct *area)
 {
 	/* vma_open is called when a new VMA is created on top of our VMA.
@@ -1128,6 +1130,15 @@ static void mlx5_ib_set_vma_data(struct
 
 	list_add(&vma_prv->list, vma_head);
 }
+#else
+static void mlx5_ib_set_vma_data(struct vm_area_struct *vma,
+				 struct mlx5_ib_ucontext *ctx,
+				 struct mlx5_ib_vma_private_data *vma_prv)
+{
+	/* In case vma->vm_ops is not supported just free the vma_prv */
+	kfree(vma_prv);
+}
+#endif /* defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED) */
 
 static inline bool mlx5_writecombine_available(void)
 {
@@ -1392,9 +1403,11 @@ static unsigned long mlx5_ib_get_unmappe
 					       unsigned long flags)
 {
 	struct mm_struct *mm;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 11, 0))
 	struct vm_area_struct *vma;
 	unsigned long start_addr;
 	unsigned long order;
+#endif
 	unsigned long command;
 
 	mm = current->mm;
@@ -1418,6 +1431,7 @@ static unsigned long mlx5_ib_get_unmappe
 		return -EINVAL;
 	}
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 11, 0))
 	order = get_pg_order(pgoff);
 
 	/*
@@ -1442,6 +1456,10 @@ full_search:
 			return addr;
 		addr = ALIGN(vma->vm_end, 1 << order);
 	}
+#else
+	return current->mm->get_unmapped_area(file, addr, len,
+						      pgoff, flags);
+#endif
 }
 
 static int alloc_pa_mkey(struct mlx5_ib_dev *dev, u32 *key, u32 pdn)
@@ -4192,7 +4210,9 @@ static void *mlx5_ib_add(struct mlx5_cor
 	dev->ib_dev.check_mr_status	= mlx5_ib_check_mr_status;
 	dev->ib_dev.alloc_indir_reg_list = mlx5_ib_alloc_indir_reg_list;
 	dev->ib_dev.free_indir_reg_list  = mlx5_ib_free_indir_reg_list;
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	dev->ib_dev.disassociate_ucontext = mlx5_ib_disassociate_ucontext;
+#endif
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	dev->ib_dev.exp_prefetch_mr	= mlx5_ib_prefetch_mr;
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -38,6 +38,9 @@
 #include <linux/delay.h>
 #include <linux/device.h>
 #include <linux/sysfs.h>
+#ifndef ARCH_KMALLOC_MINALIGN
+#include <linux/crypto.h>
+#endif
 #include <rdma/ib_umem.h>
 #include <rdma/ib_umem_odp.h>
 #include <rdma/ib_verbs.h>
@@ -2061,7 +2064,11 @@ static ssize_t order_attr_store(struct k
 	return oa->store(co, oa, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops order_sysfs_ops = {
+#else
+static struct sysfs_ops order_sysfs_ops = {
+#endif
 	.show = order_attr_show,
 	.store = order_attr_store,
 };
@@ -2199,7 +2206,11 @@ static ssize_t cache_attr_store(struct k
 	return ca->store(dev, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops cache_sysfs_ops = {
+#else
+static  struct sysfs_ops cache_sysfs_ops = {
+#endif
 	.show = cache_attr_show,
 	.store = cache_attr_store,
 };
@@ -2304,7 +2315,11 @@ mlx5_ib_alloc_indir_reg_list(struct ib_d
 	}
 
 	dsize = sizeof(*mirl->klms) * max_indir_list_len;
+#ifdef ARCH_KMALLOC_MINALIGN
 	dsize += max_t(int, MLX5_UMR_ALIGN - ARCH_KMALLOC_MINALIGN, 0);
+#else
+	dsize += max_t(int, MLX5_UMR_ALIGN - CRYPTO_MINALIGN, 0);
+#endif
 	mirl->mapped_ilist = kzalloc(dsize, GFP_KERNEL);
 	if (!mirl->mapped_ilist) {
 		err = -ENOMEM;
--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
@@ -864,7 +864,11 @@ static void cmd_work_handler(struct work
 	lay->status_own = CMD_OWNER_HW;
 	set_signature(ent, !cmd->checksum_disabled);
 	dump_command(dev, ent, 1);
+#ifdef HAVE_KTIME_GET_NS
 	ent->ts1 = ktime_get_ns();
+#else
+	ktime_get_ts(&ent->ts1);
+#endif
 
 	if (ent->callback)
 		schedule_delayed_work(&ent->cb_timeout_work, cb_timeout);
@@ -961,6 +965,9 @@ static int mlx5_cmd_invoke(struct mlx5_c
 	struct mlx5_cmd *cmd = &dev->cmd;
 	struct mlx5_cmd_work_ent *ent;
 	struct mlx5_cmd_stats *stats;
+#ifndef HAVE_KTIME_GET_NS
+	ktime_t t1, t2, delta;
+#endif
 	int err = 0;
 	s64 ds;
 	u16 op;
@@ -993,7 +1000,14 @@ static int mlx5_cmd_invoke(struct mlx5_c
 	if (err == -ETIMEDOUT)
 		goto out_free;
 
+#ifdef HAVE_KTIME_GET_NS
 	ds = ent->ts2 - ent->ts1;
+#else
+	t1 = timespec_to_ktime(ent->ts1);
+	t2 = timespec_to_ktime(ent->ts2);
+	delta = ktime_sub(t2, t1);
+	ds = ktime_to_ns(delta);
+#endif
 	op = be16_to_cpu(((struct mlx5_inbox_hdr *)in->first.data)->opcode);
 	if (op < ARRAY_SIZE(cmd->stats)) {
 		stats = &cmd->stats[op];
@@ -1443,6 +1457,9 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 	void *context;
 	int err;
 	int i;
+#ifndef HAVE_KTIME_GET_NS
+	ktime_t t1, t2, delta;
+#endif
 	s64 ds;
 	struct mlx5_cmd_stats *stats;
 	unsigned long flags;
@@ -1456,12 +1473,20 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 
 			ent = cmd->ent_arr[i];
 			if (ent->callback)
+#ifdef HAVE___CANCEL_DELAYED_WORK
+				__cancel_delayed_work(&ent->cb_timeout_work);
+#else
 				cancel_delayed_work(&ent->cb_timeout_work);
+#endif
 			if (ent->page_queue)
 				sem = &cmd->pages_sem;
 			else
 				sem = &cmd->sem;
+#ifdef HAVE_KTIME_GET_NS
 			ent->ts2 = ktime_get_ns();
+#else
+			ktime_get_ts(&ent->ts2);
+#endif
 			memcpy(ent->out->first.data, ent->lay->out, sizeof(ent->lay->out));
 			dump_command(dev, ent, 0);
 			if (!ent->ret) {
@@ -1480,7 +1505,14 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 			free_ent(cmd, ent->idx);
 
 			if (ent->callback) {
+#ifdef HAVE_KTIME_GET_NS
 				ds = ent->ts2 - ent->ts1;
+#else
+				t1 = timespec_to_ktime(ent->ts1);
+				t2 = timespec_to_ktime(ent->ts2);
+				delta = ktime_sub(t2, t1);
+				ds = ktime_to_ns(delta);
+#endif
 				if (ent->op < ARRAY_SIZE(cmd->stats)) {
 					stats = &cmd->stats[ent->op];
 					spin_lock_irqsave(&stats->lock, flags);
@@ -2028,7 +2060,11 @@ static ssize_t num_ent_store(struct mlx5
 	u32 var;
 	int i;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	spin_lock_irqsave(&ch->lock, flags);
@@ -2086,7 +2122,11 @@ static ssize_t miss_store(struct mlx5_cm
 	unsigned long flags;
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var) {
@@ -2122,7 +2162,11 @@ static ssize_t total_commands_store(stru
 	unsigned long flags;
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var) {
@@ -2181,7 +2225,11 @@ static ssize_t real_miss_store(struct de
 	struct mlx5_core_dev *cdev = pci_get_drvdata(pdev);
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var) {
@@ -2194,7 +2242,11 @@ static ssize_t real_miss_store(struct de
 	return count;
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops cmd_cache_sysfs_ops = {
+#else
+static struct sysfs_ops cmd_cache_sysfs_ops = {
+#endif
 	.show = cmd_cache_attr_show,
 	.store = cmd_cache_attr_store,
 };
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -34,15 +34,29 @@
 
 #include <linux/if_vlan.h>
 #include <linux/etherdevice.h>
+#ifdef HAVE_TIMECOUNTER_H
 #include <linux/timecounter.h>
+#else
+#include <linux/clocksource.h>
+#endif
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 #include <linux/ptp_clock_kernel.h>
+#endif
 #include <linux/net_tstamp.h>
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 #include <linux/hashtable.h>
+#endif
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/device.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
 #include <linux/mlx5/transobj.h>
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#include <linux/inet_lro.h>
+#else
+#include <net/ip.h>
+#endif
+
 #include "linux/mlx5/vport.h"
 #include "wq.h"
 #include "mlx5_core.h"
@@ -100,7 +114,11 @@
 #define MLX5E_INDIR_RQT_SIZE           BIT(MLX5E_LOG_INDIR_RQT_SIZE)
 #define MLX5E_MAX_NUM_CHANNELS         (MLX5E_INDIR_RQT_SIZE >> 1)
 #define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 #define MLX5E_MAX_RL_QUEUES            2048
+#else
+#define MLX5E_MAX_RL_QUEUES            0
+#endif
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_BF_BUDGET             16
@@ -118,6 +136,11 @@ do {
 
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+/* Minimum packet number till arming the CQ */
+#define MLX5_EN_MIN_RX_ARM     2097152
+#endif
+
 enum {
 	MLX5E_LINK_SPEED,
 	MLX5E_LINK_STATE,
@@ -241,6 +264,7 @@ struct mlx5e_cq {
 	struct mlx5_wq_ctrl        wq_ctrl;
 } ____cacheline_aligned_in_smp;
 
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
 	/* to be added in future commits */
 	"hw_lro",
@@ -249,6 +273,21 @@ static const char mlx5e_priv_flags[][ETH
 	"qos_with_dcbx_by_fw",
 	"rx_cqe_moder",
 };
+#endif
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#define IS_HW_LRO(priv) \
+	(priv->params.lro_en && (priv->pflags & MLX5E_PRIV_FLAG_HWLRO))
+#define IS_SW_LRO(priv) \
+	(priv->params.lro_en && !(priv->pflags & MLX5E_PRIV_FLAG_HWLRO))
+
+/* SW LRO defines for MLX5 */
+#define MLX5E_LRO_MAX_DESC	32
+struct mlx5e_sw_lro {
+	struct net_lro_mgr	lro_mgr;
+	struct net_lro_desc	lro_desc[MLX5E_LRO_MAX_DESC];
+};
+#endif
 
 struct mlx5e_rx_am_stats {
 	int ppms; /* packets per msec */
@@ -299,6 +338,9 @@ struct mlx5e_rq {
 	int                    ix;
 
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_sw_lro sw_lro;
+#endif
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
@@ -413,7 +455,9 @@ struct mlx5e_channel {
 
 	/* data path - accessed per napi poll */
 	struct irq_desc           *irq_desc;
-
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	u32 tot_rx;
+#endif
 	/* control */
 	struct mlx5e_priv         *priv;
 	int                        ix;
@@ -675,12 +719,18 @@ struct mlx5e_ecn_enable_ctx {
 
 #define MLX5E_NIC_DEFAULT_PRIO	0
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#define MLX5E_PRIV_FLAG_HWLRO (1<<0)
+#endif
+
 struct mlx5e_tstamp {
 	rwlock_t                   lock;
 	struct cyclecounter        cycles;
 	struct timecounter         clock;
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	struct ptp_clock          *ptp;
 	struct ptp_clock_info      ptp_info;
+#endif
 	struct hwtstamp_config     hwtstamp_config;
 	u32                        nominal_c_mult;
 	unsigned long              last_overflow_check;
@@ -692,6 +742,11 @@ struct mlx5e_direct_tir {
 	u32              rqtn;
 };
 
+#ifdef HAVE_IEEE_DCBNL_ETS
+/* CEE DCBX std supported values */
+#define CEE_DCBX_MAX_PGS   8
+#define CEE_DCBX_MAX_PRIO  8
+
 struct mlx5e_cee_config {
 	/* bw pct for priority group */
 	u8                         pg_bw_pct[CEE_DCBX_MAX_PGS];
@@ -707,13 +762,19 @@ struct mlx5e_dcbx {
 	/* The only setting that cannot be read from FW */
 	u8                         tc_tsa[IEEE_8021QAZ_MAX_TCS];
 };
+#endif
 
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	int                        default_vlan_prio;
 	struct mlx5e_sq            **txq_to_sq_map;
 	int tc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	DECLARE_HASHTABLE(flow_map_hash, ilog2(MLX5E_MAX_RL_QUEUES));
+#endif
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+	struct vlan_group          *vlan_grp;
+#endif
 	/* priv data path fields - end */
 
 	unsigned long              state;
@@ -748,17 +809,26 @@ struct mlx5e_priv {
 	struct delayed_work        update_stats_work;
 	struct delayed_work        service_task;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 	struct mlx5e_dcbx          dcbx;
+#endif
 
 	struct mlx5_core_dev      *mdev;
+#ifdef HAVE_GET_SET_DUMP
 	struct {
 		__u32 flag;
 		u32 mst_size;
 	} dump;
+#endif
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
 	struct mlx5e_tstamp        tstamp;
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	u32                        pflags;
+#endif
+#ifndef HAVE_NDO_GET_STATS64
+	struct net_device_stats    netdev_stats;
+#endif
 	u16                        counter_set_id;
 
 	struct dentry *dfs_root;
@@ -810,8 +880,17 @@ static inline int mlx5_max_log_rq_size(i
 }
 
 void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw);
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 		       void *accel_priv, select_queue_fallback_t fallback);
+#else
+		       void *accel_priv);
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb);
+#endif
+
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
@@ -835,6 +914,7 @@ void free_rq_res(struct mlx5e_rq *rq);
 void free_striding_rq_res(struct mlx5e_rq *rq);
 int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
 void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
+#ifdef HAVE_SPLIT_PAGE_EXPORTED
 int mlx5e_alloc_striding_rx_wqe(struct mlx5e_rq *rq,
 				struct mlx5e_rx_wqe *wqe, u16 ix);
 void mlx5e_post_rx_fragmented_mpwqe(struct mlx5e_rq *rq);
@@ -853,6 +933,7 @@ void mlx5e_free_rx_linear_mpwqe(struct m
 void mlx5e_free_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
 				    struct mlx5e_rx_wqe_info *wi);
 void mlx5e_dealloc_striding_rx_wqe(struct mlx5e_rq *rq, u16 ix);
+#endif
 
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 void mlx5e_free_rx_descs(struct mlx5e_rq *rq);
@@ -879,10 +960,22 @@ void mlx5e_ptp_overflow_check(struct mlx
 void mlx5e_ptp_init(struct mlx5e_priv *priv);
 void mlx5e_ptp_cleanup(struct mlx5e_priv *priv);
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid);
+#else
+void mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid);
+#endif
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid);
+#else
+void mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid);
+#endif
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 int mlx5e_add_all_vlan_rules(struct mlx5e_priv *priv);
@@ -968,10 +1061,12 @@ static inline int mlx5e_max_num_channels
 
 static inline enum rq_type mlx5e_get_rq_type_cap(struct mlx5_core_dev *mdev)
 {
+#ifdef HAVE_SPLIT_PAGE_EXPORTED
 	if (MLX5_CAP_GEN(mdev, striding_rq) &&
 	    MLX5_CAP_GEN(mdev, umr_ptr_rlky) &&
 	    MLX5_CAP_ETH(mdev, reg_umr_sq))
 		return RQ_TYPE_STRIDE;
+#endif
 	return RQ_TYPE_NONE;
 }
 
@@ -981,12 +1076,22 @@ static inline u16 mlx5e_get_mtt_octw(u16
 }
 
 extern const struct ethtool_ops mlx5e_ethtool_ops;
+#ifdef HAVE_ETHTOOL_OPS_EXT
+extern const struct ethtool_ops_ext mlx5e_ethtool_ops_ext;
+#endif
+
+#ifdef HAVE_IEEE_DCBNL_ETS
+#ifdef CONFIG_COMPAT_IS_DCBNL_OPS_CONST
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
+#else
+extern struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
+#endif
 int mlx5e_dcbnl_set_dcbx_mode(struct mlx5e_priv *priv,
 			      enum mlx5_dcbx_oper_mode mode);
 void mlx5e_dcbnl_query_dcbx_mode(struct mlx5e_priv *priv,
 				 enum mlx5_dcbx_oper_mode *mode);
 void mlx5e_dcbnl_initialize(struct net_device *netdev);
+#endif
 
 #ifndef CONFIG_RFS_ACCEL
 static inline int mlx5e_arfs_create_tables(struct mlx5e_priv *priv)
@@ -1012,9 +1117,11 @@ int mlx5e_rx_flow_steer(struct net_devic
 			u16 rxq_index, u32 flow_id);
 #endif
 
+#ifdef HAVE_GET_SET_DUMP
 int mlx5e_get_dump_flag(struct net_device *netdev, struct ethtool_dump *dump);
 int mlx5e_get_dump_data(struct net_device *netdev, struct ethtool_dump *dump,
 			void *buffer);
 int mlx5e_set_dump(struct net_device *dev, struct ethtool_dump *dump);
+#endif
 
 #endif /* __MLX5_EN_H__ */
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_arfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_arfs.c
@@ -70,7 +70,7 @@ struct arfs_rule {
 
 #define mlx5e_for_each_hash_arfs_rule(hn, tmp, hash, j) \
 	for (j = 0; j < ARFS_HASH_SIZE; j++) \
-		hlist_for_each_entry_safe(hn, tmp, &hash[j], hlist)
+		compat_hlist_for_each_entry_safe(hn, tmp, &hash[j], hlist)
 
 static enum mlx5e_traffic_types arfs_get_tt(enum arfs_type type)
 {
@@ -156,8 +156,10 @@ void mlx5e_arfs_destroy_tables(struct ml
 {
 	int i;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	if (!(priv->netdev->hw_features & NETIF_F_NTUPLE))
 		return;
+#endif
 
 	arfs_del_rules(priv);
 	destroy_workqueue(priv->fs.arfs.wq);
@@ -321,8 +323,10 @@ void mlx5e_arfs_shutdown_tables(struct m
 {
 	int i;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	if (!(priv->netdev->hw_features & NETIF_F_NTUPLE))
 		return;
+#endif
 
 	for (i = 0; i < ARFS_NUM_TYPES; i++)
 		if (priv->fs.arfs.arfs_tables[i].default_rule)
@@ -334,8 +338,10 @@ int mlx5e_arfs_init_tables(struct mlx5e_
 	int err;
 	int i;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	if (!(priv->netdev->hw_features & NETIF_F_NTUPLE))
 		return 0;
+#endif
 
 	for (i = 0; i < ARFS_NUM_TYPES; i++) {
 		err = arfs_add_default_rule(priv, i);
@@ -380,8 +386,10 @@ int mlx5e_arfs_create_tables(struct mlx5
 	int err = 0;
 	int i;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	if (!(priv->netdev->hw_features & NETIF_F_NTUPLE))
 		return 0;
+#endif
 
 	spin_lock_init(&priv->fs.arfs.arfs_lock);
 	INIT_LIST_HEAD(&priv->fs.arfs.rules);
@@ -409,6 +417,9 @@ static void arfs_may_expire_flow(struct
 	int quota = 0;
 	int i;
 	int j;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	HLIST_HEAD(del_list);
 	spin_lock_bh(&priv->fs.arfs.arfs_lock);
@@ -424,7 +435,7 @@ static void arfs_may_expire_flow(struct
 		}
 	}
 	spin_unlock_bh(&priv->fs.arfs.arfs_lock);
-	hlist_for_each_entry_safe(arfs_rule, htmp, &del_list, hlist) {
+	compat_hlist_for_each_entry_safe(arfs_rule, htmp, &del_list, hlist) {
 		if (arfs_rule->rule)
 			mlx5_del_flow_rule(arfs_rule->rule);
 		hlist_del(&arfs_rule->hlist);
@@ -438,6 +449,9 @@ static void arfs_del_rules(struct mlx5e_
 	struct arfs_rule *rule;
 	int i;
 	int j;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	HLIST_HEAD(del_list);
 	spin_lock_bh(&priv->fs.arfs.arfs_lock);
@@ -447,7 +461,7 @@ static void arfs_del_rules(struct mlx5e_
 	}
 	spin_unlock_bh(&priv->fs.arfs.arfs_lock);
 
-	hlist_for_each_entry_safe(rule, htmp, &del_list, hlist) {
+	compat_hlist_for_each_entry_safe(rule, htmp, &del_list, hlist) {
 		cancel_work_sync(&rule->arfs_work);
 		if (rule->rule)
 			mlx5_del_flow_rule(rule->rule);
@@ -713,9 +727,12 @@ static struct arfs_rule *arfs_find_rule(
 	struct hlist_head *head;
 	__be16 src_port = arfs_get_src_port(skb);
 	__be16 dst_port = arfs_get_dst_port(skb);
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	head = arfs_hash_bucket(arfs_t, src_port, dst_port);
-	hlist_for_each_entry(arfs_rule, head, hlist) {
+	compat_hlist_for_each_entry(arfs_rule, head, hlist) {
 		if (arfs_rule->tuple.src_port == src_port &&
 		    arfs_rule->tuple.dst_port == dst_port &&
 		    arfs_cmp_ips(&arfs_rule->tuple, skb)) {
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
@@ -41,6 +41,7 @@ void mlx5e_fill_hwstamp(struct mlx5e_tst
 			struct skb_shared_hwtstamps *hwts,
 			u64 timestamp)
 {
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	unsigned long flags;
 	u64 nsec;
 
@@ -53,6 +54,9 @@ void mlx5e_fill_hwstamp(struct mlx5e_tst
 	read_unlock_irqrestore(&tstamp->lock, flags);
 
 	hwts->hwtstamp = ns_to_ktime(nsec);
+#else
+	memset(hwts, 0, sizeof(struct skb_shared_hwtstamps));
+#endif
 }
 
 static cycle_t mlx5e_read_clock(const struct cyclecounter *cc)
@@ -80,12 +84,22 @@ void mlx5e_ptp_overflow_check(struct mlx
 	}
 }
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
+
 static int mlx5e_ptp_settime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			     const struct timespec *ts)
+#else
 			     const struct timespec64 *ts)
+#endif
 {
 	struct mlx5e_tstamp *tstamp = container_of(ptp, struct mlx5e_tstamp,
 						   ptp_info);
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	u64 ns = timespec_to_ns(ts);
+#else
 	u64 ns = timespec64_to_ns(ts);
+#endif
 	unsigned long flags;
 
 	write_lock_irqsave(&tstamp->lock, flags);
@@ -96,18 +110,31 @@ static int mlx5e_ptp_settime(struct ptp_
 }
 
 static int mlx5e_ptp_gettime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			     struct timespec *ts)
+#else
 			     struct timespec64 *ts)
+#endif
 {
 	struct mlx5e_tstamp *tstamp = container_of(ptp, struct mlx5e_tstamp,
 						   ptp_info);
+
 	u64 ns;
+#ifndef HAVE_NS_TO_TIMESPACE64
+	u32 remainder;
+#endif
 	unsigned long flags;
 
 	write_lock_irqsave(&tstamp->lock, flags);
 	ns = timecounter_read(&tstamp->clock);
 	write_unlock_irqrestore(&tstamp->lock, flags);
 
+#ifdef HAVE_NS_TO_TIMESPACE64
 	*ts = ns_to_timespec64(ns);
+#else
+	ts->tv_sec = div_u64_rem(ns, NSEC_PER_SEC, &remainder);
+	ts->tv_nsec = remainder;
+#endif
 
 	return 0;
 }
@@ -158,15 +185,22 @@ static const struct ptp_clock_info mlx5e
 	.n_alarm	= 0,
 	.n_ext_ts	= 0,
 	.n_per_out	= 0,
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	.n_pins		= 0,
+#endif
 	.pps		= 0,
 	.adjfreq	= mlx5e_ptp_adjfreq,
 	.adjtime	= mlx5e_ptp_adjtime,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	.gettime	= mlx5e_ptp_gettime,
+	.settime	= mlx5e_ptp_settime,
+#else
 	.gettime64	= mlx5e_ptp_gettime,
 	.settime64	= mlx5e_ptp_settime,
+#endif
 	.enable		= NULL,
 };
-
+#endif
 
 static void mlx5e_ptp_init_config(struct mlx5e_tstamp *tstamp)
 {
@@ -180,7 +214,11 @@ void mlx5e_ptp_init(struct mlx5e_priv *p
 	struct net_device *netdev = priv->netdev;
 	struct mlx5e_tstamp *tstamp = &priv->tstamp;
 	unsigned long flags;
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
 	u64 ns, zero = 0;
+#else
+	u64 ns;
+#endif
 
 	rwlock_init(&tstamp->lock);
 	mlx5e_ptp_init_config(tstamp);
@@ -207,11 +245,16 @@ void mlx5e_ptp_init(struct mlx5e_priv *p
 	/* Calculate period in seconds to call the overflow watchdog - to make
 	 * sure counter is checked at least once every wrap around.
 	 */
-	ns = cyclecounter_cyc2ns(&tstamp->cycles, tstamp->cycles.mask, zero,
-				 &zero);
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
+	ns = cyclecounter_cyc2ns(&tstamp->cycles, tstamp->cycles.mask,
+				 zero, &zero);
+#else
+	ns = cyclecounter_cyc2ns(&tstamp->cycles, tstamp->cycles.mask);
+#endif
 	do_div(ns, NSEC_PER_SEC / 2 / HZ);
 	tstamp->overflow_period = ns;
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	/* Configure the PHC */
 	tstamp->ptp_info = mlx5e_ptp_clock_info;
 	snprintf(tstamp->ptp_info.name, 16, "mlx5 ptp");
@@ -223,12 +266,15 @@ void mlx5e_ptp_init(struct mlx5e_priv *p
 		netdev_err(netdev, "%s: ptp_clock_register failed\n",
 			   __func__);
 	}
+#endif
 }
 
 void mlx5e_ptp_cleanup(struct mlx5e_priv *priv)
 {
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	if (priv->tstamp.ptp) {
 		ptp_clock_unregister(priv->tstamp.ptp);
 		priv->tstamp.ptp = NULL;
 	}
+#endif
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_dcb_nl.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_dcb_nl.c
@@ -41,6 +41,7 @@
 #define MLX5E_CEE_STATE_UP    1
 #define MLX5E_CEE_STATE_DOWN  0
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 /* If dcbx mode is non-host and qos_with_dcbx_by_fw is off, set the
  * dcbx mode to host.
  */
@@ -191,7 +192,11 @@ static int mlx5e_dcbnl_ieee_setets_core(
 	u8 tc_tx_bw[IEEE_8021QAZ_MAX_TCS] = { 0 };
 	u8 tc_group[IEEE_8021QAZ_MAX_TCS] = { 0 };
 	int max_tc = mlx5_max_tc(mdev);
+#ifdef HAVE_NETDEV_SET_PRIO_TC_MAP
 	int err, prio;
+#else
+	int err;
+#endif
 
 	if (!MLX5_CAP_GEN(priv->mdev, ets))
 		return -ENOTSUPP;
@@ -205,9 +210,10 @@ static int mlx5e_dcbnl_ieee_setets_core(
 	if (err)
 		return err;
 
+#ifdef HAVE_NETDEV_SET_PRIO_TC_MAP
 	for (prio = 0; prio < MLX5E_MAX_PRIORITY; prio++)
 		netdev_set_prio_tc_map(priv->netdev, prio, ets->prio_tc[prio]);
-
+#endif
 	/* higher TC means higher priority => higher TCG */
 	mlx5e_build_tc_group(ets, tc_group, max_tc);
 	mlx5e_build_tc_tx_bw(ets, tc_tx_bw, tc_group, max_tc);
@@ -631,8 +637,13 @@ static u8 mlx5e_dcbnl_getcap(struct net_
 	return rval;
 }
 
+#ifdef HAVE_DCBNL_RTNL_OPS_GETNUMTCS_RET_INT
 static int mlx5e_dcbnl_getnumtcs(struct net_device *netdev,
 				 int tcs_id, u8 *num)
+#else
+static u8 mlx5e_dcbnl_getnumtcs(struct net_device *netdev,
+				int tcs_id, u8 *num)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct mlx5_core_dev *mdev = priv->mdev;
@@ -679,7 +690,11 @@ static void mlx5e_dcbnl_setpfcstate(stru
 	cee_cfg->pfc_enable = state;
 }
 
+#ifdef CONFIG_COMPAT_IS_DCBNL_OPS_CONST
 const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops = {
+#else
+struct dcbnl_rtnl_ops mlx5e_dcbnl_ops = {
+#endif
 	.ieee_getets	= mlx5e_dcbnl_ieee_getets,
 	.ieee_setets	= mlx5e_dcbnl_ieee_setets,
 #ifdef HAVE_IEEE_GET_SET_MAXRATE
@@ -780,3 +795,4 @@ void mlx5e_dcbnl_initialize(struct net_d
 
 	mlx5e_ets_init(netdev);
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_diag.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_diag.c
@@ -39,6 +39,7 @@
 #define DIAG_GET_NEXT_BLK(dump_hdr) \
 	((struct mlx5_diag_blk *)(dump_hdr->dump + dump_hdr->total_length))
 
+#ifdef HAVE_GET_SET_DUMP
 static int mlx5e_diag_fill_device_name(struct mlx5e_priv *priv, void *buff)
 {
 	struct mlx5_core_dev *mdev = priv->mdev;
@@ -294,3 +295,4 @@ int mlx5e_get_dump_data(struct net_devic
 
 	return 0;
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
@@ -240,8 +240,10 @@ static int mlx5e_get_sset_count(struct n
 		       MLX5E_NUM_PFC_COUNTERS(priv);
 	case ETH_SS_TEST:
 		return MLX5E_NUM_SELF_TEST;
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	case ETH_SS_PRIV_FLAGS:
 		return ARRAY_SIZE(mlx5e_priv_flags);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -339,11 +341,13 @@ static void mlx5e_get_strings(struct net
 	int i;
 
 	switch (stringset) {
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	case ETH_SS_PRIV_FLAGS:
 		for (i = 0; i < ARRAY_SIZE(mlx5e_priv_flags); i++)
 			strcpy(data + i * ETH_GSTRING_LEN,
 			       mlx5e_priv_flags[i]);
 		break;
+#endif
 
 	case ETH_SS_TEST:
 		for (i = 0; i < MLX5E_NUM_SELF_TEST; i++)
@@ -578,6 +582,7 @@ static int mlx5e_set_ringparam(struct ne
 	return err;
 }
 
+#if defined(HAVE_GET_SET_CHANNELS) || defined(HAVE_GET_SET_CHANNELS_EXT)
 static void mlx5e_get_channels(struct net_device *dev,
 			       struct ethtool_channels *ch)
 {
@@ -586,8 +591,10 @@ static void mlx5e_get_channels(struct ne
 
 	ch->max_combined   = mlx5e_max_num_channels(ncv);
 	ch->combined_count = priv->params.num_channels;
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	ch->max_other      = MLX5E_MAX_RL_QUEUES;
 	ch->other_count    = priv->params.num_rl_txqs;
+#endif
 }
 
 static int mlx5e_set_channels(struct net_device *dev,
@@ -623,7 +630,9 @@ static int mlx5e_set_channels(struct net
 	mutex_lock(&priv->state_lock);
 	new_params = priv->params;
 	new_params.num_channels = count;
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	new_params.num_rl_txqs = rl_count;
+#endif
 	mlx5e_build_default_indir_rqt(priv->mdev, new_params.indirection_rqt,
 				      MLX5E_INDIR_RQT_SIZE, count);
 	err = mlx5e_update_priv_params(priv, &new_params);
@@ -631,6 +640,7 @@ static int mlx5e_set_channels(struct net
 
 	return err;
 }
+#endif
 
 static int mlx5e_get_coalesce(struct net_device *netdev,
 			      struct ethtool_coalesce *coal)
@@ -1027,10 +1037,13 @@ static void mlx5e_get_pauseparam(struct
 	}
 }
 
+#if defined(HAVE_GET_TS_INFO) || defined(HAVE_GET_TS_INFO_EXT)
 static int mlx5e_get_ts_info(struct net_device *dev,
 			     struct ethtool_ts_info *info)
 {
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	struct mlx5e_priv *priv = netdev_priv(dev);
+#endif
 	int ret;
 
 	ret = ethtool_op_get_ts_info(dev, info);
@@ -1050,11 +1063,14 @@ static int mlx5e_get_ts_info(struct net_
 			(1 << HWTSTAMP_FILTER_NONE) |
 			(1 << HWTSTAMP_FILTER_ALL);
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	if (priv->tstamp.ptp)
 		info->phc_index = ptp_clock_index(priv->tstamp.ptp);
+#endif
 
 	return ret;
 }
+#endif
 
 static void mlx5e_fill_wol_supported(struct mlx5_core_dev *mdev,
 				     struct ethtool_wolinfo *wol)
@@ -1177,6 +1193,7 @@ static void mlx5e_set_msglevel(struct ne
 	((struct mlx5e_priv *)netdev_priv(dev))->msg_level = val;
 }
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 static void qos_with_dcbx_by_fw_handler(struct net_device *netdev,
 					u32 wanted_flags)
 {
@@ -1223,6 +1240,7 @@ static void dcbx_handle_by_fw_handler(st
 
 	priv->pflags ^= MLX5E_PRIV_FLAGS_DCBX_HANDLE_BY_FW;
 }
+#endif
 
 static int rx_cqe_based_moder_handler(struct mlx5e_priv *priv,
 				      struct mlx5e_params *new_params,
@@ -1251,6 +1269,7 @@ static int rx_cqe_based_moder_handler(st
 	return 0;
 }
 
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 static int mlx5e_set_priv_flags(struct net_device *dev, u32 flags)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -1262,6 +1281,16 @@ static int mlx5e_set_priv_flags(struct n
 	mutex_lock(&priv->state_lock);
 	new_params = priv->params;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (changes & MLX5E_PRIV_FLAG_HWLRO) {
+		priv->pflags ^= MLX5E_PRIV_FLAG_HWLRO;
+		if (!test_bit(MLX5E_STATE_OPENED, &priv->state))
+			goto out;
+		if (priv->params.lro_en)
+			update_params = true;
+	}
+#endif
+
 	if (changes & MLX5E_PRIV_FLAGS_SNIFFER_EN) {
 		priv->pflags ^= MLX5E_PRIV_FLAGS_SNIFFER_EN;
 		if (priv->pflags & MLX5E_PRIV_FLAGS_SNIFFER_EN) {
@@ -1273,11 +1302,13 @@ static int mlx5e_set_priv_flags(struct n
 		}
 	}
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 	if (changes & MLX5E_PRIV_FLAGS_QOS_WITH_DCBX_BY_FW)
 		qos_with_dcbx_by_fw_handler(dev, flags);
 
 	if (changes & MLX5E_PRIV_FLAGS_DCBX_HANDLE_BY_FW)
 		dcbx_handle_by_fw_handler(dev, flags);
+#endif
 
 	if (changes & MLX5E_PRIV_FLAGS_RX_CQE_BASED_MODER) {
 		err = rx_cqe_based_moder_handler(priv, &new_params, flags);
@@ -1293,6 +1324,9 @@ static int mlx5e_set_priv_flags(struct n
 	if (update_params)
 		err = mlx5e_update_priv_params(priv, &new_params);
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+out:
+#endif
 	mutex_unlock(&priv->state_lock);
 	return err ? : !(flags == priv->pflags);
 }
@@ -1303,7 +1337,96 @@ static u32 mlx5e_get_priv_flags(struct n
 
 	return priv->pflags;
 }
+#endif
+
+#ifdef LEGACY_ETHTOOL_OPS
+#ifdef HAVE_GET_SET_FLAGS
+static int mlx5e_set_flags(struct net_device *dev, u32 data)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+	struct mlx5e_params new_params;
+	u32 changes = data ^ dev->features;
+	bool update_params = false;
+
+	mutex_lock(&priv->state_lock);
+
+	new_params = priv->params;
 
+	if (changes & ETH_FLAG_LRO) {
+		new_params.lro_en = !new_params.lro_en;
+		update_params = true;
+	}
+
+	if (!update_params)
+		goto out;
+
+	mlx5e_update_priv_params(priv, &new_params);
+
+	if (priv->params.lro_en)
+		dev->features |= NETIF_F_LRO;
+	else
+		dev->features &= ~NETIF_F_LRO;
+
+out:
+	if (changes & ETH_FLAG_RXVLAN) {
+		if (test_bit(MLX5E_STATE_OPENED, &priv->state))
+			mlx5e_modify_rqs_vsd(priv, data & ETH_FLAG_RXVLAN ?
+					     0 : 1);
+		dev->features ^= NETIF_F_HW_VLAN_CTAG_RX;
+	}
+
+	if (changes & ETH_FLAG_TXVLAN)
+		dev->features ^= NETIF_F_HW_VLAN_CTAG_TX;
+
+	mutex_unlock(&priv->state_lock);
+	return 0;
+}
+
+static u32 mlx5e_get_flags(struct net_device *dev)
+{
+	return ethtool_op_get_flags(dev) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_RX) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_TX);
+}
+#endif
+
+#ifdef HAVE_GET_SET_TSO
+static u32 mlx5e_get_tso(struct net_device *dev)
+{
+       return (dev->features & NETIF_F_TSO) != 0;
+}
+
+static int mlx5e_set_tso(struct net_device *dev, u32 data)
+{
+       if (data)
+               dev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+       else
+               dev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+       return 0;
+}
+#endif
+
+#ifdef LEGACY_ETHTOOL_OPS
+#ifdef HAVE_GET_SET_RX_CSUM
+static u32 mlx5e_get_rx_csum(struct net_device *dev)
+{
+       return dev->features & NETIF_F_RXCSUM;
+}
+
+static int mlx5e_set_rx_csum(struct net_device *dev, u32 data)
+{
+       if (!data) {
+               dev->features &= ~NETIF_F_RXCSUM;
+               return 0;
+       }
+       dev->features |= NETIF_F_RXCSUM;
+       return 0;
+}
+#endif
+#endif
+#endif
+
+#if defined(HAVE_SET_PHYS_ID) || defined(HAVE_SET_PHYS_ID_EXT)
 static int mlx5e_set_phys_id(struct net_device *dev,
 			     enum ethtool_phys_id_state state)
 {
@@ -1330,21 +1453,36 @@ static int mlx5e_set_phys_id(struct net_
 	err = mlx5_set_port_beacon(mdev, out, sizeof(out), beacon_duration);
 	return err;
 }
+#endif
 
+#if defined(HAVE_RXFH_INDIR_SIZE) || defined(HAVE_RXFH_INDIR_SIZE_EXT)
 static u32 mlx5e_get_rxfh_indir_size(struct net_device *netdev)
 {
 	return MLX5E_INDIR_RQT_SIZE;
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static u32 mlx5e_get_rxfh_key_size(struct net_device *netdev)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 
 	return sizeof(priv->params.toeplitz_hash_key);
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static int mlx5e_get_rxfh(struct net_device *netdev, u32 *indir,
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 			  u8 *key, u8 *hfunc)
+#else
+			  u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx5e_get_rxfh_indir(struct net_device *netdev, u32 *indir)
+#endif
+#if defined(HAVE_GET_SET_RXFH) || defined(HAVE_GET_SET_RXFH_INDIR) || \
+				  defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 
@@ -1352,27 +1490,44 @@ static int mlx5e_get_rxfh(struct net_dev
 		memcpy(indir, priv->params.indirection_rqt,
 		       sizeof(priv->params.indirection_rqt));
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key)
 		memcpy(key, priv->params.toeplitz_hash_key,
 		       sizeof(priv->params.toeplitz_hash_key));
 
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (hfunc)
 		*hfunc = priv->params.rss_hfunc;
+#endif
+#endif
 
 	return 0;
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static int mlx5e_set_rxfh(struct net_device *netdev, const u32 *indir,
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 			  const u8 *key, const u8 hfunc)
+#else
+			  const u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx5e_set_rxfh_indir(struct net_device *netdev, const u32 *indir)
+#endif
+#if defined(HAVE_GET_SET_RXFH) || defined(HAVE_GET_SET_RXFH_INDIR) || \
+				  defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct mlx5e_params new_params;
 	int err = 0;
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	if ((hfunc != ETH_RSS_HASH_NO_CHANGE) &&
 	    (hfunc != ETH_RSS_HASH_XOR) &&
 	    (hfunc != ETH_RSS_HASH_TOP))
 		return -EINVAL;
+#endif
 
 	mutex_lock(&priv->state_lock);
 
@@ -1382,12 +1537,16 @@ static int mlx5e_set_rxfh(struct net_dev
 		memcpy(new_params.indirection_rqt, indir,
 		       sizeof(new_params.indirection_rqt));
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key)
 		memcpy(new_params.toeplitz_hash_key, key,
 		       sizeof(new_params.toeplitz_hash_key));
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	if (hfunc != ETH_RSS_HASH_NO_CHANGE)
 		new_params.rss_hfunc = hfunc;
+#endif
 
 	err = mlx5e_update_priv_params(priv, &new_params);
 
@@ -1395,9 +1554,14 @@ static int mlx5e_set_rxfh(struct net_dev
 
 	return err;
 }
+#endif
 
 static int mlx5e_get_rxnfc(struct net_device *netdev,
+#ifdef HAVE_ETHTOOL_OPS_GET_RXNFC_U32_RULE_LOCS
 			   struct ethtool_rxnfc *info, u32 *rule_locs)
+#else
+			   struct ethtool_rxnfc *info, void *rule_locs)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	int err = 0;
@@ -1414,6 +1578,7 @@ static int mlx5e_get_rxnfc(struct net_de
 	return err;
 }
 
+#if defined(HAVE_GET_MODULE_EEPROM) || defined(HAVE_GET_MODULE_EEPROM_EXT)
 static int mlx5e_get_module_info(struct net_device *netdev,
 				 struct ethtool_modinfo *modinfo)
 {
@@ -1491,6 +1656,7 @@ static int mlx5e_get_module_eeprom(struc
 
 	return 0;
 }
+#endif
 
 const struct ethtool_ops mlx5e_ethtool_ops = {
 	.get_drvinfo       = mlx5e_get_drvinfo,
@@ -1503,28 +1669,96 @@ const struct ethtool_ops mlx5e_ethtool_o
 	.set_msglevel      = mlx5e_set_msglevel,
 	.get_ringparam     = mlx5e_get_ringparam,
 	.set_ringparam     = mlx5e_set_ringparam,
+#ifdef HAVE_GET_SET_CHANNELS
 	.get_channels      = mlx5e_get_channels,
 	.set_channels      = mlx5e_set_channels,
+#endif
 	.get_coalesce      = mlx5e_get_coalesce,
 	.set_coalesce      = mlx5e_set_coalesce,
 	.get_settings      = mlx5e_get_settings,
 	.set_settings      = mlx5e_set_settings,
+#if defined(HAVE_GET_TS_INFO) && !defined(HAVE_GET_TS_INFO_EXT)
 	.get_ts_info       = mlx5e_get_ts_info,
+#endif
 	.set_pauseparam    = mlx5e_set_pauseparam,
 	.get_pauseparam    = mlx5e_get_pauseparam,
+#if defined(HAVE_SET_PHYS_ID) && !defined(HAVE_SET_PHYS_ID_EXT)
 	.set_phys_id       = mlx5e_set_phys_id,
+#endif
 	.get_wol	   = mlx5e_get_wol,
 	.set_wol	   = mlx5e_set_wol,
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	.get_priv_flags	   = mlx5e_get_priv_flags,
 	.set_priv_flags	   = mlx5e_set_priv_flags,
+#endif
+#ifdef LEGACY_ETHTOOL_OPS
+#if defined(HAVE_GET_SET_FLAGS)
+	.get_flags	   = mlx5e_get_flags,
+	.set_flags	   = mlx5e_set_flags,
+#endif
+#if defined(HAVE_GET_SET_TSO)
+	.get_tso	   = mlx5e_get_tso,
+	.set_tso	   = mlx5e_set_tso,
+#endif
+#if defined(HAVE_GET_SET_SG)
+	.get_sg = ethtool_op_get_sg,
+	.set_sg = ethtool_op_set_sg,
+#endif
+#if defined(HAVE_GET_SET_RX_CSUM)
+	.get_rx_csum = mlx5e_get_rx_csum,
+	.set_rx_csum = mlx5e_set_rx_csum,
+#endif
+#if defined(HAVE_GET_SET_TX_CSUM)
+	.get_tx_csum = ethtool_op_get_tx_csum,
+	.set_tx_csum = ethtool_op_set_tx_ipv6_csum,
+#endif
+#endif
+#if defined(HAVE_RXFH_INDIR_SIZE) && !defined(HAVE_RXFH_INDIR_SIZE_EXT)
 	.get_rxfh_indir_size = mlx5e_get_rxfh_indir_size,
+#endif
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	.get_rxfh_key_size = mlx5e_get_rxfh_key_size,
 	.get_rxfh          = mlx5e_get_rxfh,
 	.set_rxfh          = mlx5e_set_rxfh,
+#elif defined(HAVE_GET_SET_RXFH_INDIR) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
+	.get_rxfh_indir = mlx5e_get_rxfh_indir,
+	.set_rxfh_indir = mlx5e_set_rxfh_indir,
+#endif
 	.get_rxnfc         = mlx5e_get_rxnfc,
+#ifdef HAVE_GET_MODULE_EEPROM
 	.get_module_info   = mlx5e_get_module_info,
 	.get_module_eeprom = mlx5e_get_module_eeprom,
+#endif
+#ifdef HAVE_GET_SET_DUMP
 	.get_dump_flag	   = mlx5e_get_dump_flag,
 	.get_dump_data     = mlx5e_get_dump_data,
 	.set_dump          = mlx5e_set_dump,
+#endif
+};
+
+#ifdef HAVE_ETHTOOL_OPS_EXT
+const struct ethtool_ops_ext mlx5e_ethtool_ops_ext = {
+	.size		   = sizeof(struct ethtool_ops_ext),
+#ifdef HAVE_RXFH_INDIR_SIZE_EXT
+	.get_rxfh_indir_size = mlx5e_get_rxfh_indir_size,
+#endif
+#ifdef HAVE_GET_SET_RXFH_INDIR_EXT
+	.get_rxfh_indir = mlx5e_get_rxfh_indir,
+	.set_rxfh_indir = mlx5e_set_rxfh_indir,
+#endif
+#ifdef HAVE_GET_SET_CHANNELS_EXT
+	.get_channels	   = mlx5e_get_channels,
+	.set_channels	   = mlx5e_set_channels,
+#endif
+#ifdef HAVE_GET_TS_INFO_EXT
+	.get_ts_info = mlx5e_get_ts_info,
+#endif
+#ifdef HAVE_GET_MODULE_EEPROM_EXT
+	.get_module_info   = mlx5e_get_module_info,
+	.get_module_eeprom = mlx5e_get_module_eeprom,
+#endif
+#if !defined(HAVE_SET_PHYS_ID) && defined(HAVE_SET_PHYS_ID_EXT)
+	.set_phys_id       = mlx5e_set_phys_id,
+#endif
 };
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_fs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_fs.c
@@ -78,8 +78,11 @@ static void mlx5e_add_l2_to_hash(struct
 	struct mlx5e_l2_hash_node *hn;
 	int ix = mlx5e_hash_l2(addr);
 	int found = 0;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
-	hlist_for_each_entry(hn, &hash[ix], hlist)
+	compat_hlist_for_each_entry(hn, &hash[ix], hlist)
 		if (ether_addr_equal_64bits(hn->ai.addr, addr)) {
 			found = 1;
 			break;
@@ -310,8 +313,14 @@ void mlx5e_disable_vlan_filter(struct ml
 	}
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid)
+#else
+void mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int err = 0;
@@ -324,11 +333,20 @@ int mlx5e_vlan_rx_add_vid(struct net_dev
 					  vid);
 	mutex_unlock(&priv->state_lock);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return err;
+#endif
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid)
+#else
+void mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
@@ -339,7 +357,10 @@ int mlx5e_vlan_rx_kill_vid(struct net_de
 
 	mutex_unlock(&priv->state_lock);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return 0;
+#endif
 }
 
 int mlx5e_add_all_vlan_rules(struct mlx5e_priv *priv)
@@ -635,7 +656,7 @@ void mlx5e_del_tunneling_rule(struct mlx
 
 #define mlx5e_for_each_hash_node(hn, tmp, hash, i) \
 	for (i = 0; i < MLX5E_L2_ADDR_HASH_SIZE; i++) \
-		hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
+		compat_hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
 
 static void mlx5e_execute_l2_action(struct mlx5e_priv *priv,
 				    struct mlx5e_l2_hash_node *hn)
@@ -657,6 +678,9 @@ static void mlx5e_sync_netdev_addr(struc
 {
 	struct net_device *netdev = priv->netdev;
 	struct netdev_hw_addr *ha;
+#ifndef HAVE_NETDEV_FOR_EACH_MC_ADDR
+	struct dev_mc_list *mclist;
+#endif
 
 	netif_addr_lock_bh(netdev);
 
@@ -666,8 +690,14 @@ static void mlx5e_sync_netdev_addr(struc
 	netdev_for_each_uc_addr(ha, netdev)
 		mlx5e_add_l2_to_hash(priv->fs.l2.netdev_uc, ha->addr);
 
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	netdev_for_each_mc_addr(ha, netdev)
 		mlx5e_add_l2_to_hash(priv->fs.l2.netdev_mc, ha->addr);
+#else
+	for (mclist = netdev->mc_list; mclist; mclist = mclist->next)
+		mlx5e_add_l2_to_hash(priv->fs.l2.netdev_mc,
+				     mclist->dmi_addr);
+#endif
 
 	netif_addr_unlock_bh(netdev);
 }
@@ -682,6 +712,9 @@ static void mlx5e_fill_addr_array(struct
 	struct hlist_node *tmp;
 	int i = 0;
 	int hi;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	addr_list = is_uc ? priv->fs.l2.netdev_uc : priv->fs.l2.netdev_mc;
 
@@ -711,6 +744,9 @@ static void mlx5e_vport_context_update_a
 	int size;
 	int err;
 	int hi;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	size = is_uc ? 0 : (priv->fs.l2.broadcast_enabled ? 1 : 0);
 	max_size = is_uc ?
@@ -760,6 +796,9 @@ static void mlx5e_vport_context_update(s
 static void mlx5e_apply_netdev_addr(struct mlx5e_priv *priv)
 {
 	struct mlx5e_l2_hash_node *hn;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	int i;
 
@@ -773,6 +812,9 @@ static void mlx5e_apply_netdev_addr(stru
 static void mlx5e_handle_netdev_addr(struct mlx5e_priv *priv)
 {
 	struct mlx5e_l2_hash_node *hn;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	int i;
 
@@ -1407,7 +1449,9 @@ static int mlx5e_create_flow_tables(stru
 	if (err) {
 		netdev_err(priv->netdev, "Failed to create arfs tables, err=%d\n",
 			   err);
+#ifdef HAVE_NETDEV_HW_FEATURES
 		priv->netdev->hw_features &= ~NETIF_F_NTUPLE;
+#endif
 	}
 
 	err = mlx5e_create_inner_ttc_table(priv);
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -31,8 +31,10 @@
  */
 
 #include <linux/cpumask.h>
+#if defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)
 #include <net/vxlan.h>
 #include "vxlan.h"
+#endif
 #include "en.h"
 #include "eswitch.h"
 
@@ -221,6 +223,26 @@ free_out:
 	kvfree(in);
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static void mlx5e_update_sw_lro_stats(struct mlx5e_priv *priv)
+{
+	int i;
+	struct mlx5e_sw_stats *s = &priv->stats.sw;
+
+	s->rx_sw_lro_aggregated = 0;
+	s->rx_sw_lro_flushed = 0;
+	s->rx_sw_lro_no_desc = 0;
+
+	for (i = 0; i < priv->params.num_channels; i++) {
+		struct mlx5e_rq *rq = &priv->channel[i]->rq;
+
+		s->rx_sw_lro_aggregated += rq->sw_lro.lro_mgr.stats.aggregated;
+		s->rx_sw_lro_flushed += rq->sw_lro.lro_mgr.stats.flushed;
+		s->rx_sw_lro_no_desc += rq->sw_lro.lro_mgr.stats.no_desc;
+	}
+}
+#endif
+
 void mlx5e_update_stats(struct mlx5e_priv *priv)
 {
 	struct mlx5_core_dev *mdev = priv->mdev;
@@ -230,6 +252,9 @@ void mlx5e_update_stats(struct mlx5e_pri
 	mlx5e_update_vport_counters(priv);
 	mlx5e_update_pport_counters(priv);
 	mlx5e_update_sw_counters(priv);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_update_sw_lro_stats(priv);
+#endif
 }
 
 static void mlx5e_update_stats_work(struct work_struct *work)
@@ -316,6 +341,7 @@ void free_striding_rq_res(struct mlx5e_r
 #define MLX5E_HW2SW_MTU(hwmtu) (hwmtu - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
 #define MLX5E_SW2HW_MTU(swmtu) (swmtu + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
 
+#ifdef HAVE_SPLIT_PAGE_EXPORTED
 static int mlx5e_create_umr_mkey(struct mlx5e_priv *priv,
 				 struct mlx5_core_mr *umr_mr)
 {
@@ -350,7 +376,9 @@ static int mlx5e_create_umr_mkey(struct
 
 	return err;
 }
+#endif
 
+#ifdef HAVE_SPLIT_PAGE_EXPORTED
 static void mlx5e_set_rq_umr_mkey(struct mlx5e_priv *priv,
 				  struct mlx5e_channel *c,
 				  struct mlx5e_rq *rq)
@@ -372,6 +400,7 @@ static void mlx5e_set_rq_umr_mkey(struct
 	}
 	rq->umr_mkey_be = cpu_to_be32(rq->umr_mr->key);
 }
+#endif
 
 static int mlx5e_create_rq(struct mlx5e_channel *c,
 			   struct mlx5e_rq_param *param,
@@ -399,6 +428,7 @@ static int mlx5e_create_rq(struct mlx5e_
 
 	wq_sz = mlx5_wq_ll_get_size(&rq->wq);
 
+#ifdef HAVE_SPLIT_PAGE_EXPORTED
 	if (rq->rq_type == RQ_TYPE_STRIDE) {
 		/* TODO: take them from ethtool ..*/
 		rq->num_of_strides_in_wqe =
@@ -420,7 +450,9 @@ static int mlx5e_create_rq(struct mlx5e_
 		rq->mlx5e_poll_specific_rx_cq = mlx5e_poll_striding_rx_cq;
 		rq->wqe_sz = rq->num_of_strides_in_wqe * rq->stride_size;
 		byte_count = rq->wqe_sz;
-	} else {
+	} else
+#endif
+	{
 		rq->skb = kzalloc_node(wq_sz * sizeof(*rq->skb), GFP_KERNEL,
 				       cpu_to_node(c->cpu));
 		if (!rq->skb) {
@@ -432,8 +464,13 @@ static int mlx5e_create_rq(struct mlx5e_
 		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
 		rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
 		rq->mlx5e_poll_specific_rx_cq = mlx5e_poll_default_rx_cq;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		rq->wqe_sz = IS_HW_LRO(priv) ? priv->params.lro_wqe_sz :
+			MLX5E_SW2HW_MTU(priv->netdev->mtu);
+#else
 		rq->wqe_sz = (priv->params.lro_en) ? priv->params.lro_wqe_sz :
 						     MLX5E_SW2HW_MTU(priv->netdev->mtu);
+#endif
 		byte_count = rq->wqe_sz | MLX5_HW_START_PADDING;
 		rq->wqe_sz = SKB_DATA_ALIGN(rq->wqe_sz + MLX5E_NET_IP_ALIGN);
 	}
@@ -598,6 +635,58 @@ static int mlx5e_wait_for_min_rx_wqes(st
 	return -ETIMEDOUT;
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static int get_skb_hdr(struct sk_buff *skb, void **iphdr,
+			void **tcph, u64 *hdr_flags, void *priv)
+{
+	unsigned int ip_len;
+	struct iphdr *iph;
+
+	if (unlikely(skb->protocol != htons(ETH_P_IP)))
+		return -1;
+
+	/*
+	* In the future we may add an else clause that verifies the
+	* checksum and allows devices which do not calculate checksum
+	* to use LRO.
+	*/
+	if (unlikely(skb->ip_summed != CHECKSUM_UNNECESSARY))
+		return -1;
+
+	/* Check for non-TCP packet */
+	skb_reset_network_header(skb);
+	iph = ip_hdr(skb);
+	if (iph->protocol != IPPROTO_TCP)
+		return -1;
+
+	ip_len = ip_hdrlen(skb);
+	skb_set_transport_header(skb, ip_len);
+	*tcph = tcp_hdr(skb);
+
+	/* check if IP header and TCP header are complete */
+	if (ntohs(iph->tot_len) < ip_len + tcp_hdrlen(skb))
+		return -1;
+
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+	*iphdr = iph;
+
+	return 0;
+}
+
+static void mlx5e_rq_sw_lro_init(struct mlx5e_rq *rq)
+{
+	rq->sw_lro.lro_mgr.max_aggr 		= 64;
+	rq->sw_lro.lro_mgr.max_desc		= MLX5E_LRO_MAX_DESC;
+	rq->sw_lro.lro_mgr.lro_arr		= rq->sw_lro.lro_desc;
+	rq->sw_lro.lro_mgr.get_skb_header	= get_skb_hdr;
+	rq->sw_lro.lro_mgr.features		= LRO_F_NAPI;
+	rq->sw_lro.lro_mgr.frag_align_pad	= NET_IP_ALIGN;
+	rq->sw_lro.lro_mgr.dev			= rq->netdev;
+	rq->sw_lro.lro_mgr.ip_summed		= CHECKSUM_UNNECESSARY;
+	rq->sw_lro.lro_mgr.ip_summed_aggr	= CHECKSUM_UNNECESSARY;
+}
+#endif
+
 static int mlx5e_open_rq(struct mlx5e_channel *c,
 			 struct mlx5e_rq_param *param,
 			 struct mlx5e_rq *rq)
@@ -615,6 +704,10 @@ static int mlx5e_open_rq(struct mlx5e_ch
 	if (err)
 		goto err_destroy_rq;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_rq_sw_lro_init(rq);
+#endif
+
 	err = mlx5e_modify_rq_state(rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);
 	if (err)
 		goto err_disable_rq;
@@ -1118,6 +1211,7 @@ static void mlx5e_service_task(struct wo
 					       service_task);
 
 	mutex_lock(&priv->state_lock);
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	if (test_bit(MLX5E_STATE_OPENED, &priv->state)) {
 		if (priv->tstamp.ptp) {
 			mlx5e_ptp_overflow_check(priv);
@@ -1128,6 +1222,7 @@ static void mlx5e_service_task(struct wo
 			queue_delayed_work(priv->wq, dwork, MLX5E_SERVICE_TASK_DELAY);
 		}
 	}
+#endif
 	mutex_unlock(&priv->state_lock);
 }
 
@@ -1138,8 +1233,11 @@ static int mlx5e_get_cpu(struct mlx5e_pr
 	if (!priv->mdev->priv.irq_info[ix].mask)
 		return 0;
 
+#ifdef CONFIG_CPUMASK_OFFSTACK
 	affinity_mask = priv->mdev->priv.irq_info[ix].mask;
-
+#else
+	affinity_mask[0] = *(priv->mdev->priv.irq_info[ix].mask);
+#endif
 	return cpumask_first(affinity_mask);
 }
 
@@ -1364,12 +1462,20 @@ static int mlx5e_open_channel(struct mlx
 		}
 	}
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 	c->irq_desc = irq_to_desc(c->rq.cq.mcq.irqn);
+#endif
+
 	err = mlx5e_open_rq(c, &cparam->rq, &c->rq);
 	if (err)
 		goto err_close_sqs;
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)) || \
+     defined(CONFIG_COMPAT_IS_NETIF_SET_XPS_QUEUE_NOT_CONST_CPUMASK)
+	netif_set_xps_queue(netdev, (struct cpumask *)get_cpu_mask(c->cpu), ix);
+#else
 	netif_set_xps_queue(netdev, get_cpu_mask(c->cpu), ix);
+#endif
 	*cp = c;
 
 	mlx5_rename_comp_eq(priv->mdev, ix, priv->netdev->name);
@@ -1686,9 +1792,13 @@ static void mlx5e_close_rqts(struct mlx5
 
 static int mlx5e_rx_hash_fn(int hfunc)
 {
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	return (hfunc == ETH_RSS_HASH_TOP) ?
 	       MLX5_TIRC_RX_HASH_FN_HASH_TOEPLITZ :
 	       MLX5_TIRC_RX_HASH_FN_HASH_INVERTED_XOR8;
+#else
+	return MLX5_TIRC_RX_HASH_FN_HASH_INVERTED_XOR8;
+#endif
 }
 
 static int mlx5e_bits_invert(unsigned long a, int size)
@@ -1718,8 +1828,12 @@ static void mlx5e_fill_indir_rqt_rqns(st
 		int ix = i;
 		u32 rqn;
 
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		if (priv->params.rss_hfunc == ETH_RSS_HASH_XOR)
 			ix = mlx5e_bits_invert(i, MLX5E_LOG_INDIR_RQT_SIZE);
+#else
+		ix = mlx5e_bits_invert(i, MLX5E_LOG_INDIR_RQT_SIZE);
+#endif
 
 		ix = priv->params.indirection_rqt[ix];
 		rqn = priv->channel[ix]->rq.rqn;
@@ -1797,7 +1911,11 @@ void mlx5e_build_tir_ctx_common(struct m
 	if (MLX5_CAP_ETH(priv->mdev, self_lb_uc))
 		MLX5_SET(tirc, tirc, self_lb_block, 1);
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_HW_LRO(priv)) {
+#else
 	if (priv->params.lro_en) {
+#endif
 		MLX5_SET(tirc, tirc, lro_enable_mask,
 			 MLX5_TIRC_LRO_ENABLE_MASK_IPV4_LRO |
 			 MLX5_TIRC_LRO_ENABLE_MASK_IPV6_LRO);
@@ -1862,6 +1980,7 @@ static void mlx5e_build_tir_ctx(struct m
 			 priv->indir_rqtn);
 		MLX5_SET(tirc, tirc, rx_hash_fn,
 			 mlx5e_rx_hash_fn(priv->params.rss_hfunc));
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		if (priv->params.rss_hfunc == ETH_RSS_HASH_TOP) {
 			void *rss_key = MLX5_ADDR_OF(tirc, tirc,
 						     rx_hash_toeplitz_key);
@@ -1871,6 +1990,7 @@ static void mlx5e_build_tir_ctx(struct m
 			memcpy(rss_key, priv->params.toeplitz_hash_key,
 			       MLX5E_RSS_TOEPLITZ_KEY_SIZE);
 		}
+#endif
 		break;
 	}
 
@@ -2091,26 +2211,38 @@ static void mlx5e_close_tirs(struct mlx5
 static void mlx5e_netdev_set_tcs(struct net_device *netdev)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#ifdef HAVE_NETDEV_SET_TC_QUEUE
 	int nch = priv->params.num_channels;
+#endif
 	int ntc = priv->params.num_tc;
+#ifdef HAVE_NETDEV_SET_PRIO_TC_MAP
 	int prio;
+#endif
+#ifdef HAVE_NETDEV_SET_TC_QUEUE
 	int tc;
+#endif
 
+#ifdef HAVE_NETDEV_RESET_TC
 	netdev_reset_tc(netdev);
+#endif
 
 	if (ntc == 1)
 		return;
 
+#ifdef HAVE_NETDEV_SET_NUM_TC
 	netdev_set_num_tc(netdev, ntc);
-
+#endif
+#ifdef HAVE_NETDEV_SET_TC_QUEUE
 	/* Map netdev TCs to offset 0
 	* We have our own UP to TXQ mapping for QoS
 	*/
 	for (tc = 0; tc < ntc; tc++)
 		netdev_set_tc_queue(netdev, tc, nch, 0);
-
+#endif
+#ifdef HAVE_NETDEV_SET_PRIO_TC_MAP
 	for (prio = 0; prio < MLX5E_MAX_NUM_PRIO; prio++)
 		netdev_set_prio_tc_map(netdev, prio, prio % ntc);
+#endif
 }
 
 static int mlx5e_set_mtu(struct mlx5e_priv *priv, u16 mtu)
@@ -2212,6 +2344,7 @@ int mlx5e_open_locked(struct net_device
 		goto err_close_tirs;
 	}
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	err = mlx5e_rl_init_sysfs(netdev);
 	if (err) {
 		netdev_err(netdev, "%s: Failed to create rate limit sysfs entries, %d\n",
@@ -2219,19 +2352,26 @@ int mlx5e_open_locked(struct net_device
 		goto err_destroy_flow_steering;
 	}
 	hash_init(priv->flow_map_hash);
+#endif
 
 	mlx5e_init_eth_addr(priv);
 
+#if defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)
 	mlx5e_vxlan_init(priv);
+#endif
 
 	set_bit(MLX5E_STATE_OPENED, &priv->state);
 
 	mlx5e_create_debugfs(priv);
+#ifdef HAVE_NETIF_TX_START_STOP_ALL_QUEUES
 	netif_tx_start_all_queues(netdev);
+#endif
 	mlx5e_update_carrier(priv);
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #ifdef CONFIG_RFS_ACCEL
 	priv->netdev->rx_cpu_rmap = priv->mdev->rmap;
 #endif
+#endif
 	mlx5e_set_rx_mode_core(priv);
 
 	queue_delayed_work(priv->wq, &priv->update_stats_work, 0);
@@ -2239,8 +2379,10 @@ int mlx5e_open_locked(struct net_device
 
 	return 0;
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 err_destroy_flow_steering:
 	mlx5e_destroy_flow_steering(priv);
+#endif
 
 err_close_tirs:
 	mlx5e_close_tirs(priv);
@@ -2270,8 +2412,10 @@ static int mlx5e_open(struct net_device
 	err = mlx5e_open_locked(netdev);
 	mutex_unlock(&priv->state_lock);
 
+#if defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)
 	if (!err && mlx5e_vxlan_allowed(priv->mdev))
 		vxlan_get_rx_port(netdev);
+#endif
 
 	return err;
 }
@@ -2292,16 +2436,24 @@ int mlx5e_close_locked(struct net_device
 	clear_bit(MLX5E_STATE_OPENED, &priv->state);
 
 	mlx5e_set_rx_mode_core(priv);
+#if defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)
 	mlx5e_vxlan_cleanup(priv);
+#endif
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	if (priv->pflags & MLX5E_PRIV_FLAGS_SNIFFER_EN) {
 		mlx5e_sniffer_turn_off(netdev);
 		priv->pflags ^= MLX5E_PRIV_FLAGS_SNIFFER_EN;
 	}
+#endif
 	netif_carrier_off(priv->netdev);
+#ifdef HAVE_NETIF_TX_START_STOP_ALL_QUEUES
 	netif_tx_stop_all_queues(netdev);
 	netif_tx_disable(netdev);
+#endif
 	mlx5e_destroy_debugfs(priv);
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	mlx5e_rl_remove_sysfs(netdev);
+#endif
 	mlx5e_destroy_flow_steering(priv);
 	mlx5e_close_tirs(priv);
 	mlx5e_close_rqts(priv);
@@ -2415,14 +2567,33 @@ int mlx5e_setup_tc(struct net_device *ne
 	return err;
 }
 
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
+static int mlx5e_ndo_setup_tc(struct net_device *dev, u32 handle,
+			      __be16 proto, struct tc_to_netdev *tc)
+{
+	if (tc->type != TC_SETUP_MQPRIO)
+		return -EINVAL;
+
+	return mlx5e_setup_tc(dev, tc->tc);
+}
+#endif /* HAVE_NDO_SETUP_TC_4_PARAMS */
+
+#ifdef HAVE_NDO_GET_STATS64
 static struct rtnl_link_stats64 *
 mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#else
+static struct net_device_stats *mlx5e_get_stats(struct net_device *dev)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5e_sw_stats *sstats = &priv->stats.sw;
 	struct mlx5e_vport_stats *vstats = &priv->stats.vport;
 	struct mlx5e_pport_stats *pstats = &priv->stats.pport;
 
+#ifndef HAVE_NDO_GET_STATS64
+	struct net_device_stats *stats = &priv->netdev_stats;
+#endif
+
 	stats->rx_packets = sstats->rx_packets;
 	stats->rx_bytes   = sstats->rx_bytes;
 	stats->tx_packets = sstats->tx_packets;
@@ -2480,6 +2651,7 @@ static int mlx5e_set_mac(struct net_devi
 	return 0;
 }
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 #ifdef CONFIG_RFS_ACCEL
 static int set_feature_arfs(struct net_device *netdev, bool enable)
 {
@@ -2494,9 +2666,15 @@ static int set_feature_arfs(struct net_d
 	return err;
 }
 #endif
+#endif
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int mlx5e_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			      u32 features)
+#else
 			      netdev_features_t features)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	netdev_features_t changes = features ^ netdev->features;
@@ -2537,6 +2715,7 @@ out:
 
 	return err;
 }
+#endif
 
 static int mlx5e_change_mtu(struct net_device *netdev, int new_mtu)
 {
@@ -2585,7 +2764,11 @@ static int mlx5e_change_mtu(struct net_d
 	return err2 ? err2 : err1;
 }
 
+#ifdef HAVE_SIOCGHWTSTAMP
 static int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr)
+#else
+static int mlx5e_hwstamp_ioctl(struct net_device *dev, struct ifreq *ifr)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct hwtstamp_config config;
@@ -2633,6 +2816,7 @@ static int mlx5e_hwstamp_set(struct net_
 			    sizeof(config)) ? -EFAULT : 0;
 }
 
+#ifdef HAVE_SIOCGHWTSTAMP
 static int mlx5e_hwstamp_get(struct net_device *dev, struct ifreq *ifr)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -2640,19 +2824,33 @@ static int mlx5e_hwstamp_get(struct net_
 	return copy_to_user(ifr->ifr_data, &priv->tstamp.hwtstamp_config,
 			    sizeof(priv->tstamp.hwtstamp_config)) ? -EFAULT : 0;
 }
+#endif
 
 static int mlx5e_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 {
 	switch (cmd) {
 	case SIOCSHWTSTAMP:
+#ifdef HAVE_SIOCGHWTSTAMP
 		return mlx5e_hwstamp_set(dev, ifr);
 	case SIOCGHWTSTAMP:
 		return mlx5e_hwstamp_get(dev, ifr);
+#else
+	return mlx5e_hwstamp_ioctl(dev, ifr);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
 }
 
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+void mlx5e_vlan_register(struct net_device *netdev, struct vlan_group *grp)
+{
+        struct mlx5e_priv *priv = netdev_priv(netdev);
+        priv->vlan_grp = grp;
+}
+#endif
+
+#ifdef HAVE_NDO_SET_VF_MAC
 static int mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -2668,7 +2866,9 @@ static int mlx5e_set_vf_vlan(struct net_
 
 	return mlx5_eswitch_set_vport_vlan(mdev->priv.eswitch, vf + 1, vlan, qos);
 }
+#endif
 
+#ifdef HAVE_LINKSTATE
 static int mlx5_vport_link2ifla(u8 esw_link)
 {
 	switch (esw_link) {
@@ -2690,7 +2890,9 @@ static int mlx5_ifla_link2vport(u8 ifla_
 	};
 	return MLX5_ESW_VPORT_ADMIN_STATE_AUTO;
 }
+#endif
 
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
 static int mlx5e_set_vf_link_state(struct net_device *dev, int vf,
 				   int link_state)
 {
@@ -2700,7 +2902,9 @@ static int mlx5e_set_vf_link_state(struc
 	return mlx5_eswitch_set_vport_state(mdev->priv.eswitch, vf + 1,
 					    mlx5_ifla_link2vport(link_state));
 }
+#endif
 
+#if defined(HAVE_VF_INFO_SPOOFCHK) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK)
 static int mlx5e_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -2708,7 +2912,9 @@ static int mlx5e_set_vf_spoofchk(struct
 
 	return mlx5_eswitch_set_vport_spoofchk(mdev->priv.eswitch, vf + 1, setting);
 }
+#endif
 
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_TRUST) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_TRUST)
 static int mlx5e_set_vf_trust(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -2716,20 +2922,30 @@ static int mlx5e_set_vf_trust(struct net
 
 	return mlx5_eswitch_set_vport_trust(mdev->priv.eswitch, vf + 1, setting);
 }
+#endif
 
+#ifdef HAVE_NDO_SET_VF_MAC
+#ifdef HAVE_VF_TX_RATE
+static int mlx5e_set_vf_rate(struct net_device *dev, int vf, int max_tx_rate)
+#else
 static int mlx5e_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate,
 			     int max_tx_rate)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5_core_dev *mdev = priv->mdev;
 
+#ifndef HAVE_VF_TX_RATE
 	if (min_tx_rate)
 		return -EOPNOTSUPP;
+#endif
 
 	return mlx5_eswitch_set_vport_rate(mdev->priv.eswitch, vf + 1,
 					   max_tx_rate);
 }
+#endif
 
+#ifdef HAVE_NDO_SET_VF_MAC
 static int mlx5e_get_vf_config(struct net_device *dev,
 			       int vf, struct ifla_vf_info *ivi)
 {
@@ -2748,13 +2964,26 @@ static int mlx5e_get_vf_config(struct ne
 	ivi->vlan = evi.vlan;
 	ivi->qos = evi.qos;
 	memcpy(ivi->mac, evi.mac, sizeof(ivi->mac));
+#ifdef HAVE_LINKSTATE
 	ivi->linkstate = mlx5_vport_link2ifla(evi.linkstate);
+#endif
+#ifdef HAVE_VF_INFO_SPOOFCHK
 	ivi->spoofchk = evi.spoofchk;
+#endif
+#ifdef HAVE_VF_INFO_TRUST
 	ivi->trusted = evi.trust;
+#endif
+#ifdef HAVE_TX_RATE_LIMIT
 	ivi->max_tx_rate = evi.max_tx_rate;
+#else
+	ivi->tx_rate = evi.max_tx_rate;
+#endif
+
 	return err;
 }
+#endif
 
+#ifdef HAVE_NDO_GET_VF_STATS
 static int mlx5e_get_vf_stats(struct net_device *dev,
 			      int vf, struct ifla_vf_stats *vf_stats)
 {
@@ -2763,7 +2992,9 @@ static int mlx5e_get_vf_stats(struct net
 
 	return mlx5_eswitch_get_vport_stats(mdev->priv.eswitch, vf + 1, vf_stats);
 }
+#endif
 
+#if defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)
 static void mlx5e_add_vxlan_port(struct net_device *netdev,
 				 sa_family_t sa_family, __be16 port)
 {
@@ -2786,6 +3017,7 @@ static void mlx5e_del_vxlan_port(struct
 	mlx5e_vxlan_queue_work(priv, sa_family, be16_to_cpu(port), 0);
 }
 
+#if defined(HAVE_NETDEV_FEATURES_T)
 static netdev_features_t mlx5e_vxlan_features_check(struct mlx5e_priv *priv,
 						    struct sk_buff *skb,
 						    netdev_features_t features)
@@ -2825,7 +3057,9 @@ static netdev_features_t mlx5e_features_
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 
+#ifdef HAVE_VLAN_FEATURES_CHECK
 	features = vlan_features_check(skb, features);
+#endif
 	features = vxlan_features_check(skb, features);
 
 	/* Validate if the tunneled packet is being offloaded by HW */
@@ -2835,6 +3069,31 @@ static netdev_features_t mlx5e_features_
 
 	return features;
 }
+#elif defined(HAVE_VXLAN_GSO_CHECK)
+static bool mlx5e_gso_check(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	struct udphdr *udph;
+	u16 port;
+
+	if (!vxlan_gso_check(skb))
+		return false;
+
+	if (!skb->encapsulation)
+		return true;
+
+	udph = udp_hdr(skb);
+	port = be16_to_cpu(udph->dest);
+
+	if (!mlx5e_vxlan_lookup_port(priv, port)) {
+		skb->ip_summed = CHECKSUM_NONE;
+		return false;
+	}
+
+	return true;
+}
+#endif
+#endif
 
 static void mlx5e_tx_timeout(struct net_device *dev)
 {
@@ -2848,16 +3107,24 @@ static void mlx5e_tx_timeout(struct net_
 	num_sqs = priv->params.num_channels * priv->params.num_tc +
 		  priv->params.num_rl_txqs;
 
+#if (defined(HAVE_NETIF_XMIT_STOPPED) || defined(HAVE_NETIF_TX_QUEUE_STOPPED)) && defined (HAVE_NETDEV_GET_TX_QUEUE)
 	for (i = 0; i < num_sqs; i++) {
 		struct mlx5e_sq *sq = priv->txq_to_sq_map[i];
 
+#if defined(HAVE_NETIF_XMIT_STOPPED)
 		if (!netif_xmit_stopped(netdev_get_tx_queue(dev, i)))
+#else
+		if (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, i)))
+#endif
 			continue;
 		sched_work = true;
 		set_bit(MLX5E_SQ_TX_TIMEOUT, &sq->state);
 		netdev_err(dev, "TX timeout on queue: %d, SQ: 0x%x, CQ: 0x%x, SQ Cons: 0x%x SQ Prod: 0x%x\n",
 			   i, sq->sqn, sq->cq.mcq.cqn, sq->cc, sq->pc);
 	}
+#else
+	sched_work = true;
+#endif
 
 	if (sched_work && test_bit(MLX5E_STATE_OPENED, &priv->state))
 		queue_work(priv->wq, &priv->tx_timeout_work);
@@ -2867,34 +3134,91 @@ static struct net_device_ops mlx5e_netde
 	.ndo_open                = mlx5e_open,
 	.ndo_stop                = mlx5e_close,
 	.ndo_start_xmit          = mlx5e_xmit,
+#ifdef HAVE_NDO_SETUP_TC
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
+	.ndo_setup_tc            = mlx5e_ndo_setup_tc,
+#else  /* HAVE_NDO_SETUP_TC_4_PARAMS */
 	.ndo_setup_tc            = mlx5e_setup_tc,
+#endif /* HAVE_NDO_SETUP_TC_4_PARAMS */
+#endif /* HAVE_NDO_SETUP_TC */
 	.ndo_select_queue        = mlx5e_select_queue,
+#ifdef HAVE_NDO_GET_STATS64
 	.ndo_get_stats64         = mlx5e_get_stats,
+#else
+	.ndo_get_stats           = mlx5e_get_stats,
+#endif
 	.ndo_set_rx_mode         = mlx5e_set_rx_mode,
 	.ndo_set_mac_address     = mlx5e_set_mac,
 	.ndo_vlan_rx_add_vid	 = mlx5e_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid	 = mlx5e_vlan_rx_kill_vid,
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+	.ndo_vlan_rx_register    = mlx5e_vlan_register,
+#endif
+#if (defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_features        = mlx5e_set_features,
+#endif
 	.ndo_change_mtu		 = mlx5e_change_mtu,
+#ifdef HAVE_NDO_SET_VF_MAC
 	.ndo_set_vf_mac          = mlx5e_set_vf_mac,
 	.ndo_set_vf_vlan         = mlx5e_set_vf_vlan,
 	.ndo_get_vf_config       = mlx5e_get_vf_config,
+#endif
+#ifdef HAVE_NDO_GET_VF_STATS
 	.ndo_get_vf_stats        = mlx5e_get_vf_stats,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_link_state   = mlx5e_set_vf_link_state,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_spoofchk     = mlx5e_set_vf_spoofchk,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_TRUST) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_trust        = mlx5e_set_vf_trust,
+#endif
+#ifdef HAVE_NDO_SET_VF_MAC
+#ifndef HAVE_VF_TX_RATE
 	.ndo_set_vf_rate         = mlx5e_set_vf_rate,
+#else
+	.ndo_set_vf_tx_rate      = mlx5e_set_vf_rate,
+#endif
+#endif
+#if defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)
 	.ndo_add_vxlan_port	 = mlx5e_add_vxlan_port,
 	.ndo_del_vxlan_port	 = mlx5e_del_vxlan_port,
+#if defined(HAVE_NETDEV_FEATURES_T)
 	.ndo_features_check	 = mlx5e_features_check,
+#elif defined(HAVE_VXLAN_GSO_CHECK)
+	.ndo_gso_check           = mlx5e_gso_check,
+#endif
+#endif
 	.ndo_do_ioctl            = mlx5e_ioctl,
+#ifdef HAVE_NDO_RX_FLOW_STEER
 #ifdef CONFIG_RFS_ACCEL
 	.ndo_rx_flow_steer	 = mlx5e_rx_flow_steer,
 #endif
+#endif
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	.ndo_set_tx_maxrate      = mlx5e_set_tx_maxrate,
+#endif
 	.ndo_tx_timeout          = mlx5e_tx_timeout,
 };
 
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+static const struct net_device_ops_ext mlx5_netdev_ops_ext = {
+	.size             = sizeof(struct net_device_ops_ext),
+	.ndo_set_features = mlx5e_set_features,
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK
+	.ndo_set_vf_spoofchk    = mlx5e_set_vf_spoofchk,
+#endif
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_TRUST
+	.ndo_set_vf_trust       = mlx5e_set_vf_trust,
+#endif
+#if defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
+	.ndo_set_vf_link_state  = mlx5e_set_vf_link_state,
+#endif
+};
+#endif
+
 static int mlx5e_check_required_hca_cap(struct mlx5_core_dev *mdev)
 {
 	if (MLX5_CAP_GEN(mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
@@ -3001,7 +3325,9 @@ static void mlx5e_build_netdev_priv(stru
 		MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS;
 	priv->params.num_tc                = 1;
 	priv->params.default_vlan_prio     = 0;
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	priv->params.rss_hfunc             = ETH_RSS_HASH_XOR;
+#endif
 
 	mlx5e_build_default_indir_rqt(mdev, priv->params.indirection_rqt,
 				      MLX5E_INDIR_RQT_SIZE, num_channels);
@@ -3021,7 +3347,9 @@ static void mlx5e_build_netdev_priv(stru
 	if (mlx5e_get_rq_type_cap(mdev) == RQ_TYPE_STRIDE &&
 	    MLX5_CAP_ETH(mdev, lro_cap)) {
 		priv->params.lro_en = true;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
 		priv->pflags |= MLX5E_PRIV_FLAG_HWLRO;
+#endif
 	} else {
 		priv->params.lro_en = false;
 	}
@@ -3067,8 +3395,14 @@ static void mlx5e_set_netdev_dev_addr(st
 
 	if (!MLX5_CAP_GEN(priv->mdev, vport_group_manager) &&
 	    is_zero_ether_addr(netdev->dev_addr)) {
+#ifdef HAVE_NETDEV_ADDR_ASSIGN_TYPE
 		netdev->addr_assign_type |= NET_ADDR_RANDOM;
+#endif
+#ifdef HAVE_ETH_RANDOM_ADDR
 		eth_random_addr(netdev->dev_addr);
+#else
+		eth_hw_addr_random(netdev);
+#endif
 		mlx5_core_info(priv->mdev, "Assigned random MAC address %pM\n", netdev->dev_addr);
 	}
 }
@@ -3083,8 +3417,16 @@ static void mlx5e_build_netdev(struct ne
 	netdev->netdev_ops        = &mlx5e_netdev_ops;
 	netdev->watchdog_timeo    = 15 * HZ;
 
-	netdev->ethtool_ops	  = &mlx5e_ethtool_ops;
+#ifdef HAVE_ETHTOOL_OPS_EXT
+	SET_ETHTOOL_OPS(netdev, &mlx5e_ethtool_ops);
+	set_ethtool_ops_ext(netdev, &mlx5e_ethtool_ops_ext);
+#else
+	netdev->ethtool_ops       = &mlx5e_ethtool_ops;
+#endif
+
+#ifdef HAVE_IEEE_DCBNL_ETS
 	netdev->dcbnl_ops	  = &mlx5e_dcbnl_ops;
+#endif
 
 	netdev->vlan_features    |= NETIF_F_SG;
 	netdev->vlan_features    |= NETIF_F_IP_CSUM;
@@ -3093,11 +3435,14 @@ static void mlx5e_build_netdev(struct ne
 	netdev->vlan_features    |= NETIF_F_TSO;
 	netdev->vlan_features    |= NETIF_F_TSO6;
 	netdev->vlan_features    |= NETIF_F_RXCSUM;
+#ifdef HAVE_NETIF_F_RXHASH
 	netdev->vlan_features    |= NETIF_F_RXHASH;
+#endif
 
 	if (!!MLX5_CAP_ETH(mdev, lro_cap))
 		netdev->vlan_features    |= NETIF_F_LRO;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	netdev->hw_features       = netdev->vlan_features;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_TX;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_RX;
@@ -3111,17 +3456,37 @@ static void mlx5e_build_netdev(struct ne
 	    FT_CAP(flow_table_modify))
 		netdev->hw_features	 |= NETIF_F_NTUPLE;
 #endif
+#if defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)
 	if (mlx5e_vxlan_allowed(mdev)) {
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL
 		netdev->hw_features	|= NETIF_F_GSO_UDP_TUNNEL;
+#endif
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 		netdev->hw_enc_features |= NETIF_F_IP_CSUM;
 		netdev->hw_enc_features |= NETIF_F_RXCSUM;
 		netdev->hw_enc_features |= NETIF_F_TSO;
 		netdev->hw_enc_features |= NETIF_F_TSO6;
+#ifdef HAVE_NETIF_F_RXHASH
 		netdev->hw_enc_features |= NETIF_F_RXHASH;
+#endif
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL
 		netdev->hw_enc_features |= NETIF_F_GSO_UDP_TUNNEL;
+#endif
+#endif /* HAVE_NETDEV_HW_ENC_FEATURES */
 	}
+#endif /* HAVE_VXLAN_ENABLED && HAVE_VXLAN_DYNAMIC_PORT */
 
 	netdev->features          = netdev->hw_features;
+#else /* HAVE_NETDEV_HW_FEATURES */
+	netdev->features       = netdev->vlan_features;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_TX;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_RX;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_FILTER;
+#ifdef HAVE_SET_NETDEV_HW_FEATURES
+	set_netdev_hw_features(netdev, netdev->features);
+#endif
+#endif /* HAVE_NETDEV_HW_FEATURES */
+
 
 #ifdef CONFIG_RFS_ACCEL
 	netdev->features	 &= ~NETIF_F_NTUPLE;
@@ -3132,7 +3497,13 @@ static void mlx5e_build_netdev(struct ne
 
 	netdev->features         |= NETIF_F_HIGHDMA;
 
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	netdev->priv_flags       |= IFF_UNICAST_FLT;
+#endif
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+	set_netdev_ops_ext(netdev, &mlx5_netdev_ops_ext);
+#endif
 
 	mlx5e_set_netdev_dev_addr(netdev);
 }
@@ -3209,8 +3580,14 @@ static int mlx5e_attach_netdev(struct ml
 		goto err_destroy_mkey;
 	}
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 	if (MLX5_CAP_GEN(mdev, vport_group_manager))
 		mlx5e_dcbnl_initialize(netdev);
+#endif
+
+
+	if (!is_valid_ether_addr(netdev->perm_addr))
+		memcpy(netdev->perm_addr, netdev->dev_addr, netdev->addr_len);
 
 	mlx5e_enable_async_events(priv);
 
@@ -3277,9 +3654,13 @@ static void *mlx5e_create_netdev(struct
 	if (mlx5e_check_required_hca_cap(mdev))
 		return NULL;
 
+#ifdef HAVE_NEW_TX_RING_SCHEME
 	netdev = alloc_etherdev_mqs(sizeof(struct mlx5e_priv),
 				    nch * MLX5E_MAX_NUM_TC + MLX5E_MAX_RL_QUEUES,
 				    nch);
+#else
+	netdev = alloc_etherdev_mq(sizeof(struct mlx5e_priv), nch);
+#endif
 	if (!netdev) {
 		mlx5_core_err(mdev, "alloc_etherdev_mqs() failed\n");
 		return NULL;
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -144,6 +144,7 @@ void mlx5e_dealloc_rx_wqe(struct mlx5e_r
 	}
 }
 
+#ifdef HAVE_SPLIT_PAGE_EXPORTED
 static inline void
 mlx5e_dma_pre_sync_linear_mpwqe(struct device *pdev,
 				struct mlx5e_rx_wqe_info *wi,
@@ -366,8 +367,13 @@ static int mlx5e_alloc_rx_fragmented_mpw
 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
 		if (unlikely(mlx5e_alloc_and_map_page(rq, wi, i)))
 			goto err_unmap;
+#ifdef HAVE_MM_PAGE__COUNT
 		atomic_add(mlx5e_mpwqe_strides_per_page(rq),
 			   &wi->umr.dma_info[i].page->_count);
+#else
+		page_ref_add(wi->umr.dma_info[i].page,
+			     mlx5e_mpwqe_strides_per_page(rq));
+#endif
 		wi->skbs_frags[i] = 0;
 	}
 
@@ -385,8 +391,13 @@ err_unmap:
 	while (--i >= 0) {
 		dma_unmap_page(rq->pdev, wi->umr.dma_info[i].addr, PAGE_SIZE,
 			       PCI_DMA_FROMDEVICE);
+#ifdef HAVE_MM_PAGE__COUNT
 		atomic_sub(mlx5e_mpwqe_strides_per_page(rq),
 			   &wi->umr.dma_info[i].page->_count);
+#else
+		page_ref_sub(wi->umr.dma_info[i].page,
+			     mlx5e_mpwqe_strides_per_page(rq));
+#endif
 		put_page(wi->umr.dma_info[i].page);
 	}
 	dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz, PCI_DMA_TODEVICE);
@@ -410,8 +421,13 @@ void mlx5e_free_rx_fragmented_mpwqe(stru
 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
 		dma_unmap_page(rq->pdev, wi->umr.dma_info[i].addr, PAGE_SIZE,
 			       PCI_DMA_FROMDEVICE);
+#ifdef HAVE_MM_PAGE__COUNT
 		atomic_sub(mlx5e_mpwqe_strides_per_page(rq) - wi->skbs_frags[i],
 			   &wi->umr.dma_info[i].page->_count);
+#else
+		page_ref_sub(wi->umr.dma_info[i].page,
+			     mlx5e_mpwqe_strides_per_page(rq) - wi->skbs_frags[i]);
+#endif
 		put_page(wi->umr.dma_info[i].page);
 	}
 	dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz, PCI_DMA_TODEVICE);
@@ -454,7 +470,11 @@ inline int mlx5e_alloc_rx_linear_mpwqe(s
 		goto err_put_page;
 	}
 
+#ifdef HAVE_MM_PAGE__COUNT
 	atomic_add(rq->num_of_strides_in_wqe, &wi->dma_info.page->_count);
+#else
+	page_ref_add(wi->dma_info.page, rq->num_of_strides_in_wqe);
+#endif
 	wi->skbs_frags[0] = 0;
 	wi->used_strides = 0;
 	wi->dma_pre_sync = mlx5e_dma_pre_sync_linear_mpwqe;
@@ -476,8 +496,13 @@ void mlx5e_free_rx_linear_mpwqe(struct m
 {
 	dma_unmap_page(rq->pdev, wi->dma_info.addr, rq->wqe_sz,
 		       PCI_DMA_FROMDEVICE);
+#ifdef HAVE_MM_PAGE__COUNT
 	atomic_sub(rq->num_of_strides_in_wqe - wi->skbs_frags[0],
 		   &wi->dma_info.page->_count);
+#else
+	page_ref_sub(wi->dma_info.page,
+		     rq->num_of_strides_in_wqe - wi->skbs_frags[0]);
+#endif
 	put_page(wi->dma_info.page);
 }
 
@@ -507,6 +532,7 @@ void mlx5e_dealloc_striding_rx_wqe(struc
 
 	wi->free_wqe(rq, wi);
 }
+#endif /* HAVE_SPLIT_PAGE_EXPORTED */
 
 #define RQ_CANNOT_POST(rq) \
 		(!test_bit(MLX5E_RQ_STATE_ACTIVE, &rq->state) || \
@@ -603,15 +629,21 @@ static void mlx5e_lro_update_hdr(struct
 	/* TODO: handle tcp checksum */
 }
 
+#ifdef HAVE_NETIF_F_RXHASH
 static inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,
 				      struct sk_buff *skb)
 {
+#ifdef HAVE_SKB_SET_HASH
 	u8 cht = cqe->rss_hash_type;
 	int ht = (cht & CQE_RSS_HTYPE_L4) ? PKT_HASH_TYPE_L4 :
 		 (cht & CQE_RSS_HTYPE_IP) ? PKT_HASH_TYPE_L3 :
 					    PKT_HASH_TYPE_NONE;
 	skb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);
+#else
+	skb->rxhash = be32_to_cpu(cqe->rss_hash_result);
+#endif
 }
+#endif
 
 static void mlx5e_validate_loopback(struct mlx5e_priv *priv,
 				    struct sk_buff *skb)
@@ -650,7 +682,11 @@ static inline void mlx5e_handle_csum(str
 	    likely(cqe->hds_ip_ext & CQE_L4_OK)) {
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 		if (cqe->outer_l3_tunneled & CQE_TUNNELED) {
+#if defined(HAVE_SK_BUFF_CSUM_LEVEL)
 			skb->csum_level = 1;
+#elif defined(HAVE_SK_BUFF_ENCAPSULATION)
+			skb->encapsulation = 1;
+#endif
 			rq->stats.csum_unnecessary_inner++;
 		} else
 			rq->stats.csum_unnecessary++;
@@ -705,12 +741,18 @@ static inline void mlx5e_build_rx_skb(st
 
 	skb_record_rx_queue(skb, rq->ix);
 
+#ifdef HAVE_NETIF_F_RXHASH
 	if (likely(netdev->features & NETIF_F_RXHASH))
 		mlx5e_skb_set_hash(cqe, skb);
+#endif
 
 	if (cqe_has_vlan(cqe))
+#ifndef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
+		__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->vlan_info));
+#else
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
 				       be16_to_cpu(cqe->vlan_info));
+#endif
 }
 
 #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD      (128 - NET_IP_ALIGN)
@@ -760,9 +802,44 @@ static struct sk_buff *mlx5e_get_rx_skb(
 }
 
 static inline void mlx5e_receive_skb(struct mlx5e_cq *cq, struct mlx5e_rq *rq,
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+			    struct sk_buff *skb, struct mlx5_cqe64 *prev_cqe)
+#else
 			    struct sk_buff *skb)
+#endif
 {
-	napi_gro_receive(cq->napi, skb);
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX || defined CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct net_device *netdev = rq->netdev;
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+#endif
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		if (IS_SW_LRO(priv))
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+			if (priv->vlan_grp && cqe_has_vlan(prev_cqe))
+				lro_vlan_hwaccel_receive_skb(&rq->sw_lro.lro_mgr,
+							     skb,priv->vlan_grp,
+							     be16_to_cpu(prev_cqe->vlan_info),
+							     NULL);
+			else
+#endif
+			lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+		else
+#endif
+
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+		if (priv->vlan_grp && cqe_has_vlan(prev_cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+			vlan_gro_receive(cq->napi, priv->vlan_grp,
+					 be16_to_cpu(prev_cqe->vlan_info),
+					 skb);
+#else
+			vlan_hwaccel_receive_skb(skb, priv->vlan_grp,
+					be16_to_cpu(prev_cqe->vlan_info));
+#endif
+		else
+#endif
+			napi_gro_receive(cq->napi, skb);
 }
 
 struct sk_buff *mlx5e_poll_striding_rx_cq(struct mlx5_cqe64 *cqe,
@@ -809,6 +886,9 @@ bool mlx5e_poll_rx_cq(struct mlx5e_cq *c
 	struct net_device *netdev = rq->netdev;
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct mlx5_cqe64 *cqe;
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+	struct mlx5_cqe64 *prev_cqe;
+#endif
 	int i;
 
 	if (unlikely(!test_bit(MLX5E_RQ_STATE_ACTIVE, &rq->state)))
@@ -850,9 +930,17 @@ bool mlx5e_poll_rx_cq(struct mlx5e_cq *c
 		rq->stats.packets++;
 		rq->stats.bytes += bytes_recv;
 
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+                prev_cqe = cqe;
+#endif
+
 		cqe = mlx5e_get_cqe(cq);
 
-		mlx5e_receive_skb(cq, rq, skb);
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+                mlx5e_receive_skb(cq, rq, skb, prev_cqe);
+#else
+                mlx5e_receive_skb(cq, rq, skb);
+#endif
 
 wq_ll_pop:
 		if (!rq->is_poll || (rq->is_poll && rq->is_poll(rq)))
@@ -865,6 +953,11 @@ wq_ll_pop:
 	/* ensure cq space is freed before enabling more cqes */
 	wmb();
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(priv))
+		lro_flush_all(&rq->sw_lro.lro_mgr);
+#endif
+
 	return (i == budget);
 }
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
@@ -71,6 +71,11 @@ struct mlx5e_sw_stats {
 	u64 tx_queue_stopped;
 	u64 tx_queue_wake;
 	u64 tx_queue_dropped;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	u64 rx_sw_lro_aggregated;
+	u64 rx_sw_lro_flushed;
+	u64 rx_sw_lro_no_desc;
+#endif
 	u64 rx_wqe_err;
 	u64 rx_cqe_compress_pkts;
 	u64 rx_cqe_compress_blks;
@@ -102,6 +107,11 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_queue_stopped) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_queue_wake) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_queue_dropped) },
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_aggregated) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_flushed) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_no_desc) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_wqe_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cqe_compress_pkts) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cqe_compress_blks) },
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_sysfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_sysfs.c
@@ -32,6 +32,7 @@
 
 #include <linux/device.h>
 #include <linux/netdevice.h>
+#include <linux/dcbnl.h>
 #include "en.h"
 #include "en_ecn.h"
 
@@ -40,6 +41,7 @@
 #define MLX5E_100MBPS_TO_KBPS 100000
 #define set_kobj_mode(mdev) mlx5_core_is_pf(mdev) ? S_IWUSR | S_IRUGO : S_IRUGO
 
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
 static ssize_t mlx5e_show_tc_num(struct device *device,
 				 struct device_attribute *attr,
 				 char *buf)
@@ -76,6 +78,7 @@ static ssize_t mlx5e_store_tc_num(struct
 	rtnl_unlock();
 	return count;
 }
+#endif
 
 static  ssize_t mlx5e_show_maxrate(struct device *device,
 				   struct device_attribute *attr,
@@ -184,8 +187,10 @@ out:
 
 static DEVICE_ATTR(maxrate, S_IRUGO | S_IWUSR,
 		   mlx5e_show_maxrate, mlx5e_store_maxrate);
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
 static DEVICE_ATTR(tc_num, S_IRUGO | S_IWUSR,
 		   mlx5e_show_tc_num, mlx5e_store_tc_num);
+#endif
 
 static ssize_t mlx5e_show_lro_timeout(struct device *device,
 				      struct device_attribute *attr,
@@ -515,7 +520,9 @@ static struct attribute *mlx5e_debug_gro
 };
 
 static struct attribute *mlx5e_qos_attrs[] = {
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
 	&dev_attr_tc_num.attr,
+#endif
 	&dev_attr_maxrate.attr,
 	NULL,
 };
@@ -569,6 +576,7 @@ void mlx5e_sysfs_remove(struct net_devic
 	kobject_put(priv->ecn_root_kobj);
 }
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 enum {
 	ATTR_DST_IP,
 	ATTR_DST_PORT,
@@ -728,3 +736,4 @@ void mlx5e_rl_remove_sysfs(struct net_de
 		sysfs_remove_group(&txq->kobj, &mlx5e_txmap_attr);
 	}
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -37,9 +37,15 @@
 #define MLX5E_SQ_NOPS_ROOM  MLX5_SEND_WQE_MAX_WQEBBS
 #define MLX5E_SQ_STOP_ROOM (MLX5_SEND_WQE_MAX_WQEBBS +\
 			    MLX5E_SQ_NOPS_ROOM)
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 #define MLX5E_TX_HW_STAMP(priv, skb)					\
 	(priv->tstamp.hwtstamp_config.tx_type == HWTSTAMP_TX_ON &&	\
 	skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+#else
+#define MLX5E_TX_HW_STAMP(priv, skb)					\
+	(priv->tstamp.hwtstamp_config.tx_type == HWTSTAMP_TX_ON &&	\
+	skb_shinfo(skb)->tx_flags.flags & SKBTX_HW_TSTAMP)
+#endif
 
 void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw)
 {
@@ -108,6 +114,7 @@ static void mlx5e_dma_unmap_wqe_err(stru
 	}
 }
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 static u16 mlx5e_select_queue_assigned(struct mlx5e_priv *priv,
 				       struct sk_buff *skb)
 {
@@ -152,14 +159,28 @@ static u16 mlx5e_select_queue_assigned(s
 fallback:
 	return 0;
 }
+#endif
+
+#ifndef HAVE_SELECT_QUEUE_FALLBACK_T
+#define fallback(dev, skb) __netdev_pick_tx(dev, skb)
+#endif
 
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 		       void *accel_priv, select_queue_fallback_t fallback)
+#else
+		       void *accel_priv)
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int channel_ix = fallback(dev, skb);
 	int up = 0;
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	if (priv->params.num_rl_txqs) {
 		u16 ix = mlx5e_select_queue_assigned(priv, skb);
 
@@ -168,9 +189,12 @@ u16 mlx5e_select_queue(struct net_device
 			return ix;
 		}
 	}
+#endif
 
+#ifdef HAVE_NETDEV_GET_NUM_TC
 	if (!netdev_get_num_tc(dev))
 		return channel_ix;
+#endif
 
 	if (skb_vlan_tag_present(skb))
 		up = skb->vlan_tci >> VLAN_PRIO_SHIFT;
@@ -220,7 +244,11 @@ static inline void mlx5e_insert_vlan(voi
 
 	memcpy(vhdr, *skb_data, cpy1_sz);
 	mlx5e_tx_skb_pull_inline(skb_data, skb_len, cpy1_sz);
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	vhdr->h_vlan_proto = skb->vlan_proto;
+#else
+	vhdr->h_vlan_proto = cpu_to_be16(ETH_P_8021Q);
+#endif
 	vhdr->h_vlan_TCI = cpu_to_be16(skb_vlan_tag_get(skb));
 	memcpy(&vhdr->h_vlan_encapsulated_proto, *skb_data, cpy2_sz);
 	mlx5e_tx_skb_pull_inline(skb_data, skb_len, cpy2_sz);
@@ -253,14 +281,18 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
 		eseg->cs_flags	= MLX5_ETH_WQE_L3_CSUM;
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER)
 		if (skb->encapsulation) {
 			eseg->cs_flags |= MLX5_ETH_WQE_L3_INNER_CSUM |
 					  MLX5_ETH_WQE_L4_INNER_CSUM;
 			sq->stats.csum_partial_inner++;
 		} else {
+#endif
 			eseg->cs_flags |= MLX5_ETH_WQE_L4_CSUM;
 			sq->stats.csum_partial++;
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER)
 		}
+#endif
 	} else
 		sq->stats.csum_none++;
 
@@ -273,21 +305,27 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 		eseg->mss = cpu_to_be16(skb_shinfo(skb)->gso_size);
 		opcode    = MLX5_OPCODE_LSO;
 
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER)
 		if (skb->encapsulation) {
 			ihs = skb_inner_transport_header(skb) - skb->data +
 			      inner_tcp_hdrlen(skb);
 			sq->stats.tso_inner_packets++;
 			sq->stats.tso_inner_bytes += skb->len - ihs;
 		} else {
+#endif
 			ihs = skb_transport_offset(skb) + tcp_hdrlen(skb);
 			sq->stats.tso_packets++;
 			sq->stats.tso_bytes += skb->len - ihs;
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER)
 		}
+#endif
 
 		num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
 	} else {
 		bf = sq->bf_budget   &&
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 		     !skb->xmit_more &&
+#endif
 		     !skb_shinfo(skb)->nr_frags;
 		ihs = mlx5e_get_inline_hdr_size(sq, skb, bf);
 		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
@@ -360,7 +398,11 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 	sq->pc += wi->num_wqebbs;
 
 	if (unlikely(MLX5E_TX_HW_STAMP(sq->channel->priv, skb)))
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+#else
+		skb_shinfo(skb)->tx_flags.flags |= SKBTX_IN_PROGRESS;
+#endif
 
 	netdev_tx_sent_queue(sq->txq, wi->num_bytes);
 
@@ -369,7 +411,10 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 		sq->stats.queue_stopped++;
 	}
 
-	if (!skb->xmit_more || netif_xmit_stopped(sq->txq)) {
+#ifdef HAVE_SK_BUFF_XMIT_MORE
+	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
+#endif
+	{
 		int bf_sz = 0;
 
 		if (bf && sq->uar_bf_map)
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -62,10 +62,25 @@ struct mlx5_cqe64 *mlx5e_get_cqe(struct
 
 static inline bool mlx5e_no_channel_affinity_change(struct mlx5e_channel *c)
 {
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 	int current_cpu = smp_processor_id();
+#ifdef HAVE_IRQ_DATA_AFFINITY
 	struct cpumask *aff = irq_desc_get_irq_data(c->irq_desc)->affinity;
+#else
+	struct irq_data *idata;
+	const struct cpumask *aff;
+	idata = irq_desc_get_irq_data(c->irq_desc);
+	aff = irq_data_get_affinity_mask(idata);
+#endif
 
 	return cpumask_test_cpu(current_cpu, aff);
+#else
+	if (c->tot_rx < MLX5_EN_MIN_RX_ARM)
+		return true;
+
+	c->tot_rx = 0;
+	return false;
+#endif
 }
 
 static void mlx5e_poll_ico_cq(struct mlx5e_cq *cq)
@@ -103,9 +118,11 @@ static void mlx5e_poll_ico_cq(struct mlx
 		switch (icowi->opcode) {
 		case MLX5_OPCODE_NOP:
 			break;
+#ifdef HAVE_SPLIT_PAGE_EXPORTED
 		case MLX5_OPCODE_UMR:
 			mlx5e_post_rx_fragmented_mpwqe(&sq->channel->rq);
 			break;
+#endif
 		default:
 			WARN_ONCE(true,
 				  "mlx5e: Bad OPCODE in ICOSQ WQE info: 0x%x\n",
@@ -140,6 +157,9 @@ int mlx5e_napi_poll(struct napi_struct *
 	for (i = 0; i < c->num_tx; i++)
 		busy |= mlx5e_poll_tx_cq(&c->sq[i].cq);
 
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	c->tot_rx += budget;
+#endif
 	if (busy && likely(mlx5e_no_channel_affinity_change(c)))
 		return budget;
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
@@ -705,6 +705,7 @@ static void esw_apply_vport_addr_list(st
 	struct hlist_head *hash;
 	struct hlist_node *tmp;
 	int hi;
+	COMPAT_HL_NODE
 
 	vport_addr_add = is_uc ? esw_add_uc_addr :
 				 esw_add_mc_addr;
@@ -742,6 +743,7 @@ static void esw_update_vport_addr_list(s
 	int err;
 	int hi;
 	int i;
+	COMPAT_HL_NODE
 
 	size = is_uc ? MLX5_MAX_UC_PER_VPORT(esw->dev) :
 		       MLX5_MAX_MC_PER_VPORT(esw->dev);
@@ -824,6 +826,7 @@ static void esw_update_vport_mc_promisc(
 	struct hlist_head *hash;
 	struct hlist_node *tmp;
 	int hi;
+	COMPAT_HL_NODE
 
 	hash = vport->mc_list;
 
@@ -1790,6 +1793,10 @@ int mlx5_eswitch_init(struct mlx5_core_d
 	    MLX5_CAP_GEN(dev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
 		return 0;
 
+	/* In RH6.5 pci_sriov_get_totalvfs might return -EINVAL instead of 0 */
+	if (total_vports < 1)
+		total_vports = 1;
+
 	esw_info(dev,
 		 "Total vports %d, l2 table size(%d), per vport: max uc(%d) max mc(%d)\n",
 		 total_vports, l2_table_size,
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@ -56,14 +56,21 @@ struct l2addr_node {
 
 #define for_each_l2hash_node(hn, tmp, hash, i) \
 	for (i = 0; i < MLX5_L2_ADDR_HASH_SIZE; i++) \
-		hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
+		compat_hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
+
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+#define COMPAT_HL_NODE struct hlist_node *hlnode;
+#else
+#define COMPAT_HL_NODE
+#endif
 
 #define l2addr_hash_find(hash, mac, type) ({                \
 	int ix = MLX5_L2_ADDR_HASH(mac);                    \
 	bool found = false;                                 \
 	type *ptr = NULL;                                   \
+        COMPAT_HL_NODE                                      \
 							    \
-	hlist_for_each_entry(ptr, &hash[ix], node.hlist)    \
+	compat_hlist_for_each_entry(ptr, &hash[ix], node.hlist)\
 		if (ether_addr_equal(ptr->node.addr, mac)) {\
 			found = true;                       \
 			break;                              \
@@ -177,6 +184,17 @@ struct mlx5_eswitch {
 	struct esw_mc_addr      *mc_promisc;
 };
 
+#ifndef HAVE_NDO_GET_VF_STATS
+struct ifla_vf_stats {
+	__u64 rx_packets;
+	__u64 tx_packets;
+	__u64 rx_bytes;
+	__u64 tx_bytes;
+	__u64 broadcast;
+	__u64 multicast;
+};
+#endif
+
 /* E-Switch API */
 int mlx5_eswitch_init(struct mlx5_core_dev *dev);
 void mlx5_eswitch_cleanup(struct mlx5_eswitch *esw);
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -282,6 +282,9 @@ static int mlx5_enable_msix(struct mlx5_
 	int num_eqs = 1 << MLX5_CAP_GEN(dev, log_max_eq);
 	struct mlx5_priv *priv = &dev->priv;
 	int nvec;
+#ifndef HAVE_PCI_ENABLE_MSIX_RANGE
+	int err;
+#endif
 	int i;
 
 	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
@@ -301,12 +304,25 @@ static int mlx5_enable_msix(struct mlx5_
 	for (i = 0; i < nvec; i++)
 		priv->msix_arr[i].entry = i;
 
+#ifdef HAVE_PCI_ENABLE_MSIX_RANGE
 	nvec = pci_enable_msix_range(dev->pdev, priv->msix_arr,
 				     MLX5_EQ_VEC_COMP_BASE + 1, nvec);
 	if (nvec < 0)
 		return nvec;
 
 	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+#else
+retry:
+	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+	err = pci_enable_msix(dev->pdev, priv->msix_arr, nvec);
+	if (err <= 0) {
+		return err;
+	} else if (err > 2) {
+		nvec = err;
+		goto retry;
+	}
+	mlx5_core_dbg(dev, "received %d MSI vectors out of %d requested\n", err, nvec);
+#endif
 
 	return 0;
 
@@ -843,7 +859,13 @@ static void mlx5_irq_set_affinity_hint(s
 	return;
 
 err_clear_mask:
+#ifdef CONFIG_CPUMASK_OFFSTACK
 	priv->irq_info[i].mask = NULL;
+#else
+	/* just to keep gcc happy - (we can't have a label at the end of a
+	 * function) */
+	return;
+#endif
 }
 
 static void mlx5_irq_clear_affinity_hint(struct mlx5_core_dev *mdev, int i)
@@ -851,10 +873,15 @@ static void mlx5_irq_clear_affinity_hint
 	struct mlx5_priv *priv  = &mdev->priv;
 	struct msix_entry *msix = priv->msix_arr;
 	int irq                 = msix[i + MLX5_EQ_VEC_COMP_BASE].vector;
-	cpumask_var_t mask      = priv->irq_info[i].mask;
+	cpumask_var_t mask;
 
 	if (!priv->irq_info[i].mask)
 		return;
+#ifdef CONFIG_CPUMASK_OFFSTACK
+	mask                    = priv->irq_info[i].mask;
+#else
+	mask[0]                 = *(priv->irq_info[i].mask);
+#endif
 
 	irq_set_affinity_hint(irq, NULL);
 	free_cpumask_var(mask);
@@ -945,6 +972,7 @@ unlock:
 	spin_unlock(&table->lock);
 }
 
+#ifdef HAVE_GET_SET_DUMP
 int mlx5_vector2eq(struct mlx5_core_dev *dev, int vector, struct mlx5_eq *eqc)
 {
 	struct mlx5_eq_table *table = &dev->priv.eq_table;
@@ -963,6 +991,7 @@ int mlx5_vector2eq(struct mlx5_core_dev
 
 	return err;
 }
+#endif
 
 static void free_comp_eqs(struct mlx5_core_dev *dev)
 {
@@ -1500,11 +1529,13 @@ static int mlx5_init_once(struct mlx5_co
 		goto err_tables_cleanup;
 	}
 
+#ifdef HAVE_GET_SET_DUMP
 	err = mlx5_mst_dump_init(dev);
 	if (err) {
 		dev_err(&pdev->dev, "Failed to init mst dump %d\n", err);
 		goto err_rl_cleanup;
 	}
+#endif
 
 	err = mlx5_eswitch_init(dev);
 	if (err) {
@@ -1524,9 +1555,11 @@ err_eswitch_cleanup:
 	mlx5_eswitch_cleanup(dev->priv.eswitch);
 
 err_mst_dump_cleanup:
+#ifdef HAVE_GET_SET_DUMP
 	mlx5_mst_dump_cleanup(dev);
 
 err_rl_cleanup:
+#endif
 	mlx5_cleanup_rl_table(dev);
 
 err_tables_cleanup:
@@ -1547,7 +1580,9 @@ static void mlx5_cleanup_once(struct mlx
 {
 	mlx5_sriov_cleanup(dev);
 	mlx5_eswitch_cleanup(dev->priv.eswitch);
+#ifdef HAVE_GET_SET_DUMP
 	mlx5_mst_dump_cleanup(dev);
+#endif
 	mlx5_cleanup_rl_table(dev);
 	mlx5_cleanup_dct_table(dev);
 	mlx5_cleanup_mr_table(dev);
@@ -2115,7 +2150,11 @@ static void mlx5_pci_resume(struct pci_d
 		dev_info(&pdev->dev, "%s: device recovered\n", __func__);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_PCI_ERROR_HANDLERS
 static const struct pci_error_handlers mlx5_err_handler = {
+#else
+static struct pci_error_handlers mlx5_err_handler = {
+#endif
 	.error_detected = mlx5_pci_err_detected,
 	.slot_reset	= mlx5_pci_slot_reset,
 	.resume		= mlx5_pci_resume
@@ -2173,7 +2212,9 @@ static struct pci_driver mlx5_core_drive
 	.remove			= remove_one,
 	.shutdown		= shutdown,
 	.err_handler		= &mlx5_err_handler,
+#ifdef HAVE_PCI_DRIVER_SRIOV_CONFIGURE
 	.sriov_configure	= mlx5_core_sriov_configure,
+#endif
 };
 
 static int __init init(void)
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -194,10 +194,13 @@ int mlx5_query_port_ets_rate_limit(struc
 				   u8 max_bw_value[MLX5_MAX_NUM_TC],
 				   u8 max_bw_unit[MLX5_MAX_NUM_TC]);
 struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn);
+#ifdef HAVE_GET_SET_DUMP 
 int mlx5_vector2eq(struct mlx5_core_dev *dev, int vector, struct mlx5_eq *eqc);
+#endif
 void mlx5_cq_tasklet_cb(unsigned long data);
 u32 mlx5_get_msix_vec(struct mlx5_core_dev *dev, int vecidx);
 
+#ifdef HAVE_GET_SET_DUMP 
 int mlx5_mst_dump_init(struct mlx5_core_dev *dev);
 int mlx5_mst_capture(struct mlx5_core_dev *dev);
 u32 mlx5_mst_dump(struct mlx5_core_dev *dev, void *buff, u32 buff_sz);
@@ -209,6 +212,7 @@ int mlx5_icmd_access_register(struct mlx
 			      int method,
 			      void *io_buff,
 			      u32 io_buff_dw_sz);
+#endif
 
 void mlx5e_init(void);
 void mlx5e_cleanup(void);
--- a/drivers/net/ethernet/mellanox/mlx5/core/mst_dump.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mst_dump.c
@@ -64,6 +64,7 @@
 #define MLX5_MST_DUMP_SIZE_BYTES_MT4115 (0x561e0l)
 #define MLX5_MST_DUMP_SIZE_BYTES_MT4117 (0x4eed0l)
 
+#ifdef HAVE_GET_SET_DUMP
 static const unsigned long
 mlx5_mst_dump_regs_mt4117[MLX5_NUM_MST_OFFSETS_MT4117][2] = {{0x000000, 16388},
 	{0x010084, 1},
@@ -7391,4 +7392,4 @@ unlock:
 out:
 	return ret;
 }
-
+#endif /* HAVE_GET_SET_DUMP */
--- a/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
@@ -32,6 +32,7 @@
 
 #include <linux/pci.h>
 #include <linux/sysfs.h>
+#include <linux/etherdevice.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/vport.h>
 #include "mlx5_core.h"
@@ -719,7 +720,11 @@ static ssize_t stats_store(struct mlx5_s
 	return -ENOTSUPP;
 }
 
+#ifndef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
+static struct sysfs_ops vf_sysfs_ops = {
+#else
 static const struct sysfs_ops vf_sysfs_ops = {
+#endif
 	.show = vf_attr_show,
 	.store = vf_attr_store,
 };
@@ -835,8 +840,12 @@ static ssize_t num_vf_store(struct devic
 	int req_vfs;
 	int err;
 
-	if (kstrtoint(buf, 0, &req_vfs) || req_vfs < 0 ||
-	    req_vfs > pci_sriov_get_totalvfs(pdev))
+	if (kstrtoint(buf, 0, &req_vfs) || req_vfs < 0
+#ifdef HAVE_PCI_SRIOV_GET_TOTALVFS
+	    || req_vfs > pci_sriov_get_totalvfs(pdev))
+#else
+	)
+#endif
 		return -EINVAL;
 
 	err = mlx5_core_sriov_configure(pdev, req_vfs);
--- a/drivers/net/ethernet/mellanox/mlx5/core/uar.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/uar.c
@@ -211,7 +211,12 @@ int mlx5_alloc_map_uar(struct mlx5_core_
 
 	if (mdev->priv.bf_mapping)
 		uar->bf_map = io_mapping_map_wc(mdev->priv.bf_mapping,
+#ifdef HAVE_IO_MAPPING_MAP_WC_3_PARAMS
+						uar->index << PAGE_SHIFT,
+						PAGE_SIZE);
+#else
 						uar->index << PAGE_SHIFT);
+#endif
 
 	return 0;
 
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -42,7 +42,11 @@
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 #include <linux/radix-tree.h>
+#ifdef HAVE_TIMECOUNTER_H
 #include <linux/timecounter.h>
+#else
+#include <linux/clocksource.h>
+#endif
 
 #include <linux/mlx5/device.h>
 #include <linux/mlx5/doorbell.h>
@@ -662,8 +666,9 @@ struct mlx5_special_contexts {
 	int resd_lkey;
 };
 
+#ifdef HAVE_GET_SET_DUMP
 struct mlx5_mst_dump;
-
+#endif
 struct mlx5_core_dev {
 	struct pci_dev	       *pdev;
 	struct mutex		pci_status_mutex;
@@ -699,7 +704,9 @@ struct mlx5_core_dev {
 	struct mlx5_flow_root_namespace *sniffer_rx_root_ns;
 	struct mlx5_flow_root_namespace *sniffer_tx_root_ns;
 	u32 num_q_counter_allocated[MLX5_INTERFACE_NUMBER];
+#ifdef HAVE_GET_SET_DUMP
 	struct mlx5_mst_dump *mst_dump;
+#endif
 };
 
 struct mlx5_db {
@@ -749,8 +756,13 @@ struct mlx5_cmd_work_ent {
 	int			page_queue;
 	u8			status;
 	u8			token;
+#ifdef HAVE_KTIME_GET_NS
 	u64			ts1;
 	u64			ts2;
+#else
+	struct timespec ts1;
+	struct timespec ts2;
+#endif
 	u16			op;
 };
 
@@ -1228,6 +1240,7 @@ static inline bool mlx5_rl_is_supported(
 
 /* MLX5 Diagnostics */
 
+#ifdef HAVE_GET_SET_DUMP
 #define MLX5_DIAG_DUMP_VERSION  1
 
 #define MLX5_DIAG_FLAG_MST	BIT(0)
@@ -1283,4 +1296,5 @@ struct mlx5_diag_dump {
 	char dump[0];
 } __packed;
 
+#endif /* HAVE_GET_SET_DUMP */
 #endif /* MLX5_DRIVER_H */
