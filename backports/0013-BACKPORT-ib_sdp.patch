From: Vladimir Sokolovsky <vlad@mellanox.com>
Subject: [PATCH] BACKPORT: ib_sdp

Change-Id: I25082aa2e036cc7b4dd32363f0c071a3d6fda54d
Signed-off-by: Vladimir Sokolovsky <vlad@mellanox.com>
---
 drivers/infiniband/ulp/sdp/sdp.h      |  22 ++++-
 drivers/infiniband/ulp/sdp/sdp_cma.c  |  39 ++++++--
 drivers/infiniband/ulp/sdp/sdp_main.c | 176 +++++++++++++++++++++++++++++++---
 drivers/infiniband/ulp/sdp/sdp_proc.c |  32 +++++++
 drivers/infiniband/ulp/sdp/sdp_rx.c   |  24 +++--
 drivers/infiniband/ulp/sdp/sdp_tx.c   |   2 +-
 6 files changed, 260 insertions(+), 35 deletions(-)

--- a/drivers/infiniband/ulp/sdp/sdp.h
+++ b/drivers/infiniband/ulp/sdp/sdp.h
@@ -50,7 +50,7 @@
 #define TCPHDR_URG TCPCB_FLAG_URG
 #endif
 
-#ifdef CONFIG_COMPAT_IS_INET_SOCK_INET_NUM
+#ifdef HAVE_INET_SOCK_INET_NUM
 #define sdp_inet_num(sk)	(inet_sk(sk)->inet_num)
 #define sdp_inet_sport(sk)	(inet_sk(sk)->inet_sport)
 #define sdp_inet_dport(sk)	(inet_sk(sk)->inet_dport)
@@ -73,7 +73,7 @@
 #endif
 #define sk_ssk(ssk) ((struct sock *)ssk)
 
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0))
+#ifndef HAVE_SK_PAGE_FRAG
 	#define TCP_PAGE(sk)    (sk->sk_sndmsg_page)
 	#define TCP_OFF(sk)     (sk->sk_sndmsg_off)
 #else
@@ -658,11 +658,11 @@ static inline struct sk_buff *sdp_stream
 	skb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);
 	if (skb) {
 
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 6, 0))
-		if ((kind == SK_MEM_RECV && sk_rmem_schedule(sk,
+#ifdef HAVE_SK_RMEM_SCHEDULE_3P
+		if ((kind == SK_MEM_RECV && sk_rmem_schedule(sk, skb,
 			skb->truesize)) ||
 #else
-		if ((kind == SK_MEM_RECV && sk_rmem_schedule(sk, skb,
+		if ((kind == SK_MEM_RECV && sk_rmem_schedule(sk,
 			skb->truesize)) ||
 #endif
 			(kind == SK_MEM_SEND && sk_wmem_schedule(sk,
@@ -867,6 +867,7 @@ static inline void sdpstats_hist(u32 *h,
 	h[idx]++;
 }
 
+#if defined(HAVE_THIS_CPU_PTR) && ! defined(HAVE_GET_CPU_VAR)
 #define SDPSTATS_COUNTER_INC(stat) do { this_cpu_ptr(&sdpstats)->stat++; } while (0)
 #define SDPSTATS_COUNTER_ADD(stat, val) do { this_cpu_ptr(&sdpstats)->stat += val; } while (0)
 #define SDPSTATS_COUNTER_MID_INC(stat, mid) do { this_cpu_ptr(&sdpstats)->stat[mid]++; } \
@@ -876,6 +877,17 @@ static inline void sdpstats_hist(u32 *h,
 
 #define SDPSTATS_HIST_LINEAR(stat, size) \
 	sdpstats_hist(this_cpu_ptr(&sdpstats)->stat, size, ARRAY_SIZE(this_cpu_ptr(&sdpstats)->stat) - 1, 0)
+#else
+#define SDPSTATS_COUNTER_INC(stat) do { __get_cpu_var(sdpstats).stat++; } while (0)
+#define SDPSTATS_COUNTER_ADD(stat, val) do { __get_cpu_var(sdpstats).stat += val; } while (0)
+#define SDPSTATS_COUNTER_MID_INC(stat, mid) do { __get_cpu_var(sdpstats).stat[mid]++; } \
+	while (0)
+#define SDPSTATS_HIST(stat, size) \
+	sdpstats_hist(__get_cpu_var(sdpstats).stat, size, ARRAY_SIZE(__get_cpu_var(sdpstats).stat) - 1, 1)
+
+#define SDPSTATS_HIST_LINEAR(stat, size) \
+	sdpstats_hist(__get_cpu_var(sdpstats).stat, size, ARRAY_SIZE(__get_cpu_var(sdpstats).stat) - 1, 0)
+#endif
 
 #else
 #define SDPSTATS_COUNTER_INC(stat)
--- a/drivers/infiniband/ulp/sdp/sdp_cma.c
+++ b/drivers/infiniband/ulp/sdp/sdp_cma.c
@@ -52,10 +52,6 @@
 
 #define SDP_MAJV_MINV 0x22
 
-#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 3, 0))
-	#define ipv6_addr_copy(a, b) (*(a) = *(b))
-#endif
-
 SDP_MODPARAM_INT(sdp_rx_size, 0x40, "HW rx queue size (max num of credits)."
 		" Must be power of 2.");
 
@@ -189,11 +185,7 @@ static int sdp_connect_handler(struct so
 	if (!h->max_adverts)
 		return -EINVAL;
 
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 3, 0))
-	child = sk_clone(sk, GFP_KERNEL);
-#else
 	child = sk_clone_lock(sk, GFP_KERNEL);
-#endif
 	if (!child)
 		return -ENOMEM;
 
@@ -211,6 +203,7 @@ static int sdp_connect_handler(struct so
 		memcpy(newnp, inet6_sk(sk), sizeof(struct ipv6_pinfo));
 		if ((h->ipv_cap & HH_IPV_MASK) == HH_IPV4) {
 			/* V6 mapped */
+#ifdef HAVE_SK_V6_RCV_SADDR
 			sdp_inet_daddr(child) = dst_addr->sin_addr.s_addr;
 			ipv6_addr_set(&child->sk_v6_daddr, 0, 0, htonl(0x0000FFFF),
 					h->src_addr.ip4.addr);
@@ -227,6 +220,23 @@ static int sdp_connect_handler(struct so
 			ipv6_addr_copy(&child->sk_v6_daddr, &dst_addr6->sin6_addr);
 			ipv6_addr_copy(&child->sk_v6_rcv_saddr, &src_addr6->sin6_addr);
 			ipv6_addr_copy(&newnp->saddr, &src_addr6->sin6_addr);
+#else
+			ipv6_addr_set(&newnp->daddr, 0, 0, htonl(0x0000FFFF),
+					h->src_addr.ip4.addr);
+
+			ipv6_addr_set(&newnp->saddr, 0, 0, htonl(0x0000FFFF),
+					h->dst_addr.ip4.addr);
+
+			ipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);
+		} else if ((h->ipv_cap & HH_IPV_MASK) == HH_IPV6) {
+			struct sockaddr_in6 *dst_addr6 = (struct sockaddr_in6 *)dst_addr;
+			struct sockaddr_in6 *src_addr6 = 
+				(struct sockaddr_in6 *)&id->route.addr.src_addr;
+
+			ipv6_addr_copy(&newnp->daddr, &dst_addr6->sin6_addr);
+			ipv6_addr_copy(&newnp->saddr, &src_addr6->sin6_addr);
+			ipv6_addr_copy(&newnp->rcv_saddr, &src_addr6->sin6_addr);
+#endif
 		} else {
 			sdp_warn(child, "Bad IPV field: 0x%x\n", h->ipv_cap & HH_IPV_MASK);
 		}
@@ -283,7 +293,11 @@ static int sdp_connect_handler(struct so
 
 	/* child->sk_write_space(child); */
 	/* child->sk_data_ready(child, 0); */
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+	sk->sk_data_ready(sk, 0);
+#else
 	sk->sk_data_ready(sk);
+#endif
 
 	return 0;
 }
@@ -466,12 +480,21 @@ int sdp_cma_handler(struct rdma_cm_id *i
 
 			if (src_addr->sa_family == AF_INET) {
 				/* IPv4 over IPv6 */
+#ifdef HAVE_SK_V6_RCV_SADDR
 				ipv6_addr_set(&sk->sk_v6_rcv_saddr, 0, 0, htonl(0xFFFF),
 						addr4->sin_addr.s_addr);
 			} else {
 				sk->sk_v6_rcv_saddr = addr6->sin6_addr;
 			}
 			inet6_sk(sk)->saddr = sk->sk_v6_rcv_saddr;
+#else
+				ipv6_addr_set(&inet6_sk(sk)->rcv_saddr, 0, 0, htonl(0xFFFF),
+						addr4->sin_addr.s_addr);
+			} else {
+				inet6_sk(sk)->rcv_saddr = addr6->sin6_addr;
+			}
+			inet6_sk(sk)->saddr = inet6_sk(sk)->rcv_saddr;
+#endif
 		}
 			else
 #endif
--- a/drivers/infiniband/ulp/sdp/sdp_main.c
+++ b/drivers/infiniband/ulp/sdp/sdp_main.c
@@ -81,10 +81,6 @@ MODULE_AUTHOR("Michael S. Tsirkin");
 MODULE_DESCRIPTION("InfiniBand SDP module");
 MODULE_LICENSE("Dual BSD/GPL");
 
-#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 3, 0))
-	#define ipv6_addr_copy(a, b) (*(a) = *(b))
-#endif
-
 #ifdef CONFIG_INFINIBAND_SDP_DEBUG
 SDP_MODPARAM_INT(sdp_debug_level, 0, "Enable debug tracing if > 0.");
 #endif
@@ -172,6 +168,7 @@ static int sdp_get_port(struct sock *sk,
 
 #if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
 	if (inet6_sk(sk)) {
+#ifdef HAVE_SK_V6_RCV_SADDR
 		int addr_type = ipv6_addr_type(&sk->sk_v6_rcv_saddr);
 		if (addr_type == IPV6_ADDR_MAPPED) {
 			addr4->sin_family = AF_INET;
@@ -185,6 +182,21 @@ static int sdp_get_port(struct sock *sk,
 			ipv6_addr_copy(&addr6->sin6_addr, &sk->sk_v6_rcv_saddr);
 			addr_len = sizeof(*addr6);
 		}
+#else
+		int addr_type = ipv6_addr_type(&inet6_sk(sk)->rcv_saddr);
+		if (addr_type == IPV6_ADDR_MAPPED) {
+			addr4->sin_family = AF_INET;
+			addr4->sin_port = htons(snum);
+			addr4->sin_addr.s_addr = inet6_sk(sk)->rcv_saddr.s6_addr32[3];
+			addr_len = sizeof(*addr4);
+		} else {
+			addr6->sin6_family = AF_INET6;
+			addr6->sin6_port = htons(snum);
+			addr6->sin6_scope_id = sk->sk_bound_dev_if;
+			ipv6_addr_copy(&addr6->sin6_addr, &inet6_sk(sk)->rcv_saddr);
+			addr_len = sizeof(*addr6);
+		}
+#endif
 	}
 		else
 #endif
@@ -525,7 +537,7 @@ static void sdp_destroy_resources(struct
 	sk->sk_send_head = NULL;
 	skb_queue_purge(&sk->sk_write_queue);
 
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0))
+#ifndef HAVE_SK_PAGE_FRAG
         /*
          * If sendmsg cached page exists, toss it.
          */
@@ -605,10 +617,17 @@ static void sdp_destruct(struct sock *sk
 
 	if (sk->sk_wmem_queued || atomic_read(&sk->sk_rmem_alloc) || sk->sk_forward_alloc) {
 		sdp_dbg(sk, "wmem_queued: 0x%x rmem_alloc: 0x%x forward: 0x%x "
+#ifdef HAVE_ATOMIC_LONG_READ
 				"proto: 0x%lx\n", sk->sk_wmem_queued,
 				atomic_read(&sk->sk_rmem_alloc),
 				sk->sk_forward_alloc,
 				atomic_long_read(sk->sk_prot->memory_allocated));
+#else
+				"proto: 0x%x\n", sk->sk_wmem_queued,
+				atomic_read(&sk->sk_rmem_alloc),
+				sk->sk_forward_alloc,
+				atomic_read(sk->sk_prot->memory_allocated));
+#endif
 	}
 
 	if (ssk->parent)
@@ -847,13 +866,18 @@ static int sdp_ipv6_connect(struct sock
 	src_addr->sin6_port = htons(sdp_inet_sport(sk));
 	src_addr->sin6_addr = inet6_sk(sk)->saddr;
 
+#ifdef HAVE_SK_V6_RCV_SADDR
 	if (ssk->id && (addr_type != ipv6_addr_type(&sk->sk_v6_rcv_saddr))) {
+#else
+	if (ssk->id && (addr_type != ipv6_addr_type(&inet6_sk(sk)->rcv_saddr))) {
+#endif
 		sdp_dbg(sk, "Existing address type is different for the "
 				"requested. rebinding socket\n");
 		rdma_destroy_id(ssk->id);
 		ssk->id = NULL;
 	}
 
+#ifdef HAVE_SK_V6_RCV_SADDR
 	if (!ssk->id) {
 		/* If IPv4 over IPv6, make sure rdma_bind will expect ipv4 address */
 		if (addr_type == IPV6_ADDR_MAPPED)
@@ -866,6 +890,20 @@ static int sdp_ipv6_connect(struct sock
 	}
 
 	ipv6_addr_copy(&sk->sk_v6_daddr, &usin->sin6_addr);
+#else
+	if (!ssk->id) {
+		/* If IPv4 over IPv6, make sure rdma_bind will expect ipv4 address */
+		if (addr_type == IPV6_ADDR_MAPPED)
+			ipv6_addr_set(&inet6_sk(sk)->rcv_saddr, 0, 0, htonl(0x0000FFFF), 0);
+
+		rc = sdp_get_port(sk, htons(sdp_inet_sport(sk)));
+		if (rc)
+			return rc;
+		sdp_inet_sport(sk) = htons(sdp_inet_num(sk));
+	}
+
+	ipv6_addr_copy(&inet6_sk(sk)->daddr, &usin->sin6_addr);
+#endif
 
 	if (addr_type == IPV6_ADDR_MAPPED) {
 		struct sockaddr_in *addr4 = (struct sockaddr_in *)uaddr;
@@ -1296,10 +1334,10 @@ int sdp_init_sock(struct sock *sk)
 	lockdep_set_class(&sk->sk_callback_lock,
 					&ib_sdp_sk_callback_lock_key);
 
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0))
+#ifndef HAVE_NETIF_F_HW_CSUM
 	sk->sk_route_caps |= NETIF_F_SG | NETIF_F_NO_CSUM;
 #else
-	sk->sk_route_caps |= NETIF_F_SG;
+	sk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;
 #endif
 
 	skb_queue_head_init(&ssk->rx_ctl_q);
@@ -1623,7 +1661,11 @@ static int sdp_recv_urg(struct sock *sk,
 
 		if (len > 0) {
 			if (!(flags & MSG_TRUNC))
+#ifdef HAVE_MEMCPY_TO_MSG
 				err = memcpy_to_msg(msg, &c, 1);
+#else
+				err = memcpy_toiovec(msg->msg_iov, &c, 1);
+#endif
 			len = 1;
 		} else
 			msg->msg_flags |= MSG_TRUNC;
@@ -1670,7 +1712,11 @@ void sdp_skb_entail(struct sock *sk, str
 }
 
 static inline int sdp_bcopy_get(struct sock *sk, struct sk_buff *skb,
+#ifdef HAVE_UIO_IOV_ITER
 				struct iov_iter *from, int copy)
+#else
+				char __user *from, int copy)
+#endif
 {
 	int err;
 	struct sdp_sock *ssk = sdp_sk(sk);
@@ -1688,7 +1734,7 @@ static inline int sdp_bcopy_get(struct s
 		int i = skb_shinfo(skb)->nr_frags;
 		struct page *page = TCP_PAGE(sk);
 		int off = TCP_OFF(sk);
-#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 7, 0))
+#ifdef HAVE_SK_PAGE_FRAG
 		struct page_frag *pfrag = sk_page_frag(sk);
 		if (!sk_page_frag_refill(sk, pfrag))
 			return SDP_DO_WAIT_MEM;
@@ -1722,7 +1768,7 @@ static inline int sdp_bcopy_get(struct s
 
 		if (!page) {
 			/* Allocate new cache page. */
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0))
+#ifndef HAVE_SK_PAGE_FRAG
 			page = sk_stream_alloc_page(sk);
 #else
 			pfrag = sk_page_frag(sk);
@@ -1737,10 +1783,14 @@ static inline int sdp_bcopy_get(struct s
 		/* Time to copy data. We are close to
 		 * the end! */
 		SDPSTATS_COUNTER_ADD(memcpy_count, copy);
+#ifdef HAVE_SKB_COPY_TO_PAGE_NOCACHE
 		err = skb_copy_to_page_nocache(sk, from, skb, page,
+#else
+		err = skb_copy_to_page(sk, from, skb, page,
+#endif
 				       off, copy);
 		if (err) {
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0))
+#ifndef HAVE_SK_PAGE_FRAG
 			/* If this page was new, give it to the
 			 * socket so it does not get leaked.
 			 */
@@ -1877,13 +1927,27 @@ do_interrupted:
 
 /* Like tcp_sendmsg */
 /* TODO: check locking */
+#ifdef HAVE_NET_SENDMSG_4_PARAMS
 static int sdp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		size_t size)
+#else
+static int sdp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
+#endif
 {
 	int i;
 	struct sdp_sock *ssk = sdp_sk(sk);
 	struct sk_buff *skb;
+#ifdef HAVE_MSGHDR_MSG_ITER
 	int iovlen, flags, err, copied;
+#else
+	int flags, err, copied;
+#endif
+#ifndef HAVE_NET_SENDMSG_4_PARAMS
+	/* iocb is not used in this case.
+	 * Set it to NULL just to save backports lines.
+	 * */
+	struct kiocb *iocb = NULL;
+#endif
 	const int size_goal = MIN(ssk->xmit_size_goal, SDP_MAX_PAYLOAD);
 	long timeo;
 	int zcopy_thresh =
@@ -1902,8 +1966,12 @@ static int sdp_sendmsg(struct kiocb *ioc
 	timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
 
 	/* Wait for a connection to finish. */
+#ifdef HAVE_TCP_PASSIVE_FASTOPEN
 	if (((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &&
 			!tcp_passive_fastopen(sk)) {
+#else
+	if ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) {
+#endif
 		if ((err = sk_stream_wait_connect(sk, &timeo)) != 0)
 			goto out_err;
 	}
@@ -1912,18 +1980,29 @@ static int sdp_sendmsg(struct kiocb *ioc
 	clear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);
 
 	/* Ok commence sending. */
+#ifdef HAVE_MSGHDR_MSG_ITER
 	iovlen = msg->msg_iter.nr_segs;
+#endif
 	copied = 0;
 
 	err = -EPIPE;
 	if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))
 		goto do_error;
 
+#ifdef HAVE_MSGHDR_MSG_ITER
 	for (i = 0; i < iovlen; i++) {
 		struct iovec *iov = &msg->msg_iter.iov[i];
 		int seglen = iov->iov_len;
 
 		sdp_dbg_data(sk, "Sending iov: 0x%x/0x%zx %p\n", i, iovlen, iov);
+#else
+	for (i = 0; i < msg->msg_iovlen; i++) {
+		struct iovec *iov = &msg->msg_iov[i];
+		int seglen = iov->iov_len;
+		char __user *from = iov->iov_base;
+
+		sdp_dbg_data(sk, "Sending iov: 0x%x/0x%zx %p\n", i, msg->msg_iovlen, from);
+#endif
 
 		SDPSTATS_HIST(sendmsg_seglen, seglen);
 
@@ -1943,6 +2022,9 @@ static int sdp_sendmsg(struct kiocb *ioc
 
 			copied += zcopied;
 			seglen = iov->iov_len;
+#ifndef HAVE_MSGHDR_MSG_ITER
+			from = iov->iov_base;
+#endif
 
 			sdp_dbg_data(sk, "ZCopied: 0x%x/0x%x\n", zcopied, seglen);
 		}
@@ -1981,10 +2063,10 @@ new_segment:
 				 * Check whether we can use HW checksum.
 				 */
 				if (sk->sk_route_caps &
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0))
+#ifndef HAVE_NETIF_F_HW_CSUM
 				    (NETIF_F_IP_CSUM | NETIF_F_NO_CSUM |
 #else
-				    (NETIF_F_IP_CSUM |
+				    (NETIF_F_IP_CSUM | NETIF_F_HW_CSUM |
 #endif
 				     NETIF_F_HW_CSUM))
 					skb->ip_summed = CHECKSUM_PARTIAL;
@@ -2011,7 +2093,11 @@ new_segment:
 			if (copy > seglen)
 				copy = seglen;
 
+#ifdef HAVE_MSGHDR_MSG_ITER
 			copy = sdp_bcopy_get(sk, skb, &msg->msg_iter, copy);
+#else
+			copy = sdp_bcopy_get(sk, skb, from, copy);
+#endif
 
 			if (unlikely(copy < 0)) {
 				switch (copy) {
@@ -2033,6 +2119,9 @@ new_segment:
 			SDP_SKB_CB(skb)->end_seq += copy;
 			/*unused: skb_shinfo(skb)->gso_segs = 0;*/
 
+#ifndef HAVE_MSGHDR_MSG_ITER
+			from += copy;
+#endif
 			copied += copy;
 			seglen -= copy;
 			continue;
@@ -2132,8 +2221,13 @@ int sdp_abort_rx_srcavail(struct sock *s
 /* Like tcp_recvmsg */
 /* Maybe use skb_recv_datagram here? */
 /* Note this does not seem to handle vectored messages. Relevant? */
+#ifdef HAVE_NET_SENDMSG_4_PARAMS
 static int sdp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t len, int noblock, int flags, int *addr_len)
+#else
+static int sdp_recvmsg(struct sock *sk, struct msghdr *msg,
+		       size_t len, int noblock, int flags, int *addr_len)
+#endif
 {
 	int copied = 0;
 	u32 peek_seq;
@@ -2153,7 +2247,11 @@ static int sdp_recvmsg(struct kiocb *ioc
 	lock_sock(sk);
 	ssk->cpu = smp_processor_id();
 	sdp_dbg_data(sk, "iovlen: %zd iov_len: 0x%zx flags: 0x%x peek: 0x%x\n",
+#ifdef HAVE_MSGHDR_MSG_ITER
 			msg->msg_iter.nr_segs, msg->msg_iter.iov[0].iov_len, flags,
+#else
+			msg->msg_iovlen, msg->msg_iov[0].iov_len, flags,
+#endif
 			MSG_PEEK);
 
 	posts_handler_get(ssk);
@@ -2386,7 +2484,11 @@ sdp_mid_data:
 			if (rx_sa && offset >= skb->len) {
 				/* No more payload - start rdma copy */
 				sdp_dbg_data(sk, "RDMA copy of 0x%lx bytes\n", used);
+#ifdef HAVE_MSGHDR_MSG_ITER
 				err = sdp_rdma_to_iovec(sk, msg->msg_iter.iov, msg->msg_iter.nr_segs, skb,
+#else
+				err = sdp_rdma_to_iovec(sk, msg->msg_iov, msg->msg_iovlen, skb,
+#endif
 						&used, offset);
 				if (unlikely(err)) {
 					/* ssk->rx_sa might had been freed when
@@ -2402,9 +2504,16 @@ sdp_mid_data:
 				}
 			} else {
 				sdp_dbg_data(sk, "memcpy 0x%lx bytes +0x%x -> %p\n",
+#ifdef HAVE_MSGHDR_MSG_ITER
 						used, offset, msg->msg_iter.iov[0].iov_base);
 
 				err = skb_copy_datagram_msg(skb, offset, msg, used);
+#else
+						used, offset, msg->msg_iov[0].iov_base);
+
+				err = skb_copy_datagram_iovec(skb, offset,
+						msg->msg_iov, used);
+#endif
 				if (rx_sa && !(flags & MSG_PEEK)) {
 					rx_sa->copied += used;
 					rx_sa->reported += used;
@@ -2633,11 +2742,19 @@ void sdp_urg(struct sdp_sock *ssk, struc
 		BUG();
 	ssk->urg_data = TCP_URG_VALID | tmp;
 	if (!sock_flag(sk, SOCK_DEAD))
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+		sk->sk_data_ready(sk, 0);
+#else
 		sk->sk_data_ready(sk);
+#endif
 }
 
 static struct percpu_counter *sockets_allocated;
+#ifdef HAVE_ATOMIC_LONG_READ
 static atomic_long_t memory_allocated;
+#else
+static atomic_t memory_allocated;
+#endif
 static struct percpu_counter *orphan_count;
 static int memory_pressure;
 struct proto sdp_proto = {
@@ -2658,7 +2775,11 @@ struct proto sdp_proto = {
 	.enter_memory_pressure = sdp_enter_memory_pressure,
 	.memory_allocated = &memory_allocated,
 	.memory_pressure = &memory_pressure,
+#ifndef HAVE_NETNS_IPV4_SYSCTL_TCP_MEM
 	.sysctl_mem	= sysctl_tcp_mem,
+#else
+	.sysctl_mem = init_net.ipv4.sysctl_tcp_mem,
+#endif
         .sysctl_wmem            = sysctl_tcp_wmem,
         .sysctl_rmem            = sysctl_tcp_rmem,
 	.max_header  = sizeof(struct sdp_bsdh),
@@ -2716,7 +2837,11 @@ static struct proto_ops sdp_ipv6_proto_o
 #endif
 
 static int sdp_create_ipvx_socket(struct net *net, struct socket *sock, int protocol,
+#ifndef HAVE_SK_ALLOC_5_PARAMS
 	       	struct proto_ops *proto_ops)
+#else
+	       	struct proto_ops *proto_ops, int kern)
+#endif
 {
 	struct sock *sk;
 	int rc;
@@ -2737,7 +2862,11 @@ static int sdp_create_ipvx_socket(struct
 		return -EPROTONOSUPPORT;
 	}
 
+#ifndef HAVE_SK_ALLOC_5_PARAMS
 	sk = sk_alloc(net, PF_INET_SDP, GFP_KERNEL, &sdp_proto);
+#else
+	sk = sk_alloc(net, PF_INET_SDP, GFP_KERNEL, &sdp_proto, kern);
+#endif
 	if (!sk) {
 		sdp_warn(NULL, "SDP: failed to allocate socket.\n");
 		return -ENOMEM;
@@ -2775,14 +2904,24 @@ static int sdp_create_ipvx_socket(struct
 static int sdp_create_v6_socket(struct net *net, struct socket *sock, int protocol,
 				int kern)
 {
+#ifndef HAVE_SK_ALLOC_5_PARAMS
 	return sdp_create_ipvx_socket(net, sock, protocol, &sdp_ipv6_proto_ops);
+#else
+	return sdp_create_ipvx_socket(net, sock, protocol, &sdp_ipv6_proto_ops,
+				      kern);
+#endif
 }
 #endif
 
 static int sdp_create_v4_socket(struct net *net, struct socket *sock, int protocol,
 				int kern)
 {
+#ifndef HAVE_SK_ALLOC_5_PARAMS
 	return sdp_create_ipvx_socket(net, sock, protocol, &sdp_ipv4_proto_ops);
+#else
+	return sdp_create_ipvx_socket(net, sock, protocol, &sdp_ipv4_proto_ops,
+				      kern);
+#endif
 }
 
 static void sdp_add_device(struct ib_device *device)
@@ -2950,8 +3089,13 @@ static int __init sdp_init(void)
 	if (!orphan_count)
 		goto no_mem_orphan_count;
 
+#ifdef HAVE_PERCPU_COUNTER_INIT_3_ARGS
 	percpu_counter_init(sockets_allocated, 0, GFP_KERNEL);
 	percpu_counter_init(orphan_count, 0, GFP_KERNEL);
+#else
+	percpu_counter_init(sockets_allocated, 0);
+	percpu_counter_init(orphan_count, 0);
+#endif
 
 	sdp_proto.sockets_allocated = sockets_allocated;
 	sdp_proto.orphan_count = orphan_count;
@@ -3030,9 +3174,15 @@ static void __exit sdp_exit(void)
 
 	BUG_ON(!list_empty(&sock_list));
 
-	if (atomic_long_read(&memory_allocated))
+#ifdef HAVE_ATOMIC_LONG_READ
+		if (atomic_long_read(&memory_allocated))
 		sdp_dbg(NULL, "SDP detected memory leak. Memory_allocated: %ld\n",
 		       atomic_long_read(&memory_allocated));
+#else
+	if (atomic_read(&memory_allocated))
+		printk(KERN_WARNING "%s: current mem usage %d\n", __func__,
+		       atomic_read(&memory_allocated));
+#endif
 
 	if (percpu_counter_sum(sockets_allocated))
 		printk(KERN_WARNING "%s: sockets_allocated %lld\n", __func__,
--- a/drivers/infiniband/ulp/sdp/sdp_proc.c
+++ b/drivers/infiniband/ulp/sdp/sdp_proc.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#include <linux/module.h>
 #include <linux/proc_fs.h>
 #include <linux/debugfs.h>
 #include <rdma/sdp_socket.h>
@@ -179,8 +180,13 @@ static int sdp_v6_seq_show(struct seq_fi
 	__u16 srcp;
 	__u32 rx_queue, tx_queue;
 
+#ifdef HAVE_SK_V6_RCV_SADDR
 	dest = &sk->sk_v6_daddr;
 	src = &sk->sk_v6_rcv_saddr;
+#else
+	dest = &inet6_sk(sk)->daddr;
+	src = &inet6_sk(sk)->rcv_saddr;
+#endif
 	destp = ntohs(sdp_inet_dport(sk));
 	srcp = ntohs(sdp_inet_sport(sk));
 	uid = sock_i_uid(sk);
@@ -230,7 +236,11 @@ out:
 
 static int sdp_seq_open(struct inode *inode, struct file *file)
 {
+#ifdef HAVE_PDE_DATA
 	struct sdp_seq_afinfo *afinfo = (struct sdp_seq_afinfo *)PDE_DATA(inode);
+#else
+	struct sdp_seq_afinfo *afinfo = PDE(inode)->data;
+#endif
 	struct seq_file *seq;
 	struct sdp_iter_state *s;
 	int rc;
@@ -333,12 +343,21 @@ static void sdpstats_seq_hist(struct seq
 	} 							\
 })
 
+#if defined(HAVE_THIS_CPU_PTR) && ! defined(HAVE_GET_CPU_VAR)
 #define __sdpstats_seq_hist(seq, msg, hist, is_log) ({		\
 	int hist_len = ARRAY_SIZE(this_cpu_ptr(&sdpstats)->hist);\
 	memset(h, 0, sizeof(*h) * h_len);			\
 	SDPSTATS_HIST_GET(hist, hist_len, h);	\
 	sdpstats_seq_hist(seq, msg, h, hist_len, is_log);\
 })
+#else
+#define __sdpstats_seq_hist(seq, msg, hist, is_log) ({		\
+	int hist_len = ARRAY_SIZE(__get_cpu_var(sdpstats).hist);\
+	memset(h, 0, sizeof(*h) * h_len);			\
+	SDPSTATS_HIST_GET(hist, hist_len, h);	\
+	sdpstats_seq_hist(seq, msg, h, hist_len, is_log);\
+})
+#endif
 
 #define __sdpstats_seq_hist_pcpu(seq, msg, hist) ({		\
 	unsigned int __i;                                       \
@@ -385,7 +404,11 @@ static int sdpstats_seq_show(struct seq_
 	seq_printf(seq, "memcpy_count       \t\t: %u\n",
 		SDPSTATS_COUNTER_GET(memcpy_count));
 
+#if defined(HAVE_THIS_CPU_PTR) && ! defined(HAVE_GET_CPU_VAR)
         for (i = 0; i < ARRAY_SIZE(this_cpu_ptr(&sdpstats)->post_send); i++) {
+#else
+        for (i = 0; i < ARRAY_SIZE(__get_cpu_var(sdpstats).post_send); i++) {
+#endif
                 if (mid2str(i)) {
                         seq_printf(seq, "post_send %-20s\t: %u\n",
                                         mid2str(i),
@@ -714,12 +737,21 @@ static int sdp_ssk_hist_seq_show(struct
 			ssk->hst_idx, ARRAY_SIZE(ssk->hst));
 
 	seq_printf(seq, "rmem: %d wmem: %d wqueue: %d "
+#ifdef HAVE_ATOMIC_LONG_READ
 			"fw: %d prot->alloc: %ld\n",
 			atomic_read(&sk->sk_rmem_alloc),
 			atomic_read(&sk->sk_wmem_alloc),
 			sk->sk_wmem_queued,
 			sk->sk_forward_alloc,
 			atomic_long_read(sk->sk_prot->memory_allocated));
+#else
+			"fw: %d prot->alloc: %d\n",
+			atomic_read(&sk->sk_rmem_alloc),
+			atomic_read(&sk->sk_wmem_alloc),
+			sk->sk_wmem_queued,
+			sk->sk_forward_alloc,
+			atomic_read(sk->sk_prot->memory_allocated));
+#endif
 
 	for (i = 0; i < min(ssk->hst_idx, ARRAY_SIZE(ssk->hst)); ++i) {
 		struct sdp_sock_hist *hst = &ssk->hst[i];
--- a/drivers/infiniband/ulp/sdp/sdp_rx.c
+++ b/drivers/infiniband/ulp/sdp/sdp_rx.c
@@ -198,7 +198,7 @@ static int sdp_post_recv(struct sdp_sock
 			pages_alloced++;
 		}
 		frag = &skb_shinfo(skb)->frags[i];
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 2, 0))
+#ifndef HAVE_SKB_FRAG_STRUCT_PAGE_P
 		frag->page                = page;
 #else
 		frag->page.p              = page;
@@ -208,11 +208,11 @@ static int sdp_post_recv(struct sdp_sock
 		++skb_shinfo(skb)->nr_frags;
 	}
 	skb->truesize += ssk->recv_frags * min(PAGE_SIZE, SDP_MAX_PAYLOAD);
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 6, 0))
-	if (!sk_rmem_schedule(sk, ssk->recv_frags * min(PAGE_SIZE,
+#ifdef HAVE_SK_RMEM_SCHEDULE_3P
+	if (!sk_rmem_schedule(sk, skb, ssk->recv_frags * min(PAGE_SIZE,
 		SDP_MAX_PAYLOAD))) {
 #else
-	if (!sk_rmem_schedule(sk, skb, ssk->recv_frags * min(PAGE_SIZE,
+	if (!sk_rmem_schedule(sk, ssk->recv_frags * min(PAGE_SIZE,
 		SDP_MAX_PAYLOAD))) {
 #endif
 		sdp_dbg(sk, "RX couldn't post, rx posted = %d.",
@@ -237,7 +237,7 @@ static int sdp_post_recv(struct sdp_sock
 		if (rx_req->mapping[i + 1]) {
 			addr = rx_req->mapping[i + 1];
 		} else {
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 2, 0))
+#ifndef HAVE_SKB_FRAG_STRUCT_PAGE_P
 			addr = ib_dma_map_page(dev,
 					skb_shinfo(skb)->frags[i].page,
 #else
@@ -371,7 +371,11 @@ mid_data:
 	skb_queue_tail(&sk->sk_receive_queue, skb);
 
 	if (!sock_flag(sk, SOCK_DEAD))
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+		sk->sk_data_ready(sk, 0);
+#else
 		sk->sk_data_ready(sk);
+#endif
 	return skb;
 }
 
@@ -580,7 +584,7 @@ static int sdp_process_rx_skb(struct sdp
 	skb_shinfo(skb)->nr_frags = pagesz / PAGE_SIZE;
 
 	for (i = skb_shinfo(skb)->nr_frags; i < frags; ++i) {
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 2, 0))
+#ifndef HAVE_SKB_FRAG_STRUCT_PAGE_P
 		put_page(skb_shinfo(skb)->frags[i].page);
 #else
 		put_page(skb_shinfo(skb)->frags[i].page.p);
@@ -724,14 +728,18 @@ static void sdp_bzcopy_write_space(struc
 	if (skwq_has_sleeper(wq))
 		wake_up_interruptible(&wq->wait);
 	if (wq && wq->fasync_list && !(sk->sk_shutdown & SEND_SHUTDOWN))
+#ifndef HAVE_SOCK_WAKE_ASYNC_TAKES_SOCKET_WQ
 		sock_wake_async(sock, 2, POLL_OUT);
+#else /* HAVE_SOCK_WAKE_ASYNC_TAKES_SOCKET_WQ */
+		sock_wake_async(wq, 2, POLL_OUT);
+#endif /* HAVE_SOCK_WAKE_ASYNC_TAKES_SOCKET_WQ */
 	rcu_read_unlock();
-#else
+#else /* CONFIG_COMPAT_IS_SOCK_SK_WQ */
 	if (sk_sleep(sk) && waitqueue_active(sk_sleep(sk)))
 		wake_up_interruptible(sk_sleep(sk));
 	if (sock->fasync_list && !(sk->sk_shutdown & SEND_SHUTDOWN))
 		sock_wake_async(sock, 2, POLL_OUT);
-#endif
+#endif /* CONFIG_COMPAT_IS_SOCK_SK_WQ */
 }
 
 int sdp_poll_rx_cq(struct sdp_sock *ssk)
--- a/drivers/infiniband/ulp/sdp/sdp_tx.c
+++ b/drivers/infiniband/ulp/sdp/sdp_tx.c
@@ -151,7 +151,7 @@ void sdp_post_send(struct sdp_sock *ssk,
 		frags = skb_shinfo(skb)->nr_frags;
 		for (i = 0; i < frags; ++i) {
 			++sge;
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 2, 0))
+#ifndef HAVE_SKB_FRAG_STRUCT_PAGE_P
 			addr = ib_dma_map_page(dev,
 					skb_shinfo(skb)->frags[i].page,
 #else
