From: Feras Daoud <ferasda@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_rx.c

Change-Id: I1611a484a4b80477fe711adc96f8cddd74b29a2b
---
 drivers/net/ethernet/mellanox/mlx5/core/en_rx.c | 297 +++++++++++++++++++++++-
 1 file changed, 295 insertions(+), 2 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -35,7 +35,9 @@
 #include <linux/ipv6.h>
 #include <linux/tcp.h>
 #include <linux/bpf_trace.h>
+#ifdef CONFIG_NET_RX_BUSY_POLL
 #include <net/busy_poll.h>
+#endif
 #include "en.h"
 #include "en_tc.h"
 #include "eswitch.h"
@@ -190,7 +192,11 @@ static inline bool mlx5e_rx_cache_is_emp
 static inline bool mlx5e_rx_cache_page_busy(struct mlx5e_page_cache *cache,
 					    u32 i)
 {
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 	return page_ref_count(cache->page_cache[i].page) != 1;
+#else
+	return atomic_read(&cache->page_cache[i].page->_count) != 1;
+#endif
 }
 
 static inline bool mlx5e_rx_cache_check_reduce(struct mlx5e_rq *rq)
@@ -272,7 +278,11 @@ static inline bool mlx5e_rx_cache_extend
 
 static inline bool mlx5e_page_is_reserved(struct page *page)
 {
+#ifdef HAVE_PAGE_IS_PFMEMALLOC
 	return page_is_pfmemalloc(page) || page_to_nid(page) != numa_node_id();
+#else
+	return page_to_nid(page) != numa_node_id();
+#endif
 }
 
 static inline bool mlx5e_rx_cache_put(struct mlx5e_rq *rq,
@@ -292,8 +302,10 @@ static inline bool mlx5e_rx_cache_put(st
 		}
 	}
 
+#ifdef HAVE_PAGE_IS_PFMEMALLOC
 	if (unlikely(page_is_pfmemalloc(dma_info->page)))
 		return false;
+#endif
 
 	cache->page_cache[++cache->head] = *dma_info;
 	return true;
@@ -505,7 +517,11 @@ static int mlx5e_alloc_rx_umr_mpwqe(stru
 		if (unlikely(err))
 			goto err_unmap;
 		wi->umr.mtt[i] = cpu_to_be64(dma_info->addr | MLX5_EN_WR);
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 		page_ref_add(dma_info->page, pg_strides);
+#else
+		atomic_add(pg_strides, &dma_info->page->_count);
+#endif
 		wi->skbs_frags[i] = 0;
 	}
 
@@ -518,7 +534,11 @@ err_unmap:
 	while (--i >= 0) {
 		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
 
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 		page_ref_sub(dma_info->page, pg_strides);
+#else
+		atomic_sub(pg_strides, &dma_info->page->_count);
+#endif
 		mlx5e_page_release(rq, dma_info, true);
 	}
 
@@ -533,7 +553,11 @@ void mlx5e_free_rx_mpwqe(struct mlx5e_rq
 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
 		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
 
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 		page_ref_sub(dma_info->page, pg_strides - wi->skbs_frags[i]);
+#else
+		atomic_sub(pg_strides - wi->skbs_frags[i], &dma_info->page->_count);
+#endif
 		mlx5e_page_release(rq, dma_info, true);
 	}
 }
@@ -553,7 +577,11 @@ void mlx5e_post_rx_mpwqe(struct mlx5e_rq
 	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_ll_update_db_record(wq);
 
@@ -606,7 +634,11 @@ bool mlx5e_post_rx_wqes(struct mlx5e_rq
 	}
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_ll_update_db_record(wq);
 
@@ -671,16 +703,22 @@ static void mlx5e_lro_update_hdr(struct
 	}
 }
 
+#ifdef HAVE_NETIF_F_RXHASH
 static inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,
 				      struct sk_buff *skb)
 {
+#ifdef HAVE_SKB_SET_HASH
 	u8 cht = cqe->rss_hash_type;
 	int ht = (cht & CQE_RSS_HTYPE_L4) ? PKT_HASH_TYPE_L4 :
 		 (cht & CQE_RSS_HTYPE_IP) ? PKT_HASH_TYPE_L3 :
 					    PKT_HASH_TYPE_NONE;
 	skb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);
+#else
+	skb->rxhash = be32_to_cpu(cqe->rss_hash_result);
+#endif
 }
 
+#endif
 static inline bool is_first_ethertype_ip(struct sk_buff *skb)
 {
 	__be16 ethertype = ((struct ethhdr *)skb->data)->h_proto;
@@ -713,8 +751,12 @@ static inline void mlx5e_handle_csum(str
 		   (cqe->hds_ip_ext & CQE_L4_OK))) {
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 		if (cqe_is_tunneled(cqe)) {
+#ifdef HAVE_SK_BUFF_CSUM_LEVEL
 			skb->csum_level = 1;
+#endif
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 			skb->encapsulation = 1;
+#endif
 			rq->stats.csum_unnecessary_inner++;
 		}
 		return;
@@ -731,6 +773,10 @@ static inline void mlx5e_build_rx_skb(st
 {
 	struct net_device *netdev = rq->netdev;
 	int lro_num_seg;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	u8 l4_hdr_type;
+#endif
 
 	lro_num_seg = be32_to_cpu(cqe->srqn) >> 24;
 	if (lro_num_seg > 1) {
@@ -742,6 +788,16 @@ static inline void mlx5e_build_rx_skb(st
 		rq->stats.packets += lro_num_seg - 1;
 		rq->stats.lro_packets++;
 		rq->stats.lro_bytes += cqe_bcnt;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
+		/* Flush GRO to avoid OOO packets, since GSO bypasses the
+		 * GRO queue. This was fixed in dev_gro_receive() in kernel 4.10
+		 */
+#ifdef NAPI_GRO_FLUSH_2_PARAMS
+		napi_gro_flush(rq->cq.napi, false);
+#else
+		napi_gro_flush(rq->cq.napi);
+#endif
+#endif
 	}
 
 	if (unlikely(mlx5e_rx_hw_stamp(rq->tstamp)))
@@ -750,16 +806,31 @@ static inline void mlx5e_build_rx_skb(st
 
 	skb_record_rx_queue(skb, rq->ix);
 
+#ifdef HAVE_NETIF_F_RXHASH
 	if (likely(netdev->features & NETIF_F_RXHASH))
 		mlx5e_skb_set_hash(cqe, skb);
+#endif
 
 	if (cqe_has_vlan(cqe))
+#ifndef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
+		__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->vlan_info));
+#else
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
 				       be16_to_cpu(cqe->vlan_info));
+#endif
 
 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	l4_hdr_type = get_cqe_l4_hdr_type(cqe);
+	mlx5e_handle_csum(netdev, cqe, rq, skb,
+			  !!lro_num_seg ||
+			  (IS_SW_LRO(&priv->channels.params) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_NONE) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_UDP)));
+#else
 	mlx5e_handle_csum(netdev, cqe, rq, skb, !!lro_num_seg);
+#endif
 
 	skb->protocol = eth_type_trans(skb, netdev);
 	if (unlikely(mlx5_get_cqe_ft(cqe) ==
@@ -777,6 +848,7 @@ static inline void mlx5e_complete_rx_cqe
 	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 {
 	struct mlx5_wq_cyc *wq = &sq->wq;
@@ -790,7 +862,12 @@ static inline void mlx5e_xmit_xdp_doorbe
 
 static inline bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq,
 					struct mlx5e_dma_info *di,
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 					const struct xdp_buff *xdp)
+#else
+					unsigned int data_offset,
+					int len)
+#endif
 {
 	struct mlx5e_xdpsq       *sq   = &rq->xdpsq;
 	struct mlx5_wq_cyc       *wq   = &sq->wq;
@@ -801,8 +878,11 @@ static inline bool mlx5e_xmit_xdp_frame(
 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
 	struct mlx5_wqe_data_seg *dseg;
 
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	ptrdiff_t data_offset = xdp->data - xdp->data_hard_start;
+#endif
 	dma_addr_t dma_addr  = di->addr + data_offset;
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	unsigned int dma_len = xdp->data_end - xdp->data;
 
 	prefetchw(wqe);
@@ -812,6 +892,11 @@ static inline bool mlx5e_xmit_xdp_frame(
 		rq->stats.xdp_drop++;
 		return false;
 	}
+#else
+	unsigned int dma_len = len - MLX5E_XDP_MIN_INLINE;
+	void *data           = page_address(di->page) + data_offset;
+
+#endif
 
 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, 1))) {
 		if (sq->db.doorbell) {
@@ -831,7 +916,11 @@ static inline bool mlx5e_xmit_xdp_frame(
 
 	/* copy the inline part if required */
 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 		memcpy(eseg->inline_hdr.start, xdp->data, MLX5E_XDP_MIN_INLINE);
+#else
+		memcpy(eseg->inline_hdr.start, data, MLX5E_XDP_MIN_INLINE);
+#endif
 		eseg->inline_hdr.sz = cpu_to_be16(MLX5E_XDP_MIN_INLINE);
 		dma_len  -= MLX5E_XDP_MIN_INLINE;
 		dma_addr += MLX5E_XDP_MIN_INLINE;
@@ -858,40 +947,103 @@ static inline bool mlx5e_xmit_xdp_frame(
 }
 
 /* returns true if packet was consumed by xdp */
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 static inline int mlx5e_xdp_handle(struct mlx5e_rq *rq,
 				   struct mlx5e_dma_info *di,
 				   void *va, u16 *rx_headroom, u32 *len)
+#else
+static inline bool mlx5e_xdp_handle(struct mlx5e_rq *rq,
+				    const struct bpf_prog *prog,
+				    struct mlx5e_dma_info *di,
+				    void *data, u16 len)
+#endif
 {
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	const struct bpf_prog *prog = READ_ONCE(rq->xdp_prog);
+#endif
 	struct xdp_buff xdp;
 	u32 act;
 
 	if (!prog)
 		return false;
 
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	xdp.data = va + *rx_headroom;
 	xdp.data_end = xdp.data + *len;
 	xdp.data_hard_start = va;
+#else
+	xdp.data = data;
+	xdp.data_end = xdp.data + len;
+#endif
 
 	act = bpf_prog_run_xdp(prog, &xdp);
 	switch (act) {
 	case XDP_PASS:
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 		*rx_headroom = xdp.data - xdp.data_hard_start;
 		*len = xdp.data_end - xdp.data;
+#endif
 		return false;
 	case XDP_TX:
+#ifdef HAVE_TRACE_XDP_EXCEPTION
 		if (unlikely(!mlx5e_xmit_xdp_frame(rq, di, &xdp)))
 			trace_xdp_exception(rq->netdev, prog, act);
+#else
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
+		mlx5e_xmit_xdp_frame(rq, di, &xdp);
+#else
+		mlx5e_xmit_xdp_frame(rq, di, MLX5_RX_HEADROOM, len);
+#endif
+#endif
 		return true;
 	default:
 		bpf_warn_invalid_xdp_action(act);
 	case XDP_ABORTED:
+#ifdef HAVE_TRACE_XDP_EXCEPTION
 		trace_xdp_exception(rq->netdev, prog, act);
+#endif
 	case XDP_DROP:
 		rq->stats.xdp_drop++;
 		return true;
 	}
 }
+#endif /* HAVE_NETDEV_XDP */
+
+#ifndef HAVE_BUILD_SKB
+static inline struct sk_buff *mlx5e_compat_build_skb(struct mlx5e_rq *rq,
+						struct mlx5_cqe64 *cqe,
+						struct page *page,
+						u32 cqe_bcnt,
+						unsigned int offset)
+{
+	u16 headlen = min_t(u32, MLX5_MPWRQ_SMALL_PACKET_THRESHOLD, cqe_bcnt);
+	u32 frag_size = cqe_bcnt - headlen;
+	struct sk_buff *skb;
+	void *head_ptr = page_address(page) + offset + rq->rx_headroom;
+
+	skb = netdev_alloc_skb(rq->netdev, headlen + rq->rx_headroom);
+	if (unlikely(!skb))
+		return NULL;
+
+	if (frag_size) {
+		u32 frag_offset = offset + rq->rx_headroom + headlen;
+		unsigned int truesize =	SKB_TRUESIZE(frag_size);
+
+		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+				page, frag_offset,
+				frag_size, truesize);
+	}
+
+	/* copy header */
+	skb_reserve(skb, rq->rx_headroom);
+	skb_copy_to_linear_data(skb, head_ptr, headlen);
+
+	/* skb linear part was allocated with headlen and aligned to long */
+	skb->tail += headlen;
+	skb->len  += headlen;
+	return skb;
+}
+#endif
 
 static inline
 struct sk_buff *skb_from_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
@@ -901,7 +1053,9 @@ struct sk_buff *skb_from_cqe(struct mlx5
 	struct sk_buff *skb;
 	void *va, *data;
 	u16 rx_headroom = rq->rx_headroom;
+#ifdef HAVE_NETDEV_XDP
 	bool consumed;
+#endif
 	u32 frag_size;
 
 	va             = page_address(di->page) + wi->offset;
@@ -920,29 +1074,53 @@ struct sk_buff *skb_from_cqe(struct mlx5
 		return NULL;
 	}
 
+#ifdef HAVE_NETDEV_XDP
 	rcu_read_lock();
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	consumed = mlx5e_xdp_handle(rq, di, va, &rx_headroom, &cqe_bcnt);
+#else
+	consumed = mlx5e_xdp_handle(rq, READ_ONCE(rq->xdp_prog), di, data,
+				    cqe_bcnt);
+#endif
 	rcu_read_unlock();
 	if (consumed)
 		return NULL; /* page/packet was consumed by XDP */
+#endif
 
+#ifdef HAVE_BUILD_SKB
 	skb = build_skb(va, frag_size);
+#else
+	skb = mlx5e_compat_build_skb(rq, cqe, di->page, cqe_bcnt,
+				     wi->offset - frag_size);
+#endif
 	if (unlikely(!skb)) {
 		rq->stats.buff_alloc_err++;
 		return NULL;
 	}
 
 	/* queue up for recycling/reuse */
+#ifndef HAVE_BUILD_SKB
+	if (skb_shinfo(skb)->nr_frags)
+#endif
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 	page_ref_inc(di->page);
+#else
+	atomic_inc(&di->page->_count);
+#endif
 
+#ifdef HAVE_BUILD_SKB
 	skb_reserve(skb, rx_headroom);
 	skb_put(skb, cqe_bcnt);
+#endif
 
 	return skb;
 }
 
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX) || defined(CONFIG_COMPAT_LRO_ENABLED_IPOIB)
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	struct mlx5e_wqe_frag_info *wi;
 	struct mlx5e_rx_wqe *wqe;
 	__be16 wqe_counter_be;
@@ -971,6 +1149,31 @@ void mlx5e_handle_rx_cqe(struct mlx5e_rq
 	}
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+		if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+			lro_vlan_hwaccel_receive_skb(&rq->sw_lro.lro_mgr,
+						     skb, priv->channels.params.vlan_grp,
+						     be16_to_cpu(cqe->vlan_info),
+						     NULL);
+		else
+#endif
+		lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+	else
+#endif
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+                if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+                        vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+                                         be16_to_cpu(cqe->vlan_info),
+                                         skb);
+#else
+                        vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+                                        be16_to_cpu(cqe->vlan_info));
+#endif
+		else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 	mlx5e_free_rx_wqe_reuse(rq, wi);
@@ -982,11 +1185,15 @@ wq_ll_pop:
 #ifdef CONFIG_MLX5_ESWITCH
 void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#if defined(HAVE_SKB_VLAN_POP) || defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
 	struct net_device *netdev = rq->netdev;
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#ifdef HAVE_SKB_VLAN_POP
 	struct mlx5e_rep_priv *rpriv  = priv->ppriv;
 	struct mlx5_eswitch_rep *rep = rpriv->rep;
 	struct mlx5e_rep_context *context;
+#endif
+#endif
 	struct mlx5e_wqe_frag_info *wi;
 	struct mlx5e_rx_wqe *wqe;
 	struct sk_buff *skb;
@@ -1015,10 +1222,24 @@ void mlx5e_handle_rx_cqe_rep(struct mlx5
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
 
+#ifdef HAVE_SKB_VLAN_POP
 	context = mlx5e_rep_to_context(rep);
 	if (context->vlan && skb_vlan_tag_present(skb))
 		skb_vlan_pop(skb);
+#endif
 
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+	if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+		vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+				 be16_to_cpu(cqe->vlan_info),
+				 skb);
+#else
+	vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+				 be16_to_cpu(cqe->vlan_info));
+#endif
+	else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 	mlx5e_free_rx_wqe_reuse(rq, wi);
@@ -1069,6 +1290,9 @@ static inline void mlx5e_mpwqe_fill_rx_s
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[wqe_id];
 	struct mlx5e_rx_wqe  *wqe = mlx5_wq_ll_get_wqe(&rq->wq, wqe_id);
@@ -1087,9 +1311,14 @@ void mlx5e_handle_rx_cqe_mpwrq(struct ml
 		goto mpwrq_cqe_out;
 	}
 
+#ifdef HAVE_NAPI_ALLOC_SKB
 	skb = napi_alloc_skb(rq->cq.napi,
 			     ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
 				   sizeof(long)));
+#else
+	skb = netdev_alloc_skb_ip_align(rq->netdev, ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
+			   sizeof(long)));
+#endif
 	if (unlikely(!skb)) {
 		rq->stats.buff_alloc_err++;
 		goto mpwrq_cqe_out;
@@ -1100,6 +1329,31 @@ void mlx5e_handle_rx_cqe_mpwrq(struct ml
 
 	mlx5e_mpwqe_fill_rx_skb(rq, cqe, wi, cqe_bcnt, skb);
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+		if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+			lro_vlan_hwaccel_receive_skb(&rq->sw_lro.lro_mgr,
+						     skb, priv->channels.params.vlan_grp,
+						     be16_to_cpu(cqe->vlan_info),
+						     NULL);
+		else
+#endif
+		lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+	else
+#endif
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+                if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+                        vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+                                         be16_to_cpu(cqe->vlan_info),
+                                         skb);
+#else
+                        vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+                                        be16_to_cpu(cqe->vlan_info));
+#endif
+		else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 mpwrq_cqe_out:
@@ -1113,8 +1367,17 @@ mpwrq_cqe_out:
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
 {
 	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
+#ifdef HAVE_NETDEV_XDP
 	struct mlx5e_xdpsq *xdpsq = &rq->xdpsq;
+#endif
 	int work_done = 0;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv;
+	if (MLX5_CAP_GEN(cq->mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
+		priv = mlx5i_epriv(rq->netdev);
+	else
+		priv = netdev_priv(rq->netdev);
+#endif
 
 	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))
 		return 0;
@@ -1140,16 +1403,23 @@ int mlx5e_poll_rx_cq(struct mlx5e_cq *cq
 		rq->handle_rx_cqe(rq, cqe);
 	}
 
+#ifdef HAVE_NETDEV_XDP
 	if (xdpsq->db.doorbell) {
 		mlx5e_xmit_xdp_doorbell(xdpsq);
 		xdpsq->db.doorbell = false;
 	}
+#endif
 
 	mlx5_cqwq_update_db_record(&cq->wq);
 
 	/* ensure cq space is freed before enabling more cqes */
 	wmb();
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+		lro_flush_all(&rq->sw_lro.lro_mgr);
+#endif
+
 	return work_done;
 }
 
@@ -1241,6 +1511,9 @@ static inline void mlx5i_complete_rx_cqe
 	u32 qpn;
 	u8 *dgid;
 	u8 g;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+       struct mlx5e_priv *parent_priv = mlx5i_epriv(rq->netdev);
+#endif
 
 	qpn = be32_to_cpu(cqe->sop_drop_qpn) & 0xffffff;
 	netdev = mlx5i_get_qpn_netdev(rq->netdev, qpn);
@@ -1272,8 +1545,19 @@ static inline void mlx5i_complete_rx_cqe
 
 	skb->protocol = *((__be16 *)(skb->data));
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (parent_priv->netdev->features & NETIF_F_LRO) {
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	} else {
+		skb->ip_summed = CHECKSUM_COMPLETE;
+		skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
+		rq->stats.csum_complete++;
+	}
+#else
 	skb->ip_summed = CHECKSUM_COMPLETE;
 	skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
+	rq->stats.csum_complete++;
+#endif
 
 	priv = mlx5i_epriv(netdev);
 	tstamp = &priv->tstamp;
@@ -1284,8 +1568,10 @@ static inline void mlx5i_complete_rx_cqe
 
 	skb_record_rx_queue(skb, rq->ix);
 
+#ifdef HAVE_NETIF_F_RXHASH
 	if (likely(netdev->features & NETIF_F_RXHASH))
 		mlx5e_skb_set_hash(cqe, skb);
+#endif
 
 	/* 20 bytes of ipoib header and 4 for encap existing */
 	pseudo_header = skb_push(skb, MLX5_IPOIB_PSEUDO_LEN);
@@ -1295,7 +1581,6 @@ static inline void mlx5i_complete_rx_cqe
 
 	skb->dev = netdev;
 
-	rq->stats.csum_complete++;
 	rq->stats.packets++;
 	rq->stats.bytes += cqe_bcnt;
 }
@@ -1308,6 +1593,9 @@ void mlx5i_handle_rx_cqe(struct mlx5e_rq
 	struct sk_buff *skb;
 	u16 wqe_counter;
 	u32 cqe_bcnt;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = mlx5i_epriv(rq->netdev);
+#endif
 
 	wqe_counter_be = cqe->wqe_counter;
 	wqe_counter    = be16_to_cpu(wqe_counter_be);
@@ -1324,7 +1612,12 @@ void mlx5i_handle_rx_cqe(struct mlx5e_rq
 		dev_kfree_skb_any(skb);
 		goto wq_free_wqe;
 	}
-	napi_gro_receive(rq->cq.napi, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (priv->netdev->features & NETIF_F_LRO)
+		lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+	else
+#endif
+		napi_gro_receive(rq->cq.napi, skb);
 
 wq_free_wqe:
 	mlx5e_free_rx_wqe_reuse(rq, wi);
