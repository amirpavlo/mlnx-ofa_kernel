From: Vladimir Neyelov <vladimirn@mellanox.com>
Subject: [PATCH] BACKPORT: ib_iser

Change-Id: Ib85ca82399e22f4c6d053b764ce79ff4aefb25d9
Signed-off-by: Vladimir Neyelov <vladimirn@mellanox.com>
---
 drivers/infiniband/ulp/iser/iscsi_iser.c     |   86 ++++++
 drivers/infiniband/ulp/iser/iscsi_iser.h     |   31 +++
 drivers/infiniband/ulp/iser/iser_initiator.c |   73 +++++-
 drivers/infiniband/ulp/iser/iser_memory.c    |  367 +++++++++++++++++++++++++-
 drivers/infiniband/ulp/iser/iser_verbs.c     |    2 +
 5 files changed, 552 insertions(+), 7 deletions(-)

--- a/drivers/infiniband/ulp/iser/iscsi_iser.c
+++ b/drivers/infiniband/ulp/iser/iscsi_iser.c
@@ -108,7 +108,11 @@ MODULE_PARM_DESC(pi_enable, "Enable T10-
 
 int iser_pi_guard;
 module_param_named(pi_guard, iser_pi_guard, int, S_IRUGO);
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 MODULE_PARM_DESC(pi_guard, "T10-PI guard_type [deprecated]");
+#else
+MODULE_PARM_DESC(pi_guard, "T10-PI guard_type, 0:CRC|1:IP_CSUM (default:IP_CSUM)");
+#endif
 
 /*
  * iscsi_iser_recv() - Process a successful recv completion
@@ -220,6 +224,27 @@ out:
 	return ret;
 }
 
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 3, 0)
+/**
+ * set_last_ping_on_nopout_task()
+ * @task: iscsi task
+ *
+ * Workaround libiscsi not setting iscsi_conn->last_ping
+ * in case of failure.
+ * fixed in kernels > 4.3
+ */
+static inline void
+set_last_ping_on_nopout_task(struct iscsi_task *task)
+{
+	u8 task_opcode = (task->hdr->opcode & ISCSI_OPCODE_MASK);
+
+	if (task_opcode == ISCSI_OP_NOOP_OUT)
+		task->conn->last_ping = jiffies;
+}
+#endif
+
+
 /**
  * iscsi_iser_task_init() - Initialize iscsi-iser task
  * @task: iscsi task
@@ -237,8 +262,18 @@ iscsi_iser_task_init(struct iscsi_task *
 
 	ret = iser_initialize_task_headers(task, &iser_task->desc);
 	if (ret) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 3, 0)
+		u8 task_opcode = (task->hdr->opcode & ISCSI_OPCODE_MASK);
+
+		iser_err("Failed to init task %p, opcode %d, err = %d",
+			 iser_task, task_opcode, ret);
+
+		set_last_ping_on_nopout_task(task);
+#else
 		iser_err("Failed to init task %p, err = %d\n",
 			 iser_task, ret);
+#endif
+
 		return ret;
 	}
 
@@ -272,6 +307,17 @@ iscsi_iser_mtask_xmit(struct iscsi_conn
 	iser_dbg("mtask xmit [cid %d itt 0x%x]\n", conn->id, task->itt);
 
 	error = iser_send_control(conn, task);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 3, 0)
+	if (error) {
+		u8 task_opcode = (task->hdr->opcode & ISCSI_OPCODE_MASK);
+
+		iser_err("Failed to send task %p, opcode %d, err = %d",
+			 task->dd_data, task_opcode, error);
+
+		set_last_ping_on_nopout_task(task);
+
+	}
+#endif
 
 	/* since iser xmits control with zero copy, tasks can not be recycled
 	 * right after sending them.
@@ -390,6 +436,7 @@ static void iscsi_iser_cleanup_task(stru
 	}
 }
 
+#ifdef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
 /**
  * iscsi_iser_check_protection() - check protection information status of task.
  * @task:     iscsi task
@@ -414,6 +461,7 @@ iscsi_iser_check_protection(struct iscsi
 		return iser_check_task_pi_status(iser_task, ISER_DIR_OUT,
 						 sector);
 }
+#endif
 
 /**
  * iscsi_iser_conn_create() - create a new iscsi-iser connection
@@ -647,8 +695,15 @@ iscsi_iser_session_create(struct iscsi_e
 			u32 sig_caps = ib_conn->device->ib_device->attrs.sig_prot_cap;
 
 			scsi_host_set_prot(shost, iser_dif_prot_caps(sig_caps));
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 			scsi_host_set_guard(shost, SHOST_DIX_GUARD_IP |
 						   SHOST_DIX_GUARD_CRC);
+#else
+			if (iser_pi_guard)
+				scsi_host_set_guard(shost, SHOST_DIX_GUARD_IP);
+			else
+				scsi_host_set_guard(shost, SHOST_DIX_GUARD_CRC);
+#endif
 		}
 
 		if (iscsi_host_add(shost,
@@ -762,7 +817,13 @@ iscsi_iser_conn_get_stats(struct iscsi_c
 	stats->r2t_pdus = conn->r2t_pdus_cnt; /* always 0 */
 	stats->tmfcmd_pdus = conn->tmfcmd_pdus_cnt;
 	stats->tmfrsp_pdus = conn->tmfrsp_pdus_cnt;
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 	stats->custom_length = 0;
+#else
+	stats->custom_length = 1;
+        strcpy(stats->custom[0].desc, "fmr_unalign_cnt");
+        stats->custom[0].value = conn->fmr_unalign_cnt;
+#endif
 }
 
 static int iscsi_iser_get_ep_param(struct iscsi_endpoint *ep,
@@ -957,7 +1018,9 @@ static umode_t iser_attr_is_visible(int
 		case ISCSI_PARAM_TGT_RESET_TMO:
 		case ISCSI_PARAM_IFACE_NAME:
 		case ISCSI_PARAM_INITIATOR_NAME:
+#if defined(CONFIG_ISER_DISCOVERY)
 		case ISCSI_PARAM_DISCOVERY_SESS:
+#endif
 			return S_IRUGO;
 		default:
 			return 0;
@@ -967,6 +1030,7 @@ static umode_t iser_attr_is_visible(int
 	return 0;
 }
 
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 static int iscsi_iser_slave_alloc(struct scsi_device *sdev)
 {
 	struct iscsi_session *session;
@@ -982,12 +1046,17 @@ static int iscsi_iser_slave_alloc(struct
 
 	return 0;
 }
+#endif
 
 static struct scsi_host_template iscsi_iser_sht = {
 	.module                 = THIS_MODULE,
 	.name                   = "iSCSI Initiator over iSER",
 	.queuecommand           = iscsi_queuecommand,
+#ifdef HAVE_SCSI_CHANGE_QUEUE_DEPTH
 	.change_queue_depth	= scsi_change_queue_depth,
+#else
+	.change_queue_depth	= iscsi_change_queue_depth,
+#endif
 	.sg_tablesize           = ISCSI_ISER_DEF_SG_TABLESIZE,
 	.cmd_per_lun            = ISER_DEF_CMD_PER_LUN,
 	.eh_abort_handler       = iscsi_eh_abort,
@@ -995,16 +1064,24 @@ static struct scsi_host_template iscsi_i
 	.eh_target_reset_handler = iscsi_eh_recover_target,
 	.target_alloc		= iscsi_target_alloc,
 	.use_clustering         = ENABLE_CLUSTERING,
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 	.slave_alloc            = iscsi_iser_slave_alloc,
+#endif
 	.proc_name              = "iscsi_iser",
 	.this_id                = -1,
+#ifdef HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 	.track_queue_depth	= 1,
+#endif
 };
 
 static struct iscsi_transport iscsi_iser_transport = {
 	.owner                  = THIS_MODULE,
 	.name                   = "iser",
+#if defined(CONFIG_ISER_DISCOVERY)
 	.caps                   = CAP_RECOVERY_L0 | CAP_MULTI_R2T | CAP_TEXT_NEGO,
+#else
+	.caps                   = CAP_RECOVERY_L0 | CAP_MULTI_R2T,
+#endif
 	/* session management */
 	.create_session         = iscsi_iser_session_create,
 	.destroy_session        = iscsi_iser_session_destroy,
@@ -1029,7 +1106,9 @@ static struct iscsi_transport iscsi_iser
 	.xmit_task		= iscsi_iser_task_xmit,
 	.cleanup_task		= iscsi_iser_cleanup_task,
 	.alloc_pdu		= iscsi_iser_pdu_alloc,
+#ifdef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
 	.check_protection	= iscsi_iser_check_protection,
+#endif
 	/* recovery */
 	.session_recovery_timedout = iscsi_session_recovery_timedout,
 
@@ -1049,6 +1128,13 @@ static int __init iser_init(void)
 		return -EINVAL;
 	}
 
+#ifndef HAVE_SCSI_CMND_PROT_FLAGS
+	if (iser_pi_guard < 0 || iser_pi_guard > 1) {
+		iser_err("Invalid pi_guard value of %d\n", iser_pi_guard);
+		return -EINVAL;
+	}
+#endif
+
 	memset(&ig, 0, sizeof(struct iser_global));
 
 	ig.desc_cache = kmem_cache_create("iser_descriptors",
--- a/drivers/infiniband/ulp/iser/iscsi_iser.h
+++ b/drivers/infiniband/ulp/iser/iscsi_iser.h
@@ -68,10 +68,37 @@
 #include <rdma/ib_fmr_pool.h>
 #include <rdma/rdma_cm.h>
 
+#if defined(CONFIG_COMPAT_RHEL_7_3) || defined(CONFIG_COMPAT_RHEL_7_2)
+#undef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+#endif
+
+/* WA for #963642: Traffic with signature fails on dm */
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
+#undef HAVE_SCSI_CMND_PROT_FLAGS
+#endif
+
 #define DRV_NAME	"iser"
 #define PFX		DRV_NAME ": "
 #define DRV_VER		"1.6"
 
+#ifndef HAVE_SCSI_TRANSFER_LENGTH
+static inline unsigned scsi_transfer_length(struct scsi_cmnd *scmd)
+{
+	unsigned int xfer_len = scsi_bufflen(scmd);
+	unsigned int prot_op = scsi_get_prot_op(scmd);
+	unsigned int sector_size = scmd->device->sector_size;
+
+	switch (prot_op) {
+	case SCSI_PROT_NORMAL:
+	case SCSI_PROT_WRITE_STRIP:
+	case SCSI_PROT_READ_INSERT:
+		return xfer_len;
+	}
+
+	return xfer_len + (xfer_len >> ilog2(sector_size)) * 8;
+}
+#endif
+
 #define iser_dbg(fmt, arg...)				 \
 	do {						 \
 		if (unlikely(iser_debug_level > 2))	 \
@@ -206,6 +233,10 @@ struct iser_data_buf {
 	int                size;
 	unsigned long      data_len;
 	unsigned int       dma_nents;
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+	struct scatterlist *orig_sg;
+	unsigned int       orig_size;
+#endif
 };
 
 /* fwd declarations */
--- a/drivers/infiniband/ulp/iser/iser_initiator.c
+++ b/drivers/infiniband/ulp/iser/iser_initiator.c
@@ -322,7 +322,9 @@ static int iser_post_rx_bufs(struct iscs
 {
 	struct iser_conn *iser_conn = conn->dd_data;
 	struct ib_conn *ib_conn = &iser_conn->ib_conn;
+#if defined(CONFIG_ISER_DISCOVERY)
 	struct iscsi_session *session = conn->session;
+#endif
 
 	iser_dbg("req op %x flags %x\n", req->opcode, req->flags);
 	/* check if this is the last login - going to full feature phase */
@@ -335,13 +337,14 @@ static int iser_post_rx_bufs(struct iscs
 	 */
 	WARN_ON(ib_conn->post_recv_buf_count != 1);
 
+#if defined(CONFIG_ISER_DISCOVERY)
 	if (session->discovery_sess) {
 		iser_info("Discovery session, re-using login RX buffer\n");
 		return 0;
 	} else
 		iser_info("Normal session, posting batch of RX %d buffers\n",
 			  iser_conn->min_posted_rx);
-
+#endif
 	/* Initial post receive buffers */
 	if (iser_post_recvm(iser_conn, iser_conn->min_posted_rx))
 		return -ENOMEM;
@@ -365,7 +368,11 @@ int iser_send_command(struct iscsi_conn
 	unsigned long edtl;
 	int err;
 	struct iser_data_buf *data_buf, *prot_buf;
+#ifdef HAVE_ISCSI_CMD
+	struct iscsi_cmd *hdr = (struct iscsi_cmd *)task->hdr;
+#else	
 	struct iscsi_scsi_req *hdr = (struct iscsi_scsi_req *)task->hdr;
+#endif
 	struct scsi_cmnd *sc  =  task->sc;
 	struct iser_tx_desc *tx_desc = &iser_task->desc;
 	u8 sig_count = ++iser_conn->ib_conn.sig_count;
@@ -752,7 +759,11 @@ void iser_task_rdma_init(struct iscsi_is
 void iser_task_rdma_finalize(struct iscsi_iser_task *iser_task)
 {
 	int prot_count = scsi_prot_sg_count(iser_task->sc);
-
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+	int is_rdma_data_aligned = 1;
+	int is_rdma_prot_aligned = 1;
+#endif
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 	if (iser_task->dir[ISER_DIR_IN]) {
 		iser_unreg_rdma_mem(iser_task, ISER_DIR_IN);
 		iser_dma_unmap_task_data(iser_task,
@@ -774,4 +785,60 @@ void iser_task_rdma_finalize(struct iscs
 						 &iser_task->prot[ISER_DIR_OUT],
 						 DMA_TO_DEVICE);
 	}
+#else
+	/* if we were reading, copy back to unaligned sglist,
+	* anyway dma_unmap and free the copy
+	*/
+	if (iser_task->data[ISER_DIR_IN].orig_sg) {
+		is_rdma_data_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->data[ISER_DIR_IN],
+						ISER_DIR_IN);
+	}
+
+	if (iser_task->data[ISER_DIR_OUT].orig_sg) {
+		is_rdma_data_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->data[ISER_DIR_OUT],
+						ISER_DIR_OUT);
+	}
+
+	if (iser_task->prot[ISER_DIR_IN].orig_sg) {
+		is_rdma_prot_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->prot[ISER_DIR_IN],
+						ISER_DIR_IN);
+	}
+
+	if (iser_task->prot[ISER_DIR_OUT].orig_sg) {
+		is_rdma_prot_aligned = 0;
+		iser_finalize_rdma_unaligned_sg(iser_task,
+						&iser_task->prot[ISER_DIR_OUT],
+						ISER_DIR_OUT);
+	}
+
+	if (iser_task->dir[ISER_DIR_IN]) {
+		iser_unreg_rdma_mem(iser_task, ISER_DIR_IN);
+		if (is_rdma_data_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->data[ISER_DIR_IN],
+						DMA_FROM_DEVICE);
+		if (prot_count && is_rdma_prot_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->prot[ISER_DIR_IN],
+						DMA_FROM_DEVICE);
+	}
+
+	if (iser_task->dir[ISER_DIR_OUT]) {
+		iser_unreg_rdma_mem(iser_task, ISER_DIR_OUT);
+		if (is_rdma_data_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->data[ISER_DIR_OUT],
+						DMA_TO_DEVICE);
+		if (prot_count && is_rdma_prot_aligned)
+			iser_dma_unmap_task_data(iser_task,
+						&iser_task->prot[ISER_DIR_OUT],
+						DMA_TO_DEVICE);
+	}
+#endif
 }
--- a/drivers/infiniband/ulp/iser/iser_memory.c
+++ b/drivers/infiniband/ulp/iser/iser_memory.c
@@ -67,6 +67,258 @@ static const struct iser_reg_ops fmr_ops
 	.reg_desc_put	= iser_reg_desc_put_fmr,
 };
 
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+#define IS_4K_ALIGNED(addr)    ((((unsigned long)addr) & ~MASK_4K) == 0)
+static void
+iser_free_bounce_sg(struct iser_data_buf *data)
+{
+	struct scatterlist *sg;
+	int count;
+
+	for_each_sg(data->sg, sg, data->size, count)
+		__free_page(sg_page(sg));
+
+	kfree(data->sg);
+
+	data->sg = data->orig_sg;
+	data->size = data->orig_size;
+	data->orig_sg = NULL;
+	data->orig_size = 0;
+}
+
+static int
+iser_alloc_bounce_sg(struct iser_data_buf *data)
+{
+	struct scatterlist *sg;
+	struct page *page;
+	unsigned long length = data->data_len;
+	int i = 0, nents = DIV_ROUND_UP(length, PAGE_SIZE);
+
+	sg = kcalloc(nents, sizeof(*sg), GFP_ATOMIC);
+	if (!sg)
+		goto err;
+
+	sg_init_table(sg, nents);
+	while (length) {
+		u32 page_len = min_t(u32, length, PAGE_SIZE);
+
+		page = alloc_page(GFP_ATOMIC);
+		if (!page)
+			goto err;
+
+		sg_set_page(&sg[i], page, page_len, 0);
+		length -= page_len;
+		i++;
+	}
+
+	data->orig_sg = data->sg;
+	data->orig_size = data->size;
+	data->sg = sg;
+	data->size = nents;
+
+	return 0;
+
+err:
+	for (; i > 0; i--)
+		__free_page(sg_page(&sg[i - 1]));
+	kfree(sg);
+
+	return -ENOMEM;
+}
+
+static void
+iser_copy_bounce(struct iser_data_buf *data, bool to_buffer)
+{
+	struct scatterlist *osg, *bsg = data->sg;
+	void *oaddr, *baddr;
+	unsigned int left = data->data_len;
+	unsigned int bsg_off = 0;
+	int i;
+
+	for_each_sg(data->orig_sg, osg, data->orig_size, i) {
+		unsigned int copy_len, osg_off = 0;
+
+#ifdef HAVE_KM_TYPE
+		if (to_buffer)
+			oaddr = kmap_atomic(sg_page(osg), KM_USER0) + osg->offset;
+		else
+			oaddr = kmap_atomic(sg_page(osg), KM_SOFTIRQ0) + osg->offset;
+#else
+		oaddr = kmap_atomic(sg_page(osg)) + osg->offset;
+#endif
+		copy_len = min(left, osg->length);
+		while (copy_len) {
+			unsigned int len = min(copy_len, bsg->length - bsg_off);
+
+#ifdef HAVE_KM_TYPE
+			if (to_buffer)
+				baddr = kmap_atomic(sg_page(bsg), KM_USER0) + bsg->offset;
+			else
+				baddr = kmap_atomic(sg_page(bsg), KM_SOFTIRQ0) + bsg->offset;
+#else
+			baddr = kmap_atomic(sg_page(bsg)) + bsg->offset;
+#endif
+			if (to_buffer)
+				memcpy(baddr + bsg_off, oaddr + osg_off, len);
+			else
+				memcpy(oaddr + osg_off, baddr + bsg_off, len);
+
+#ifdef HAVE_KM_TYPE
+			if (to_buffer)
+				kunmap_atomic(baddr - bsg->offset, KM_USER0);
+			else
+				kunmap_atomic(baddr - bsg->offset, KM_SOFTIRQ0);
+#else
+			kunmap_atomic(baddr - bsg->offset);
+#endif
+			osg_off += len;
+			bsg_off += len;
+			copy_len -= len;
+
+			if (bsg_off >= bsg->length) {
+				bsg = sg_next(bsg);
+					bsg_off = 0;
+			}
+		}
+#ifdef HAVE_KM_TYPE
+		if (to_buffer)
+			kunmap_atomic(oaddr - osg->offset, KM_USER0);
+		else
+			kunmap_atomic(oaddr - osg->offset, KM_SOFTIRQ0);
+#else
+		kunmap_atomic(oaddr - osg->offset);
+#endif
+		left -= osg_off;
+	}
+}
+
+static inline void
+iser_copy_from_bounce(struct iser_data_buf *data)
+{
+	iser_copy_bounce(data, false);
+}
+
+static inline void
+iser_copy_to_bounce(struct iser_data_buf *data)
+{
+	iser_copy_bounce(data, true);
+}
+
+/**
+	* iser_start_rdma_unaligned_sg
+*/
+static int iser_start_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,
+					struct iser_data_buf *data,
+					enum iser_data_dir cmd_dir)
+{
+	struct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;
+	int rc;
+
+	rc = iser_alloc_bounce_sg(data);
+	if (rc) {
+		iser_err("Failed to allocate bounce for data len %lu\n",
+					data->data_len);
+		return rc;
+	}
+
+	if (cmd_dir == ISER_DIR_OUT)
+		iser_copy_to_bounce(data);
+
+	data->dma_nents = ib_dma_map_sg(dev, data->sg, data->size,
+					(cmd_dir == ISER_DIR_OUT) ?
+					DMA_TO_DEVICE : DMA_FROM_DEVICE);
+	if (!data->dma_nents) {
+		iser_err("Got dma_nents %d, something went wrong...\n",
+						data->dma_nents);
+						rc = -ENOMEM;
+		goto err;
+	}
+
+	return 0;
+err:
+	iser_free_bounce_sg(data);
+	return rc;
+}
+
+/**
+ * iser_finalize_rdma_unaligned_sg
+ */
+
+void iser_finalize_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,
+				      struct iser_data_buf *data,
+				      enum iser_data_dir cmd_dir)
+{
+	struct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;
+
+	ib_dma_unmap_sg(dev, data->sg, data->size,
+		 	(cmd_dir == ISER_DIR_OUT) ?
+			DMA_TO_DEVICE : DMA_FROM_DEVICE);
+
+	if (cmd_dir == ISER_DIR_IN)
+		iser_copy_from_bounce(data);
+
+	iser_free_bounce_sg(data);
+}
+
+/**
+ * iser_data_buf_aligned_len - Tries to determine the maximal correctly aligned
+ * for RDMA sub-list of a scatter-gather list of memory buffers, and  returns
+ * the number of entries which are aligned correctly. Supports the case where
+ * consecutive SG elements are actually fragments of the same physcial page.
+ */
+static int iser_data_buf_aligned_len(struct iser_data_buf *data,
+			 	      struct ib_device *ibdev,
+				      unsigned sg_tablesize)
+{
+	struct scatterlist *sg, *sgl, *next_sg = NULL;
+	u64 start_addr, end_addr;
+	int i, ret_len, start_check = 0;
+
+	if (data->dma_nents == 1)
+		return 1;
+
+	sgl = data->sg;
+	start_addr  = ib_sg_dma_address(ibdev, sgl);
+
+	if (unlikely(sgl[0].offset &&
+		data->data_len >= sg_tablesize * PAGE_SIZE)) {
+		iser_dbg("can't register length %lx with offset %x "
+				"fall to bounce buffer\n", data->data_len,
+			sgl[0].offset);
+			return 0;
+	}
+
+	for_each_sg(sgl, sg, data->dma_nents, i) {
+		if (start_check && !IS_4K_ALIGNED(start_addr))
+			break;
+
+		next_sg = sg_next(sg);
+		if (!next_sg)
+			break;
+
+		end_addr    = start_addr + ib_sg_dma_len(ibdev, sg);
+		start_addr  = ib_sg_dma_address(ibdev, next_sg);
+
+		if (end_addr == start_addr) {
+			start_check = 0;
+			continue;
+		} else
+			start_check = 1;
+
+		if (!IS_4K_ALIGNED(end_addr))
+			break;
+	}
+	ret_len = (next_sg) ? i : i+1;
+
+	if (unlikely(ret_len != data->dma_nents))
+		iser_warn("rdma alignment violation (%d/%d aligned)\n",
+					ret_len, data->dma_nents);
+
+	return ret_len;
+}
+
+#endif
+
 void iser_reg_comp(struct ib_cq *cq, struct ib_wc *wc)
 {
 	iser_err_comp(wc, "memreg");
@@ -160,6 +412,54 @@ static void iser_dump_page_vec(struct is
 		iser_err("vec[%d]: %llx\n", i, page_vec->pages[i]);
 }
 
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+static int fall_to_bounce_buf(struct iscsi_iser_task *iser_task,
+			       struct iser_data_buf *mem,
+			       enum iser_data_dir cmd_dir)
+{
+	struct iscsi_conn *iscsi_conn = iser_task->iser_conn->iscsi_conn;
+	struct iser_device *device = iser_task->iser_conn->ib_conn.device;
+
+	iscsi_conn->fmr_unalign_cnt++;
+
+	if (iser_debug_level > 0)
+		iser_data_buf_dump(mem, device->ib_device);
+
+	/* unmap the command data before accessing it */
+	iser_dma_unmap_task_data(iser_task, mem,
+				(cmd_dir == ISER_DIR_OUT) ?
+				DMA_TO_DEVICE : DMA_FROM_DEVICE);
+
+	/* allocate copy buf, if we are writing, copy the */
+	/* unaligned scatterlist, dma map the copy        */
+	if (iser_start_rdma_unaligned_sg(iser_task, mem, cmd_dir) != 0)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static int
+iser_handle_unaligned_buf(struct iscsi_iser_task *task,
+			   struct iser_data_buf *mem,
+			   enum iser_data_dir dir)
+{
+	struct iser_conn *iser_conn = task->iser_conn;
+	struct iser_device *device = iser_conn->ib_conn.device;
+	int err, aligned_len;
+
+	aligned_len = iser_data_buf_aligned_len(mem, device->ib_device,
+						iser_conn->scsi_sg_tablesize);
+	if (aligned_len != mem->dma_nents) {
+		err = fall_to_bounce_buf(task, mem, dir);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+#endif 
+
 int iser_dma_map_task_data(struct iscsi_iser_task *iser_task,
 			    struct iser_data_buf *data,
 			    enum iser_data_dir iser_dir,
@@ -310,8 +610,13 @@ iser_set_dif_domain(struct scsi_cmnd *sc
 		    struct ib_sig_domain *domain)
 {
 	domain->sig_type = IB_SIG_TYPE_T10_DIF;
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 	domain->sig.dif.pi_interval = scsi_prot_interval(sc);
 	domain->sig.dif.ref_tag = scsi_prot_ref_tag(sc);
+#else
+	domain->sig.dif.pi_interval = sc->device->sector_size;
+	domain->sig.dif.ref_tag = scsi_get_lba(sc) & 0xffffffff;
+#endif
 	/*
 	 * At the moment we hard code those, but in the future
 	 * we will take them from sc.
@@ -319,9 +624,15 @@ iser_set_dif_domain(struct scsi_cmnd *sc
 	domain->sig.dif.apptag_check_mask = 0xffff;
 	domain->sig.dif.app_escape = true;
 	domain->sig.dif.ref_escape = true;
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 	if (sc->prot_flags & SCSI_PROT_REF_INCREMENT)
 		domain->sig.dif.ref_remap = true;
-};
+#else
+	if (scsi_get_prot_type(sc) == SCSI_PROT_DIF_TYPE1 ||
+		scsi_get_prot_type(sc) == SCSI_PROT_DIF_TYPE2)
+		domain->sig.dif.ref_remap = true;
+#endif
+}
 
 static int
 iser_set_sig_attrs(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs)
@@ -337,16 +648,26 @@ iser_set_sig_attrs(struct scsi_cmnd *sc,
 	case SCSI_PROT_WRITE_STRIP:
 		sig_attrs->wire.sig_type = IB_SIG_TYPE_NONE;
 		iser_set_dif_domain(sc, sig_attrs, &sig_attrs->mem);
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 		sig_attrs->mem.sig.dif.bg_type = sc->prot_flags & SCSI_PROT_IP_CHECKSUM ?
 						IB_T10DIF_CSUM : IB_T10DIF_CRC;
+#else
+		sig_attrs->mem.sig.dif.bg_type = iser_pi_guard ? IB_T10DIF_CSUM :
+						IB_T10DIF_CRC;
+#endif
 		break;
 	case SCSI_PROT_READ_PASS:
 	case SCSI_PROT_WRITE_PASS:
 		iser_set_dif_domain(sc, sig_attrs, &sig_attrs->wire);
 		sig_attrs->wire.sig.dif.bg_type = IB_T10DIF_CRC;
 		iser_set_dif_domain(sc, sig_attrs, &sig_attrs->mem);
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 		sig_attrs->mem.sig.dif.bg_type = sc->prot_flags & SCSI_PROT_IP_CHECKSUM ?
 						IB_T10DIF_CSUM : IB_T10DIF_CRC;
+#else
+		sig_attrs->mem.sig.dif.bg_type = iser_pi_guard ? IB_T10DIF_CSUM :
+						IB_T10DIF_CRC;
+#endif
 		break;
 	default:
 		iser_err("Unsupported PI operation %d\n",
@@ -357,6 +678,7 @@ iser_set_sig_attrs(struct scsi_cmnd *sc,
 	return 0;
 }
 
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 static inline void
 iser_set_prot_checks(struct scsi_cmnd *sc, u8 *mask)
 {
@@ -366,6 +688,30 @@ iser_set_prot_checks(struct scsi_cmnd *s
 	if (sc->prot_flags & SCSI_PROT_GUARD_CHECK)
 		*mask |= ISER_CHECK_GUARD;
 }
+#else
+static int
+iser_set_prot_checks(struct scsi_cmnd *sc, u8 *mask)
+{
+	switch (scsi_get_prot_type(sc)) {
+	case SCSI_PROT_DIF_TYPE0:
+		*mask = 0x0;
+		break;
+	case SCSI_PROT_DIF_TYPE1:
+	case SCSI_PROT_DIF_TYPE2:
+		*mask = ISER_CHECK_GUARD | ISER_CHECK_REFTAG;
+		break;
+	case SCSI_PROT_DIF_TYPE3:
+		*mask = ISER_CHECK_GUARD;
+		break;
+	default:
+		iser_err("Unsupported protection type %d\n",
+			 scsi_get_prot_type(sc));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+#endif
 
 static inline void
 iser_inv_rkey(struct ib_send_wr *inv_wr,
@@ -397,9 +743,13 @@ iser_reg_sig_mr(struct iscsi_iser_task *
 	ret = iser_set_sig_attrs(iser_task->sc, sig_attrs);
 	if (ret)
 		goto err;
-
+#ifdef HAVE_SCSI_CMND_PROT_FLAGS
 	iser_set_prot_checks(iser_task->sc, &sig_attrs->check_mask);
-
+#else
+	ret = iser_set_prot_checks(iser_task->sc, &sig_attrs->check_mask);
+	if (ret)
+		goto err;
+#endif
 	if (pi_ctx->sig_mr_valid)
 		iser_inv_rkey(iser_tx_next_wr(tx_desc), mr, cqe);
 
@@ -421,7 +771,6 @@ iser_reg_sig_mr(struct iscsi_iser_task *
 			   IB_ACCESS_REMOTE_READ |
 			   IB_ACCESS_REMOTE_WRITE;
 	pi_ctx->sig_mr_valid = 1;
-
 	sig_reg->sge.lkey = mr->lkey;
 	sig_reg->rkey = mr->rkey;
 	sig_reg->sge.addr = 0;
@@ -524,6 +873,11 @@ int iser_reg_rdma_mem(struct iscsi_iser_
 	bool use_dma_key;
 	int err;
 
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+	err = iser_handle_unaligned_buf(task, mem, dir);
+	if (unlikely(err))
+		return err;
+#endif
 	use_dma_key = mem->dma_nents == 1 && (all_imm || !iser_always_reg) &&
 		      scsi_get_prot_op(task->sc) == SCSI_PROT_NORMAL;
 
@@ -546,6 +900,11 @@ int iser_reg_rdma_mem(struct iscsi_iser_
 
 		if (scsi_prot_sg_count(task->sc)) {
 			mem = &task->prot[dir];
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+			err = iser_handle_unaligned_buf(task, mem, dir);
+			if (unlikely(err))
+				goto err_reg;
+#endif
 			err = iser_reg_prot_sg(task, mem, desc,
 					       use_dma_key, prot_reg);
 			if (unlikely(err))
--- a/drivers/infiniband/ulp/iser/iser_verbs.c
+++ b/drivers/infiniband/ulp/iser/iser_verbs.c
@@ -1100,6 +1100,7 @@ int iser_post_send(struct ib_conn *ib_co
 	return ib_ret;
 }
 
+#ifdef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
 u8 iser_check_task_pi_status(struct iscsi_iser_task *iser_task,
 			     enum iser_data_dir cmd_dir, sector_t *sector)
 {
@@ -1147,6 +1148,7 @@ err:
 	/* Not alot we can do here, return ambiguous guard error */
 	return 0x1;
 }
+#endif
 
 void iser_err_comp(struct ib_wc *wc, const char *type)
 {
