From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/nvme/host/fc.c

Change-Id: Id5a3d60331a15a46ea4e47b0da00b14a03e2135c
---
 drivers/nvme/host/fc.c | 94 ++++++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 94 insertions(+)

--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@ -14,6 +14,8 @@
  * can be found in the file COPYING included with this package
  *
  */
+#ifdef HAVE_LINUX_NVME_FC_DRIVER_H
+
 #ifdef pr_fmt
 #undef pr_fmt
 #endif
@@ -1505,6 +1507,7 @@ __nvme_fc_exit_request(struct nvme_fc_ct
 	atomic_set(&op->state, FCPOP_STATE_UNINIT);
 }
 
+#ifdef HAVE_BLK_MQ_OPS_EXIT_REQUEST_HAS_3_PARAMS
 static void
 nvme_fc_exit_request(struct blk_mq_tag_set *set, struct request *rq,
 		unsigned int hctx_idx)
@@ -1513,6 +1516,16 @@ nvme_fc_exit_request(struct blk_mq_tag_s
 
 	return __nvme_fc_exit_request(set->driver_data, op);
 }
+#else
+static void
+nvme_fc_exit_request(void *data, struct request *rq,
+		unsigned int hctx_idx, unsigned int rq_idx)
+{
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
+
+	__nvme_fc_exit_request(data, op);
+}
+#endif
 
 static int
 __nvme_fc_abort_op(struct nvme_fc_ctrl *ctrl, struct nvme_fc_fcp_op *op)
@@ -1788,6 +1801,7 @@ out_on_error:
 	return ret;
 }
 
+#ifdef HAVE_BLK_MQ_OPS_INIT_REQUEST_HAS_4_PARAMS
 static int
 nvme_fc_init_request(struct blk_mq_tag_set *set, struct request *rq,
 		unsigned int hctx_idx, unsigned int numa_node)
@@ -1799,6 +1813,31 @@ nvme_fc_init_request(struct blk_mq_tag_s
 
 	return __nvme_fc_init_request(ctrl, queue, op, rq, queue->rqcnt++);
 }
+#else
+static int
+nvme_fc_init_request(void *data, struct request *rq,
+		unsigned int hctx_idx, unsigned int rq_idx,
+		unsigned int numa_node)
+{
+	struct nvme_fc_ctrl *ctrl = data;
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
+	struct nvme_fc_queue *queue = &ctrl->queues[hctx_idx+1];
+
+	return __nvme_fc_init_request(ctrl, queue, op, rq, queue->rqcnt++);
+}
+
+static int
+nvme_fc_init_admin_request(void *data, struct request *rq,
+		unsigned int hctx_idx, unsigned int rq_idx,
+		unsigned int numa_node)
+{
+	struct nvme_fc_ctrl *ctrl = data;
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
+	struct nvme_fc_queue *queue = &ctrl->queues[0];
+
+	return __nvme_fc_init_request(ctrl, queue, op, rq, queue->rqcnt++);
+}
+#endif
 
 static int
 nvme_fc_init_aen_ops(struct nvme_fc_ctrl *ctrl)
@@ -2050,7 +2089,9 @@ nvme_fc_ctrl_free(struct kref *ref)
 	list_del(&ctrl->ctrl_list);
 	spin_unlock_irqrestore(&ctrl->rport->lock, flags);
 
+#ifdef HAVE_BLK_MQ_UNQUIESCE_QUEUE
 	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+#endif
 	blk_cleanup_queue(ctrl->ctrl.admin_q);
 	blk_mq_free_tag_set(&ctrl->admin_tag_set);
 
@@ -2150,12 +2191,24 @@ nvme_fc_map_data(struct nvme_fc_ctrl *ct
 
 	freq->sg_cnt = 0;
 
+#ifdef HAVE_BLK_RQ_NR_PAYLOAD_BYTES
 	if (!blk_rq_payload_bytes(rq))
 		return 0;
+#else
+	if (!nvme_map_len(rq))
+		return 0;
+#endif
 
 	freq->sg_table.sgl = freq->first_sgl;
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_4_PARAMS
+	ret = sg_alloc_table_chained(&freq->sg_table,
+				blk_rq_nr_phys_segments(rq),
+				GFP_ATOMIC,
+				freq->sg_table.sgl);
+#else
 	ret = sg_alloc_table_chained(&freq->sg_table,
 			blk_rq_nr_phys_segments(rq), freq->sg_table.sgl);
+#endif
 	if (ret)
 		return -ENOMEM;
 
@@ -2321,7 +2374,14 @@ nvme_fc_start_fcp_op(struct nvme_fc_ctrl
 
 busy:
 	if (!(op->flags & FCOP_FLAGS_AEN) && queue->hctx)
+#ifdef HAVE_BLK_MQ_DELAY_RUN_HW_QUEUE
 		blk_mq_delay_run_hw_queue(queue->hctx, NVMEFC_QUEUE_DELAY);
+#else
+		{
+			blk_mq_stop_hw_queues(op->rq->q);
+			blk_mq_delay_queue(queue->hctx, NVMEFC_QUEUE_DELAY);
+		}
+#endif
 
 	return BLK_STS_RESOURCE;
 }
@@ -2357,7 +2417,11 @@ nvme_fc_queue_rq(struct blk_mq_hw_ctx *h
 	if (ret)
 		return ret;
 
+#ifdef HAVE_BLK_RQ_NR_PAYLOAD_BYTES
 	data_len = blk_rq_payload_bytes(rq);
+#else
+	data_len = nvme_map_len(rq);
+#endif
 	if (data_len)
 		io_dir = ((rq_data_dir(rq) == WRITE) ?
 					NVMEFC_FCP_WRITE : NVMEFC_FCP_READ);
@@ -2367,6 +2431,7 @@ nvme_fc_queue_rq(struct blk_mq_hw_ctx *h
 	return nvme_fc_start_fcp_op(ctrl, queue, op, data_len, io_dir);
 }
 
+#ifdef HAVE_BLK_MQ_POLL
 static struct blk_mq_tags *
 nvme_fc_tagset(struct nvme_fc_queue *queue)
 {
@@ -2398,6 +2463,7 @@ nvme_fc_poll(struct blk_mq_hw_ctx *hctx,
 
 	return ((atomic_read(&op->state) != FCPOP_STATE_ACTIVE));
 }
+#endif
 
 static void
 nvme_fc_submit_async_event(struct nvme_ctrl *arg)
@@ -2524,8 +2590,13 @@ static const struct blk_mq_ops nvme_fc_m
 	.complete	= nvme_fc_complete_rq,
 	.init_request	= nvme_fc_init_request,
 	.exit_request	= nvme_fc_exit_request,
+#ifdef HAVE_BLK_MQ_OPS_REINIT_REQUEST
+	.reinit_request = nvme_fc_reinit_request,
+#endif
 	.init_hctx	= nvme_fc_init_hctx,
+#ifdef HAVE_BLK_MQ_POLL
 	.poll		= nvme_fc_poll,
+#endif
 	.timeout	= nvme_fc_timeout,
 };
 
@@ -2624,7 +2695,11 @@ nvme_fc_reinit_io_queues(struct nvme_fc_
 
 	nvme_fc_init_io_queues(ctrl);
 
+#if defined(HAVE_BLK_MQ_TAGSET_ITER) || defined(HAVE_BLK_MQ_REINIT_TAGSET_2_PARAM)
 	ret = nvme_reinit_tagset(&ctrl->ctrl, ctrl->ctrl.tagset);
+#else
+	ret = blk_mq_reinit_tagset(&ctrl->tag_set);
+#endif
 	if (ret)
 		goto out_free_io_queues;
 
@@ -2739,7 +2814,11 @@ nvme_fc_create_association(struct nvme_f
 		goto out_delete_hw_queue;
 
 	if (ctrl->ctrl.state != NVME_CTRL_NEW)
+#ifdef HAVE_BLK_MQ_UNQUIESCE_QUEUE
 		blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+#else
+		blk_mq_start_stopped_hw_queues(ctrl->ctrl.admin_q, true);
+#endif
 
 	ret = nvmf_connect_admin_queue(&ctrl->ctrl);
 	if (ret)
@@ -2892,7 +2971,11 @@ nvme_fc_delete_association(struct nvme_f
 	 * terminate the exchanges.
 	 */
 	if (ctrl->ctrl.state != NVME_CTRL_NEW)
+#ifdef HAVE_BLK_MQ_UNQUIESCE_QUEUE
 		blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+#else
+		blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
+#endif
 	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
 				nvme_fc_terminate_exchange, &ctrl->ctrl);
 
@@ -3052,8 +3135,15 @@ nvme_fc_connect_ctrl_work(struct work_st
 static const struct blk_mq_ops nvme_fc_admin_mq_ops = {
 	.queue_rq	= nvme_fc_queue_rq,
 	.complete	= nvme_fc_complete_rq,
+#ifdef HAVE_BLK_MQ_OPS_INIT_REQUEST_HAS_4_PARAMS
 	.init_request	= nvme_fc_init_request,
+#else
+	.init_request	= nvme_fc_init_admin_request,
+#endif
 	.exit_request	= nvme_fc_exit_request,
+#ifdef HAVE_BLK_MQ_OPS_REINIT_REQUEST
+	.reinit_request = nvme_fc_reinit_request,
+#endif
 	.init_hctx	= nvme_fc_init_admin_hctx,
 	.timeout	= nvme_fc_timeout,
 };
@@ -3161,7 +3251,9 @@ nvme_fc_init_ctrl(struct device *dev, st
 	ctrl->admin_tag_set.driver_data = ctrl;
 	ctrl->admin_tag_set.nr_hw_queues = 1;
 	ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
+#ifdef HAVE_BLK_MQ_F_NO_SCHED
 	ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+#endif
 
 	ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
 	if (ret)
@@ -3456,3 +3548,5 @@ module_init(nvme_fc_init_module);
 module_exit(nvme_fc_exit_module);
 
 MODULE_LICENSE("GPL v2");
+
+#endif /* HAVE_LINUX_NVME_FC_DRIVER_H */
