From: Feras Daoud <ferasda@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_main.c

Change-Id: I2b306f86d04b1b2a2ba3c5226f55204e634dbed1
---
 drivers/net/ethernet/mellanox/mlx5/core/en_main.c | 643 +++++++++++++++++++++-
 1 file changed, 630 insertions(+), 13 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -33,8 +33,12 @@
 #include <net/tc_act/tc_gact.h>
 #include <net/pkt_cls.h>
 #include <linux/mlx5/fs.h>
+#if defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON)
 #include <net/vxlan.h>
+#endif
+#ifdef HAVE_NETDEV_XDP
 #include <linux/bpf.h>
+#endif
 #include "eswitch.h"
 #include "en.h"
 #include "en_tc.h"
@@ -42,10 +46,14 @@
 #include "en_accel/ipsec.h"
 #include "en_accel/ipsec_rxtx.h"
 #include "accel/ipsec.h"
+#if defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON)
 #include "vxlan.h"
+#endif
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #include "en_trust.h"
 #endif
+#endif
 
 struct mlx5e_rq_param {
 	u32			rqc[MLX5_ST_SZ_DW(rqc)];
@@ -67,7 +75,9 @@ struct mlx5e_cq_param {
 struct mlx5e_channel_param {
 	struct mlx5e_rq_param      rq;
 	struct mlx5e_sq_param      sq;
+#ifdef HAVE_NETDEV_XDP
 	struct mlx5e_sq_param      xdp_sq;
+#endif
 	struct mlx5e_sq_param      icosq;
 	struct mlx5e_cq_param      rx_cq;
 	struct mlx5e_cq_param      tx_cq;
@@ -102,8 +112,12 @@ void mlx5e_set_rq_type_params(struct mlx
 		params->log_rq_size = is_kdump_kernel() ?
 			MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE :
 			MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
+#ifdef HAVE_NETDEV_XDP
 		params->rq_headroom = params->xdp_prog ?
 			XDP_PACKET_HEADROOM : MLX5_RX_HEADROOM;
+#else
+		params->rq_headroom = MLX5_RX_HEADROOM;
+#endif
 		params->rq_headroom += NET_IP_ALIGN;
 
 		/* Extra room needed for build_skb */
@@ -121,7 +135,11 @@ void mlx5e_set_rq_type_params(struct mlx
 static void mlx5e_set_rq_params(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
 {
 	u8 rq_type = mlx5e_check_fragmented_striding_rq_cap(mdev) &&
+#ifdef HAVE_NETDEV_XDP
 		    !params->xdp_prog && !MLX5_IPSEC_DEV(mdev) ?
+#else
+		    !MLX5_IPSEC_DEV(mdev) ?
+#endif
 		    MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ :
 		    MLX5_WQ_TYPE_LINKED_LIST;
 	mlx5e_set_rq_type_params(mdev, params, rq_type);
@@ -334,6 +352,26 @@ static void mlx5e_update_pcie_counters(s
 	mlx5_core_access_reg(mdev, in, sz, out, sz, MLX5_REG_MPCNT, 0, 0);
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static void mlx5e_update_sw_lro_stats(struct mlx5e_priv *priv)
+{
+	int i;
+	struct mlx5e_sw_stats *s = &priv->stats.sw;
+
+	s->rx_sw_lro_aggregated = 0;
+	s->rx_sw_lro_flushed = 0;
+	s->rx_sw_lro_no_desc = 0;
+
+	for (i = 0; i < priv->channels.num; i++) {
+		struct mlx5e_rq *rq = &priv->channels.c[i]->rq;
+
+		s->rx_sw_lro_aggregated += rq->sw_lro.lro_mgr.stats.aggregated;
+		s->rx_sw_lro_flushed += rq->sw_lro.lro_mgr.stats.flushed;
+		s->rx_sw_lro_no_desc += rq->sw_lro.lro_mgr.stats.no_desc;
+	}
+}
+#endif
+
 void mlx5e_update_stats(struct mlx5e_priv *priv, bool full)
 {
 	if (full) {
@@ -344,6 +382,9 @@ void mlx5e_update_stats(struct mlx5e_pri
 	mlx5e_update_vport_counters(priv);
 	mlx5e_update_q_counter(priv);
 	mlx5e_update_sw_counters(priv);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_update_sw_lro_stats(priv);
+#endif
 }
 
 static void mlx5e_update_ndo_stats(struct mlx5e_priv *priv)
@@ -687,6 +728,7 @@ static int mlx5e_alloc_rq(struct mlx5e_c
 	rq->ix      = c->ix;
 	rq->mdev    = mdev;
 
+#ifdef HAVE_NETDEV_XDP
 	rq->xdp_prog = params->xdp_prog ? bpf_prog_inc(params->xdp_prog) : NULL;
 	if (IS_ERR(rq->xdp_prog)) {
 		err = PTR_ERR(rq->xdp_prog);
@@ -695,6 +737,9 @@ static int mlx5e_alloc_rq(struct mlx5e_c
 	}
 
 	rq->buff.map_dir = rq->xdp_prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
+#else
+	rq->buff.map_dir = DMA_FROM_DEVICE;
+#endif
 	rq->rx_headroom = params->rq_headroom;
 
 	switch (rq->wq_type) {
@@ -757,14 +802,28 @@ static int mlx5e_alloc_rq(struct mlx5e_c
 			goto err_rq_wq_destroy;
 		}
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		rq->buff.wqe_sz = (IS_HW_LRO(params)) ?
+#else
 		rq->buff.wqe_sz = params->lro_en  ?
+#endif
 				params->lro_wqe_sz :
 				MLX5E_SW2HW_MTU(c->priv, c->netdev->mtu);
 #ifdef CONFIG_MLX5_EN_IPSEC
 		if (MLX5_IPSEC_DEV(mdev))
 			rq->buff.wqe_sz += MLX5E_METADATA_ETHER_LEN;
 #endif
-		rq->wqe.page_reuse = !params->xdp_prog && !params->lro_en;
+
+#ifdef HAVE_NETDEV_XDP
+		rq->wqe.page_reuse = !params->xdp_prog;
+#else
+		rq->wqe.page_reuse = true;
+#endif
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		rq->wqe.page_reuse &= !IS_HW_LRO(params);
+#else
+		rq->wqe.page_reuse &= !params->lro_en;
+#endif
 		byte_count = rq->buff.wqe_sz;
 
 		/* calc the required page order */
@@ -797,8 +856,10 @@ err_destroy_umr_mkey:
 		mlx5_core_destroy_mkey(mdev, &rq->umr_mkey);
 
 err_rq_wq_destroy:
+#ifdef HAVE_NETDEV_XDP
 	if (rq->xdp_prog)
 		bpf_prog_put(rq->xdp_prog);
+#endif
 	mlx5_wq_destroy(&rq->wq_ctrl);
 
 	return err;
@@ -806,8 +867,10 @@ err_rq_wq_destroy:
 
 static void mlx5e_free_rq(struct mlx5e_rq *rq)
 {
+#ifdef HAVE_NETDEV_XDP
 	if (rq->xdp_prog)
 		bpf_prog_put(rq->xdp_prog);
+#endif
 
 	if (rq->page_cache.page_cache)
 		mlx5e_rx_free_page_cache(rq);
@@ -913,6 +976,7 @@ static int mlx5e_modify_rq_state(struct
 	return err;
 }
 
+#ifdef HAVE_NETIF_F_RXFCS
 static int mlx5e_modify_rq_scatter_fcs(struct mlx5e_rq *rq, bool enable)
 {
 	struct mlx5e_channel *c = rq->channel;
@@ -943,6 +1007,7 @@ static int mlx5e_modify_rq_scatter_fcs(s
 
 	return err;
 }
+#endif
 
 static int mlx5e_modify_rq_vsd(struct mlx5e_rq *rq, bool vsd)
 {
@@ -1029,6 +1094,58 @@ static void mlx5e_free_rx_descs(struct m
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static int get_skb_hdr(struct sk_buff *skb, void **iphdr,
+			void **tcph, u64 *hdr_flags, void *priv)
+{
+	unsigned int ip_len;
+	struct iphdr *iph;
+
+	if (unlikely(skb->protocol != htons(ETH_P_IP)))
+		return -1;
+
+	/*
+	* In the future we may add an else clause that verifies the
+	* checksum and allows devices which do not calculate checksum
+	* to use LRO.
+	*/
+	if (unlikely(skb->ip_summed != CHECKSUM_UNNECESSARY))
+		return -1;
+
+	/* Check for non-TCP packet */
+	skb_reset_network_header(skb);
+	iph = ip_hdr(skb);
+	if (iph->protocol != IPPROTO_TCP)
+		return -1;
+
+	ip_len = ip_hdrlen(skb);
+	skb_set_transport_header(skb, ip_len);
+	*tcph = tcp_hdr(skb);
+
+	/* check if IP header and TCP header are complete */
+	if (ntohs(iph->tot_len) < ip_len + tcp_hdrlen(skb))
+		return -1;
+
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+	*iphdr = iph;
+
+	return 0;
+}
+
+static void mlx5e_rq_sw_lro_init(struct mlx5e_rq *rq)
+{
+	rq->sw_lro.lro_mgr.max_aggr 		= 64;
+	rq->sw_lro.lro_mgr.max_desc		= MLX5E_LRO_MAX_DESC;
+	rq->sw_lro.lro_mgr.lro_arr		= rq->sw_lro.lro_desc;
+	rq->sw_lro.lro_mgr.get_skb_header	= get_skb_hdr;
+	rq->sw_lro.lro_mgr.features		= LRO_F_NAPI;
+	rq->sw_lro.lro_mgr.frag_align_pad	= NET_IP_ALIGN;
+	rq->sw_lro.lro_mgr.dev			= rq->netdev;
+	rq->sw_lro.lro_mgr.ip_summed		= CHECKSUM_UNNECESSARY;
+	rq->sw_lro.lro_mgr.ip_summed_aggr	= CHECKSUM_UNNECESSARY;
+}
+#endif
+
 static int mlx5e_open_rq(struct mlx5e_channel *c,
 			 struct mlx5e_params *params,
 			 struct mlx5e_rq_param *param,
@@ -1049,6 +1166,10 @@ static int mlx5e_open_rq(struct mlx5e_ch
 		mlx5_core_warn(c->mdev, "Failed to enable delay drop err=%d\n",
 			       err);
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_rq_sw_lro_init(rq);
+#endif
+
 	err = mlx5e_modify_rq_state(rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);
 	if (err)
 		goto err_destroy_rq;
@@ -1093,6 +1214,7 @@ static void mlx5e_close_rq(struct mlx5e_
 	mlx5e_free_rq(rq);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static void mlx5e_free_xdpsq_db(struct mlx5e_xdpsq *sq)
 {
 	kfree(sq->db.di);
@@ -1150,6 +1272,7 @@ static void mlx5e_free_xdpsq(struct mlx5
 	mlx5e_free_xdpsq_db(sq);
 	mlx5_wq_destroy(&sq->wq_ctrl);
 }
+#endif
 
 static void mlx5e_free_icosq_db(struct mlx5e_icosq *sq)
 {
@@ -1251,11 +1374,13 @@ static int mlx5e_alloc_txqsq(struct mlx5
 	sq->txq_ix    = txq_ix;
 	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
 	sq->max_inline      = params->tx_max_inline;
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	sq->min_inline_mode = mlx5e_trust_get_txsq_inline_mode(priv);
 #else
 	sq->min_inline_mode = params->tx_min_inline_mode;
 #endif
+#endif
 	if (MLX5_IPSEC_DEV(c->priv->mdev))
 		set_bit(MLX5E_SQ_STATE_IPSEC, &sq->state);
 
@@ -1527,6 +1652,7 @@ static void mlx5e_close_icosq(struct mlx
 	mlx5e_free_icosq(sq);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static int mlx5e_open_xdpsq(struct mlx5e_channel *c,
 			    struct mlx5e_params *params,
 			    struct mlx5e_sq_param *param,
@@ -1592,6 +1718,8 @@ static void mlx5e_close_xdpsq(struct mlx
 	mlx5e_free_xdpsq(sq);
 }
 
+#endif /* HAVE_NETDEV_XDP */
+
 static int mlx5e_alloc_cq_common(struct mlx5_core_dev *mdev,
 				 struct mlx5e_cq_param *param,
 				 struct mlx5e_cq *cq)
@@ -1902,6 +2030,7 @@ static int mlx5e_set_sq_maxrate(struct n
 	return 0;
 }
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 static int mlx5e_set_tx_maxrate(struct net_device *dev, int index, u32 rate)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -1932,6 +2061,7 @@ static int mlx5e_set_tx_maxrate(struct n
 
 	return err;
 }
+#endif
 
 static int mlx5e_open_channel(struct mlx5e_priv *priv, int ix,
 			      struct mlx5e_params *params,
@@ -1957,7 +2087,9 @@ static int mlx5e_open_channel(struct mlx
 	c->netdev   = priv->netdev;
 	c->mkey_be  = cpu_to_be32(priv->mdev->mlx5e_res.mkey.key);
 	c->num_tc   = params->num_tc;
+#ifdef HAVE_NETDEV_XDP
 	c->xdp      = !!params->xdp_prog;
+#endif
 
 #ifdef CONFIG_MLX5_EN_SPECIAL_SQ
 	c->num_special_sq = params->num_rl_txqs / params->num_channels +
@@ -1984,11 +2116,13 @@ static int mlx5e_open_channel(struct mlx
 	if (err)
 		goto err_close_tx_cqs;
 
+#ifdef HAVE_NETDEV_XDP
 	/* XDP SQ CQ params are same as normal TXQ sq CQ params */
 	err = c->xdp ? mlx5e_open_cq(c, params->tx_cq_moderation,
 				     &cparam->tx_cq, &c->rq.xdpsq.cq) : 0;
 	if (err)
 		goto err_close_rx_cq;
+#endif
 
 	napi_enable(&c->napi);
 
@@ -2000,9 +2134,11 @@ static int mlx5e_open_channel(struct mlx
 	if (err)
 		goto err_close_icosq;
 
+#ifdef HAVE_NETDEV_XDP
 	err = c->xdp ? mlx5e_open_xdpsq(c, params, &cparam->xdp_sq, &c->rq.xdpsq) : 0;
 	if (err)
 		goto err_close_sqs;
+#endif
 
 	err = mlx5e_open_rq(c, params, &cparam->rq, &c->rq);
 	if (err)
@@ -2012,10 +2148,12 @@ static int mlx5e_open_channel(struct mlx
 
 	return 0;
 err_close_xdp_sq:
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_xdpsq(&c->rq.xdpsq);
 
 err_close_sqs:
+#endif
 	mlx5e_close_sqs(c);
 
 err_close_icosq:
@@ -2023,10 +2161,12 @@ err_close_icosq:
 
 err_disable_napi:
 	napi_disable(&c->napi);
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_cq(&c->rq.xdpsq.cq);
 
 err_close_rx_cq:
+#endif
 	mlx5e_close_cq(&c->rq.cq);
 
 err_close_tx_cqs:
@@ -2036,7 +2176,9 @@ err_close_icosq_cq:
 	mlx5e_close_cq(&c->icosq.cq);
 
 err_napi_del:
+#ifdef HAVE_NAPI_HASH_ADD
 	netif_napi_del(&c->napi);
+#endif
 #ifdef CONFIG_MLX5_EN_SPECIAL_SQ
 	kfree(c->special_sq);
 
@@ -2058,7 +2200,12 @@ static void mlx5e_activate_channel(struc
 		mlx5e_activate_txqsq(&c->special_sq[tc]);
 #endif
 	mlx5e_activate_rq(&c->rq);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)) || \
+	defined(CONFIG_COMPAT_IS_NETIF_SET_XPS_QUEUE_NOT_CONST_CPUMASK)
+	netif_set_xps_queue(c->netdev, (struct cpumask *)get_cpu_mask(c->cpu), c->ix);
+#else
 	netif_set_xps_queue(c->netdev, get_cpu_mask(c->cpu), c->ix);
+#endif
 	if (c->ix < c->priv->mdev->priv.eq_table.num_comp_vectors)
 		mlx5_rename_comp_eq(c->priv->mdev, c->ix, c->priv->netdev->name);
 }
@@ -2081,13 +2228,17 @@ static void mlx5e_deactivate_channel(str
 static void mlx5e_close_channel(struct mlx5e_channel *c)
 {
 	mlx5e_close_rq(&c->rq);
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_xdpsq(&c->rq.xdpsq);
+#endif
 	mlx5e_close_sqs(c);
 	mlx5e_close_icosq(&c->icosq);
 	napi_disable(&c->napi);
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_cq(&c->rq.xdpsq.cq);
+#endif
 	mlx5e_close_cq(&c->rq.cq);
 	mlx5e_close_tx_cqs(c);
 	mlx5e_close_cq(&c->icosq.cq);
@@ -2236,6 +2387,7 @@ static void mlx5e_build_icosq_param(stru
 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(priv->mdev, reg_umr_sq));
 }
 
+#ifdef HAVE_NETDEV_XDP
 static void mlx5e_build_xdpsq_param(struct mlx5e_priv *priv,
 				    struct mlx5e_params *params,
 				    struct mlx5e_sq_param *param)
@@ -2246,6 +2398,7 @@ static void mlx5e_build_xdpsq_param(stru
 	mlx5e_build_sq_param_common(priv, param);
 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
 }
+#endif
 
 static void mlx5e_build_channel_param(struct mlx5e_priv *priv,
 				      struct mlx5e_params *params,
@@ -2255,14 +2408,16 @@ static void mlx5e_build_channel_param(st
 
 	mlx5e_build_rq_param(priv, params, &cparam->rq);
 	mlx5e_build_sq_param(priv, params, &cparam->sq);
+#ifdef HAVE_NETDEV_XDP
 	mlx5e_build_xdpsq_param(priv, params, &cparam->xdp_sq);
+#endif
 	mlx5e_build_icosq_param(priv, icosq_log_wq_sz, &cparam->icosq);
 	mlx5e_build_rx_cq_param(priv, params, &cparam->rx_cq);
 	mlx5e_build_tx_cq_param(priv, params, &cparam->tx_cq);
 	mlx5e_build_ico_cq_param(priv, icosq_log_wq_sz, &cparam->icosq_cq);
 }
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 static void mlx5e_rl_cleanup(struct mlx5e_priv *priv)
 {
 	mlx5e_rl_remove_sysfs(priv);
@@ -2444,9 +2599,13 @@ void mlx5e_destroy_direct_rqts(struct ml
 
 static int mlx5e_rx_hash_fn(int hfunc)
 {
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	return (hfunc == ETH_RSS_HASH_TOP) ?
 	       MLX5_RX_HASH_FN_TOEPLITZ :
 	       MLX5_RX_HASH_FN_INVERTED_XOR8;
+#else
+	return MLX5_RX_HASH_FN_INVERTED_XOR8;
+#endif
 }
 
 static int mlx5e_bits_invert(unsigned long a, int size)
@@ -2471,7 +2630,9 @@ static void mlx5e_fill_rqt_rqns(struct m
 		if (rrp.is_rss) {
 			int ix = i;
 
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 			if (rrp.rss.hfunc == ETH_RSS_HASH_XOR)
+#endif
 				ix = mlx5e_bits_invert(i, ilog2(sz));
 
 			ix = priv->channels.params.indirection_rqt[ix];
@@ -2579,7 +2740,11 @@ static void mlx5e_redirect_rqts_to_drop(
 
 static void mlx5e_build_tir_ctx_lro(struct mlx5e_params *params, void *tirc)
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (!IS_HW_LRO(params))
+#else
 	if (!params->lro_en)
+#endif
 		return;
 
 #define ROUGH_MAX_L2_L3_HDR_SZ 256
@@ -2612,6 +2777,7 @@ void mlx5e_build_indir_tir_ctx_hash(stru
 				 MLX5_HASH_FIELD_SEL_IPSEC_SPI)
 
 	MLX5_SET(tirc, tirc, rx_hash_fn, mlx5e_rx_hash_fn(params->rss_hfunc));
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (params->rss_hfunc == ETH_RSS_HASH_TOP) {
 		void *rss_key = MLX5_ADDR_OF(tirc, tirc,
 					     rx_hash_toeplitz_key);
@@ -2621,6 +2787,7 @@ void mlx5e_build_indir_tir_ctx_hash(stru
 		MLX5_SET(tirc, tirc, rx_hash_symmetric, 1);
 		memcpy(rss_key, params->toeplitz_hash_key, len);
 	}
+#endif
 
 	switch (tt) {
 	case MLX5E_TT_IPV4_TCP:
@@ -2799,22 +2966,32 @@ static int mlx5e_set_dev_port_mtu(struct
 
 static void mlx5e_netdev_set_tcs(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_NETDEV_SET_TC_QUEUE
 	int nch = priv->channels.params.num_channels;
+#endif
 	int ntc = priv->channels.params.num_tc;
+#ifdef HAVE_NETDEV_SET_TC_QUEUE
 	int tc;
+#endif
 
+#ifdef HAVE_NETDEV_RESET_TC
 	netdev_reset_tc(priv->netdev);
+#endif
 
 	if (ntc == 1)
 		return;
 
+#ifdef HAVE_NETDEV_SET_NUM_TC
 	netdev_set_num_tc(priv->netdev, ntc);
+#endif
 
+#ifdef HAVE_NETDEV_SET_TC_QUEUE
 	/* Map netdev TCs to offset 0
 	 * We have our own UP to TXQ mapping for QoS
 	 */
 	for (tc = 0; tc < ntc; tc++)
 		netdev_set_tc_queue(priv->netdev, tc, nch, 0);
+#endif
 }
 
 static void mlx5e_build_channels_tx_maps(struct mlx5e_priv *priv)
@@ -2854,7 +3031,9 @@ void mlx5e_activate_priv_channels(struct
 
 	mlx5e_netdev_set_tcs(priv);
 	netif_set_real_num_tx_queues(netdev, num_txqs);
+#ifdef HAVE_NET_DEVICE_REAL_NUM_RX_QUEUES
 	netif_set_real_num_rx_queues(netdev, priv->channels.num);
+#endif
 
 	mlx5e_build_channels_tx_maps(priv);
 	mlx5e_activate_channels(&priv->channels);
@@ -2894,7 +3073,7 @@ int mlx5e_switch_priv_channels(struct ml
 	carrier_ok = netif_carrier_ok(netdev);
 	netif_carrier_off(netdev);
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_cleanup(priv);
 	new_num_txqs += new_chs->params.num_rl_txqs;
 #endif
@@ -2920,7 +3099,7 @@ int mlx5e_switch_priv_channels(struct ml
 activate_channels:
 	mlx5e_activate_priv_channels(priv);
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_init(priv, priv->channels.params);
 #endif
 
@@ -2951,7 +3130,7 @@ int mlx5e_open_locked(struct net_device
 	mlx5e_refresh_tirs(priv, false);
 	mlx5e_activate_priv_channels(priv);
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_init(priv, priv->channels.params);
 #endif
 
@@ -3003,7 +3182,7 @@ int mlx5e_close_locked(struct net_device
 
 	netif_carrier_off(priv->netdev);
 	mlx5e_destroy_debugfs(priv);
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_cleanup(priv);
 #endif
 	mlx5e_deactivate_priv_channels(priv);
@@ -3296,6 +3475,7 @@ void mlx5e_destroy_direct_tirs(struct ml
 		mlx5e_destroy_tir(priv->mdev, &priv->direct_tir[i]);
 }
 
+#ifdef HAVE_NETIF_F_RXFCS
 static int mlx5e_modify_channels_scatter_fcs(struct mlx5e_channels *chs, bool enable)
 {
 	int err = 0;
@@ -3309,8 +3489,12 @@ static int mlx5e_modify_channels_scatter
 
 	return 0;
 }
+#endif
 
-static int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd)
+#if !defined(LEGACY_ETHTOOL_OPS) && !defined(HAVE_GET_SET_FLAGS)
+static
+#endif
+int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd)
 {
 	int err = 0;
 	int i;
@@ -3349,18 +3533,26 @@ out:
 	return err;
 }
 
+#if defined(HAVE_NDO_SETUP_TC_4_PARAMS) || defined(HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX)
 static int mlx5e_ndo_setup_tc(struct net_device *dev, u32 handle,
+#ifdef HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX
 			      u32 chain_index, __be16 proto,
+#else
+			      __be16 proto,
+#endif
 			      struct tc_to_netdev *tc)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #ifdef CONFIG_MLX5_ESWITCH
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
 	if (TC_H_MAJ(handle) != TC_H_MAJ(TC_H_INGRESS))
 		goto mqprio;
 
+#ifdef HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX
 	if (chain_index)
 		return -EOPNOTSUPP;
+#endif
 
 	switch (tc->type) {
 	case TC_SETUP_CLSFLOWER:
@@ -3369,8 +3561,10 @@ static int mlx5e_ndo_setup_tc(struct net
 			return mlx5e_configure_flower(priv, proto, tc->cls_flower);
 		case TC_CLSFLOWER_DESTROY:
 			return mlx5e_delete_flower(priv, tc->cls_flower);
+#ifdef HAVE_TC_CLSFLOWER_STATS
 		case TC_CLSFLOWER_STATS:
 			return mlx5e_stats_flower(priv, tc->cls_flower);
+#endif
 		}
 	default:
 		return -EOPNOTSUPP;
@@ -3378,22 +3572,38 @@ static int mlx5e_ndo_setup_tc(struct net
 
 mqprio:
 #endif
+#endif /* HAVE_TC_FLOWER_OFFLOAD */
 	if (tc->type != TC_SETUP_MQPRIO)
 		return -EINVAL;
 
+#ifdef HAVE_TC_TO_NETDEV_TC
+	return mlx5e_setup_tc(dev, tc->tc);
+#else
 	tc->mqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;
 
 	return mlx5e_setup_tc(dev, tc->mqprio->num_tc);
+#endif
 }
+#endif
 
-static void
-mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+static
+#ifdef HAVE_NDO_GET_STATS64_RET_VOID
+void mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#elif defined(HAVE_NDO_GET_STATS64)
+struct rtnl_link_stats64 * mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#else
+struct net_device_stats * mlx5e_get_stats(struct net_device *dev)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5e_sw_stats *sstats = &priv->stats.sw;
 	struct mlx5e_vport_stats *vstats = &priv->stats.vport;
 	struct mlx5e_pport_stats *pstats = &priv->stats.pport;
 
+#if !defined(HAVE_NDO_GET_STATS64) && !defined(HAVE_NDO_GET_STATS64_RET_VOID)
+	struct net_device_stats *stats = &priv->netdev_stats;
+#endif
+
 	if (mlx5e_is_uplink_rep(priv)) {
 		stats->rx_packets = PPORT_802_3_GET(pstats, a_frames_received_ok);
 		stats->rx_bytes   = PPORT_802_3_GET(pstats, a_octets_received_ok);
@@ -3426,6 +3636,10 @@ mlx5e_get_stats(struct net_device *dev,
 	 */
 	stats->multicast =
 		VPORT_COUNTER_GET(vstats, received_eth_multicast.packets);
+
+#ifndef HAVE_NDO_GET_STATS64_RET_VOID
+	return stats;
+#endif
 }
 
 static void mlx5e_set_rx_mode(struct net_device *dev)
@@ -3462,6 +3676,7 @@ static int mlx5e_set_mac(struct net_devi
 
 typedef int (*mlx5e_feature_handler)(struct net_device *netdev, bool enable);
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int set_feature_lro(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3501,7 +3716,9 @@ static int set_feature_vlan_filter(struc
 
 	return 0;
 }
+#endif /* (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT)) */
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 static int set_feature_tc_num_filters(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3514,7 +3731,9 @@ static int set_feature_tc_num_filters(st
 
 	return 0;
 }
+#endif
 
+#ifdef HAVE_NETIF_F_RXALL
 static int set_feature_rx_all(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3522,7 +3741,9 @@ static int set_feature_rx_all(struct net
 
 	return mlx5_set_port_fcs(mdev, !enable);
 }
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 static int set_feature_rx_fcs(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3539,7 +3760,9 @@ static int set_feature_rx_fcs(struct net
 
 	return err;
 }
+#endif
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int set_feature_rx_vlan(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3560,6 +3783,7 @@ unlock:
 
 	return err;
 }
+#endif
 
 #ifdef CONFIG_RFS_ACCEL
 static int set_feature_arfs(struct net_device *netdev, bool enable)
@@ -3576,12 +3800,22 @@ static int set_feature_arfs(struct net_d
 }
 #endif
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int mlx5e_handle_feature(struct net_device *netdev,
+#ifndef HAVE_NET_DEVICE_OPS_EXT
 				netdev_features_t wanted_features,
 				netdev_features_t feature,
+#else
+				u32 wanted_features,
+				u32 feature,
+#endif
 				mlx5e_feature_handler feature_handler)
 {
+#ifndef HAVE_NET_DEVICE_OPS_EXT
 	netdev_features_t changes = wanted_features ^ netdev->features;
+#else
+	u32 changes = wanted_features ^ netdev->features;
+#endif
 	bool enable = !!(wanted_features & feature);
 	int err;
 
@@ -3590,17 +3824,28 @@ static int mlx5e_handle_feature(struct n
 
 	err = feature_handler(netdev, enable);
 	if (err) {
+#ifndef HAVE_NET_DEVICE_OPS_EXT
 		netdev_err(netdev, "%s feature 0x%llx failed err %d\n",
 			   enable ? "Enable" : "Disable", feature, err);
+#else
+		netdev_err(netdev, "%s feature 0x%ux failed err %d\n",
+			   enable ? "Enable" : "Disable", feature, err);
+#endif
 		return err;
 	}
 
 	MLX5E_SET_FEATURE(netdev, feature, enable);
 	return 0;
 }
+#endif
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int mlx5e_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			      u32 features)
+#else
 			      netdev_features_t features)
+#endif
 {
 	int err;
 
@@ -3609,12 +3854,18 @@ static int mlx5e_set_features(struct net
 	err |= mlx5e_handle_feature(netdev, features,
 				    NETIF_F_HW_VLAN_CTAG_FILTER,
 				    set_feature_vlan_filter);
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_HW_TC,
 				    set_feature_tc_num_filters);
+#endif
+#ifdef HAVE_NETIF_F_RXALL
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_RXALL,
 				    set_feature_rx_all);
+#endif
+#ifdef HAVE_NETIF_F_RXFCS
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_RXFCS,
 				    set_feature_rx_fcs);
+#endif
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_HW_VLAN_CTAG_RX,
 				    set_feature_rx_vlan);
 #ifdef CONFIG_RFS_ACCEL
@@ -3624,6 +3875,7 @@ static int mlx5e_set_features(struct net
 
 	return err ? -EINVAL : 0;
 }
+#endif
 
 #define MXL5_HW_MIN_MTU 64
 #define MXL5E_MIN_MTU (MXL5_HW_MIN_MTU + ETH_FCS_LEN)
@@ -3653,7 +3905,11 @@ static int mlx5e_change_mtu(struct net_d
 
 	mutex_lock(&priv->state_lock);
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	reset = !IS_HW_LRO(&priv->channels.params) &&
+#else
 	reset = !priv->channels.params.lro_en &&
+#endif
 		(priv->channels.params.rq_wq_type !=
 		 MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ);
 
@@ -3678,8 +3934,14 @@ out:
 	return err;
 }
 
+#ifdef HAVE_SIOCGHWTSTAMP
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr)
 {
+#else
+int mlx5e_hwstamp_ioctl(struct net_device *dev, struct ifreq *ifr)
+{
+       struct mlx5e_priv *priv = netdev_priv(dev);
+#endif
 	struct hwtstamp_config config;
 	int err;
 
@@ -3742,6 +4004,7 @@ int mlx5e_hwstamp_set(struct mlx5e_priv
 			    sizeof(config)) ? -EFAULT : 0;
 }
 
+#ifdef HAVE_SIOCGHWTSTAMP
 int mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr)
 {
 	struct hwtstamp_config *cfg = &priv->tstamp;
@@ -3751,6 +4014,7 @@ int mlx5e_hwstamp_get(struct mlx5e_priv
 
 	return copy_to_user(ifr->ifr_data, cfg, sizeof(*cfg)) ? -EFAULT : 0;
 }
+#endif
 
 static int mlx5e_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 {
@@ -3758,14 +4022,27 @@ static int mlx5e_ioctl(struct net_device
 
 	switch (cmd) {
 	case SIOCSHWTSTAMP:
+#ifdef HAVE_SIOCGHWTSTAMP
 		return mlx5e_hwstamp_set(priv, ifr);
 	case SIOCGHWTSTAMP:
 		return mlx5e_hwstamp_get(priv, ifr);
+#else
+		return mlx5e_hwstamp_ioctl(priv->netdev, ifr);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
 }
 
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+void mlx5e_vlan_register(struct net_device *netdev, struct vlan_group *grp)
+{
+        struct mlx5e_priv *priv = netdev_priv(netdev);
+        priv->channels.params.vlan_grp = grp;
+}
+#endif
+
+#ifdef HAVE_NDO_SET_VF_MAC
 #ifdef CONFIG_MLX5_ESWITCH
 static int mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac)
 {
@@ -3774,16 +4051,28 @@ static int mlx5e_set_vf_mac(struct net_d
 
 	return mlx5_eswitch_set_vport_mac(mdev->priv.eswitch, vf + 1, mac);
 }
-
+#endif
+#endif /* HAVE_NDO_SET_VF_MAC */
+ 
+#if defined(HAVE_NDO_SET_VF_VLAN) || defined(HAVE_NDO_SET_VF_VLAN_EXTENDED)
+#ifdef HAVE_VF_VLAN_PROTO
 static int mlx5e_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos,
 			     __be16 vlan_proto)
+#else
+static int mlx5e_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5_core_dev *mdev = priv->mdev;
-
+#ifndef HAVE_VF_VLAN_PROTO
+	__be16 vlan_proto = htons(ETH_P_8021Q);
+#endif
 	return mlx5_eswitch_set_vport_vlan(mdev->priv.eswitch, vf + 1,
 					   vlan, qos, vlan_proto);
 }
+#endif /* HAVE_NDO_SET_VF_VLAN */
+ 
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_TRUNK_RANGE
 
 static int mlx5e_add_vf_vlan_trunk_range(struct net_device *dev, int vf,
 					 u16 start_vid, u16 end_vid,
@@ -3812,7 +4101,9 @@ static int mlx5e_del_vf_vlan_trunk_range
 	return mlx5_eswitch_del_vport_trunk_range(mdev->priv.eswitch, vf + 1,
 						  start_vid, end_vid);
 }
+#endif
 
+#if defined(HAVE_VF_INFO_SPOOFCHK) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK)
 static int mlx5e_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -3820,7 +4111,9 @@ static int mlx5e_set_vf_spoofchk(struct
 
 	return mlx5_eswitch_set_vport_spoofchk(mdev->priv.eswitch, vf + 1, setting);
 }
+#endif
 
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_TRUST
 static int mlx5e_set_vf_trust(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -3828,17 +4121,40 @@ static int mlx5e_set_vf_trust(struct net
 
 	return mlx5_eswitch_set_vport_trust(mdev->priv.eswitch, vf + 1, setting);
 }
+#endif
 
+#ifdef HAVE_NDO_SET_VF_MAC
+#ifdef HAVE_VF_TX_RATE
+static int mlx5e_set_vf_rate(struct net_device *dev, int vf, int max_tx_rate)
+#else
 static int mlx5e_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate,
 			     int max_tx_rate)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5_core_dev *mdev = priv->mdev;
+#ifdef HAVE_VF_TX_RATE
+	struct mlx5_eswitch *esw = mdev->priv.eswitch;
+	int min_tx_rate;
+	int vport = vf + 1;
+
+	if (!esw || !MLX5_CAP_GEN(esw->dev, vport_group_manager) ||
+	    MLX5_CAP_GEN(esw->dev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
+		return -EPERM;
+	if (vport < 0 || vport >= esw->total_vports)
+		return -EINVAL;
+
+	mutex_lock(&esw->state_lock);
+	min_tx_rate = esw->vports[vport].info.min_rate;
+	mutex_unlock(&esw->state_lock);
+#endif
 
 	return mlx5_eswitch_set_vport_rate(mdev->priv.eswitch, vf + 1,
 					   max_tx_rate, min_tx_rate);
 }
+#endif
 
+#ifdef HAVE_LINKSTATE
 static int mlx5_vport_link2ifla(u8 esw_link)
 {
 	switch (esw_link) {
@@ -3861,6 +4177,8 @@ static int mlx5_ifla_link2vport(u8 ifla_
 	return MLX5_ESW_VPORT_ADMIN_STATE_AUTO;
 }
 
+#endif
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
 static int mlx5e_set_vf_link_state(struct net_device *dev, int vf,
 				   int link_state)
 {
@@ -3870,7 +4188,9 @@ static int mlx5e_set_vf_link_state(struc
 	return mlx5_eswitch_set_vport_state(mdev->priv.eswitch, vf + 1,
 					    mlx5_ifla_link2vport(link_state));
 }
+#endif
 
+#ifdef HAVE_NDO_SET_VF_MAC
 static int mlx5e_get_vf_config(struct net_device *dev,
 			       int vf, struct ifla_vf_info *ivi)
 {
@@ -3881,10 +4201,14 @@ static int mlx5e_get_vf_config(struct ne
 	err = mlx5_eswitch_get_vport_config(mdev->priv.eswitch, vf + 1, ivi);
 	if (err)
 		return err;
+#ifdef HAVE_LINKSTATE
 	ivi->linkstate = mlx5_vport_link2ifla(ivi->linkstate);
+#endif
 	return 0;
 }
+#endif
 
+#ifdef HAVE_NDO_GET_VF_STATS
 static int mlx5e_get_vf_stats(struct net_device *dev,
 			      int vf, struct ifla_vf_stats *vf_stats)
 {
@@ -3896,6 +4220,8 @@ static int mlx5e_get_vf_stats(struct net
 }
 #endif
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 static void mlx5e_add_vxlan_port(struct net_device *netdev,
 				 struct udp_tunnel_info *ti)
 {
@@ -3923,7 +4249,32 @@ static void mlx5e_del_vxlan_port(struct
 
 	mlx5e_vxlan_queue_work(priv, ti->sa_family, be16_to_cpu(ti->port), 0);
 }
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+static void mlx5e_add_vxlan_port(struct net_device *netdev,
+				 sa_family_t sa_family, __be16 port)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+
+	if (!mlx5e_vxlan_allowed(priv->mdev))
+		return;
+
+	mlx5e_vxlan_queue_work(priv, sa_family, be16_to_cpu(port), 1);
+}
+
+static void mlx5e_del_vxlan_port(struct net_device *netdev,
+				 sa_family_t sa_family, __be16 port)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+
+	if (!mlx5e_vxlan_allowed(priv->mdev))
+		return;
+
+	mlx5e_vxlan_queue_work(priv, sa_family, be16_to_cpu(port), 0);
+}
+#endif
+#endif /* HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON */
 
+#ifdef HAVE_NETDEV_FEATURES_T
 static netdev_features_t mlx5e_tunnel_features_check(struct mlx5e_priv *priv,
 						     struct sk_buff *skb,
 						     netdev_features_t features)
@@ -3946,6 +4297,7 @@ static netdev_features_t mlx5e_tunnel_fe
 	switch (proto) {
 	case IPPROTO_GRE:
 		return features;
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	case IPPROTO_UDP:
 		udph = udp_hdr(skb);
 		port = be16_to_cpu(udph->dest);
@@ -3953,6 +4305,7 @@ static netdev_features_t mlx5e_tunnel_fe
 		/* Verify if UDP port is being offloaded by HW */
 		if (mlx5e_vxlan_lookup_port(priv, port))
 			return features;
+#endif
 	}
 
 out:
@@ -3966,8 +4319,14 @@ static netdev_features_t mlx5e_features_
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 
+#ifdef HAVE_VLAN_FEATURES_CHECK
 	features = vlan_features_check(skb, features);
+#endif
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_VXLAN_FEATURES_CHECK
 	features = vxlan_features_check(skb, features);
+#endif
+#endif /* HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON */
 
 #ifdef CONFIG_MLX5_EN_IPSEC
 	if (mlx5e_ipsec_feature_check(skb, netdev, features))
@@ -3982,6 +4341,31 @@ static netdev_features_t mlx5e_features_
 	return features;
 }
 
+#elif defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON) && defined(HAVE_VXLAN_GSO_CHECK)
+static bool mlx5e_gso_check(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	struct udphdr *udph;
+	u16 port;
+
+	if (!vxlan_gso_check(skb))
+		return false;
+
+	if (!skb->encapsulation)
+		return true;
+
+	udph = udp_hdr(skb);
+	port = be16_to_cpu(udph->dest);
+
+	if (!mlx5e_vxlan_lookup_port(priv, port)) {
+		skb->ip_summed = CHECKSUM_NONE;
+		return false;
+	}
+
+	return true;
+}
+#endif
+
 void mlx5e_do_tx_timeout(struct mlx5e_priv *priv)
 {
 	bool sched_work = false;
@@ -3991,18 +4375,26 @@ void mlx5e_do_tx_timeout(struct mlx5e_pr
 #ifdef CONFIG_MLX5_EN_SPECIAL_SQ
 	num_sqs += priv->channels.params.num_rl_txqs;
 #endif
-	netdev_err(dev, "TX timeout detected\n");
+	netdev_err(priv->netdev, "TX timeout detected\n");
 
+#if (defined(HAVE_NETIF_XMIT_STOPPED) || defined(HAVE_NETIF_TX_QUEUE_STOPPED)) && defined (HAVE_NETDEV_GET_TX_QUEUE)
 	for (i = 0; i < num_sqs; i++) {
 		struct mlx5e_txqsq *sq = priv->txq2sq[i];
 
+#if defined(HAVE_NETIF_XMIT_STOPPED)
 		if (!netif_xmit_stopped(netdev_get_tx_queue(priv->netdev, i)))
+#else
+		if (!netif_tx_queue_stopped(netdev_get_tx_queue(priv->netdev, i)))
+#endif
 			continue;
 		sched_work = true;
 		clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 		netdev_err(priv->netdev, "TX timeout on queue: %d, SQ: 0x%x, CQ: 0x%x, SQ Cons: 0x%x SQ Prod: 0x%x\n",
 			   i, sq->sqn, sq->cq.mcq.cqn, sq->cc, sq->pc);
 	}
+#else
+	sched_work = true;
+#endif
 
 	if (sched_work && test_bit(MLX5E_STATE_OPENED, &priv->state))
 		schedule_work(&priv->tx_timeout_work);
@@ -4015,6 +4407,7 @@ static void mlx5e_tx_timeout(struct net_
 	return mlx5e_do_tx_timeout(priv);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static int mlx5e_xdp_set(struct net_device *netdev, struct bpf_prog *prog)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -4031,11 +4424,13 @@ static int mlx5e_xdp_set(struct net_devi
 		goto unlock;
 	}
 
+#ifdef CONFIG_MLX5_EN_IPSEC
 	if ((netdev->features & NETIF_F_HW_ESP) && prog) {
 		netdev_warn(netdev, "can't set XDP with IPSec offload\n");
 		err = -EINVAL;
 		goto unlock;
 	}
+#endif
 
 	was_opened = test_bit(MLX5E_STATE_OPENED, &priv->state);
 	/* no need for full reset when exchanging programs */
@@ -4096,6 +4491,7 @@ unlock:
 	return err;
 }
 
+#ifdef HAVE_BPF_PROG_AUX_FEILD_ID
 static u32 mlx5e_xdp_query(struct net_device *dev)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -4110,6 +4506,14 @@ static u32 mlx5e_xdp_query(struct net_de
 
 	return prog_id;
 }
+#else /* HAVE_BPF_PROG_AUX_FEILD_ID */
+static bool mlx5e_xdp_attached(struct net_device *dev)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+
+	return !!priv->channels.params.xdp_prog;
+}
+#endif /* HAVE_BPF_PROG_AUX_FEILD_ID */
 
 static int mlx5e_xdp(struct net_device *dev, struct netdev_xdp *xdp)
 {
@@ -4117,13 +4521,18 @@ static int mlx5e_xdp(struct net_device *
 	case XDP_SETUP_PROG:
 		return mlx5e_xdp_set(dev, xdp->prog);
 	case XDP_QUERY_PROG:
+#ifdef HAVE_BPF_PROG_AUX_FEILD_ID
 		xdp->prog_id = mlx5e_xdp_query(dev);
 		xdp->prog_attached = !!xdp->prog_id;
+#else
+		xdp->prog_attached = mlx5e_xdp_attached(dev);
+#endif
 		return 0;
 	default:
 		return -EINVAL;
 	}
 }
+#endif
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
 /* Fake "interrupt" called by netpoll (eg netconsole) to send skbs without
@@ -4145,45 +4554,118 @@ static const struct net_device_ops mlx5e
 	.ndo_open                = mlx5e_open,
 	.ndo_stop                = mlx5e_close,
 	.ndo_start_xmit          = mlx5e_xmit,
+#ifdef HAVE_NDO_SETUP_TC
+#if defined(HAVE_NDO_SETUP_TC_4_PARAMS) || defined(HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX)
 	.ndo_setup_tc            = mlx5e_ndo_setup_tc,
+#else  /* HAVE_NDO_SETUP_TC_4_PARAMS */
+	.ndo_setup_tc            = mlx5e_setup_tc,
+#endif /* HAVE_NDO_SETUP_TC_4_PARAMS */
+#endif /* HAVE_NDO_SETUP_TC */
 	.ndo_select_queue        = mlx5e_select_queue,
+#if defined(HAVE_NDO_GET_STATS64) || defined(HAVE_NDO_GET_STATS64_RET_VOID)
 	.ndo_get_stats64         = mlx5e_get_stats,
+#else
+	.ndo_get_stats           = mlx5e_get_stats,
+#endif
 	.ndo_set_rx_mode         = mlx5e_set_rx_mode,
 	.ndo_set_mac_address     = mlx5e_set_mac,
 	.ndo_vlan_rx_add_vid     = mlx5e_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid    = mlx5e_vlan_rx_kill_vid,
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+	.ndo_vlan_rx_register    = mlx5e_vlan_register,
+#endif
+#if (defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_features        = mlx5e_set_features,
+#endif
 	.ndo_change_mtu          = mlx5e_change_mtu,
 	.ndo_do_ioctl            = mlx5e_ioctl,
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	.ndo_set_tx_maxrate      = mlx5e_set_tx_maxrate,
+#endif
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 	.ndo_udp_tunnel_add      = mlx5e_add_vxlan_port,
 	.ndo_udp_tunnel_del      = mlx5e_del_vxlan_port,
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+	.ndo_add_vxlan_port	 = mlx5e_add_vxlan_port,
+	.ndo_del_vxlan_port	 = mlx5e_del_vxlan_port,
+#endif
+#endif
+#ifdef HAVE_NETDEV_FEATURES_T
 	.ndo_features_check      = mlx5e_features_check,
+#elif defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON) && defined(HAVE_VXLAN_GSO_CHECK)
+	.ndo_gso_check           = mlx5e_gso_check,
+#endif
+#ifdef HAVE_NDO_RX_FLOW_STEER
 #ifdef CONFIG_RFS_ACCEL
 	.ndo_rx_flow_steer	 = mlx5e_rx_flow_steer,
 #endif
+#endif
 	.ndo_tx_timeout          = mlx5e_tx_timeout,
+#ifdef HAVE_NETDEV_XDP
 	.ndo_xdp		 = mlx5e_xdp,
+#endif
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller     = mlx5e_netpoll,
 #endif
 #ifdef CONFIG_MLX5_ESWITCH
 	/* SRIOV E-Switch NDOs */
+#ifdef HAVE_NDO_SET_VF_MAC
 	.ndo_set_vf_mac          = mlx5e_set_vf_mac,
+#endif
+#if defined(HAVE_NDO_SET_VF_VLAN)
 	.ndo_set_vf_vlan         = mlx5e_set_vf_vlan,
+#elif defined(HAVE_NDO_SET_VF_VLAN_EXTENDED)
+	.extended.ndo_set_vf_vlan  = mlx5e_set_vf_vlan,
+#endif
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_TRUNK_RANGE
 	.ndo_add_vf_vlan_trunk_range = mlx5e_add_vf_vlan_trunk_range,
 	.ndo_del_vf_vlan_trunk_range = mlx5e_del_vf_vlan_trunk_range,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_spoofchk     = mlx5e_set_vf_spoofchk,
+#endif
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_TRUST
 	.ndo_set_vf_trust        = mlx5e_set_vf_trust,
+#endif
+#ifdef HAVE_NDO_SET_VF_MAC
+#ifndef HAVE_VF_TX_RATE
 	.ndo_set_vf_rate         = mlx5e_set_vf_rate,
+#else
+	.ndo_set_vf_tx_rate      = mlx5e_set_vf_rate,
+#endif
+#endif
+#ifdef HAVE_NDO_SET_VF_MAC
 	.ndo_get_vf_config       = mlx5e_get_vf_config,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_link_state   = mlx5e_set_vf_link_state,
+#endif
+#ifdef HAVE_NDO_GET_VF_STATS
 	.ndo_get_vf_stats        = mlx5e_get_vf_stats,
+#endif
+#ifdef NDO_HAS_OFFLOAD_STATS_GETS_NET_DEVICE
 	.ndo_has_offload_stats	 = mlx5e_has_offload_stats,
+#endif
+#ifdef HAVE_NDO_GET_OFFLOAD_STATS
 	.ndo_get_offload_stats	 = mlx5e_get_offload_stats,
 #endif
+#endif /* CONFIG_MLX5_ESWITCH */
 };
 
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+static const struct net_device_ops_ext mlx5e_netdev_ops_ext= {
+	.size             = sizeof(struct net_device_ops_ext),
+	.ndo_set_features = mlx5e_set_features,
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK
+	.ndo_set_vf_spoofchk    = mlx5e_set_vf_spoofchk,
+#endif
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE
+	.ndo_set_vf_link_state  = mlx5e_set_vf_link_state,
+#endif
+};
+#endif /* HAVE_NET_DEVICE_OPS_EXT */
+
 static int mlx5e_check_required_hca_cap(struct mlx5_core_dev *mdev)
 {
 	if (MLX5_CAP_GEN(mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
@@ -4318,6 +4800,12 @@ void mlx5e_build_nic_params(struct mlx5_
 			    struct mlx5e_params *params,
 			    u16 max_channels)
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_channels *channels = container_of(params, struct mlx5e_channels,
+						       params);
+	struct mlx5e_priv *priv = container_of(channels, struct mlx5e_priv,
+					       channels);
+#endif
 	u8 cq_period_mode = 0;
 	u32 link_speed = 0;
 	u32 pci_bw = 0;
@@ -4357,6 +4845,11 @@ void mlx5e_build_nic_params(struct mlx5_
 	cq_period_mode = MLX5_CAP_GEN(mdev, cq_period_start_from_cqe) ?
 			MLX5_CQ_PERIOD_MODE_START_FROM_CQE :
 			MLX5_CQ_PERIOD_MODE_START_FROM_EQE;
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	MLX5E_SET_PFLAG(params, MLX5E_PFLAG_HWLRO, priv->channels.params.lro_en);
+#endif
+
 	params->rx_am_enabled = MLX5_CAP_GEN(mdev, cq_moderation);
 	mlx5e_set_rx_cq_mode_params(params, cq_period_mode);
 
@@ -4371,7 +4864,9 @@ void mlx5e_build_nic_params(struct mlx5_
 		params->tx_min_inline_mode = MLX5_INLINE_MODE_L2;
 
 	/* RSS */
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	params->rss_hfunc = ETH_RSS_HASH_XOR;
+#endif
 	netdev_rss_key_fill(params->toeplitz_hash_key, sizeof(params->toeplitz_hash_key));
 	mlx5e_build_default_indir_rqt(params->indirection_rqt,
 				      MLX5E_INDIR_RQT_SIZE, max_channels);
@@ -4421,31 +4916,42 @@ static void mlx5e_set_netdev_dev_addr(st
 	}
 }
 
+#ifdef HAVE_SWITCHDEV_OPS
 #if IS_ENABLED(CONFIG_NET_SWITCHDEV) && IS_ENABLED(CONFIG_MLX5_ESWITCH)
 static const struct switchdev_ops mlx5e_switchdev_ops = {
 	.switchdev_port_attr_get	= mlx5e_attr_get,
 };
 #endif
+#endif
 
 static void mlx5e_build_nic_netdev(struct net_device *netdev)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct mlx5_core_dev *mdev = priv->mdev;
+#ifdef HAVE_NETDEV_HW_FEATURES
 	bool fcs_supported;
 	bool fcs_enabled;
+#endif
 
 	SET_NETDEV_DEV(netdev, &mdev->pdev->dev);
 
 	netdev->netdev_ops = &mlx5e_netdev_ops;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	if (MLX5_CAP_GEN(mdev, vport_group_manager) && MLX5_CAP_GEN(mdev, qos))
 		netdev->dcbnl_ops = &mlx5e_dcbnl_ops;
 #endif
+#endif
 
 	netdev->watchdog_timeo    = 15 * HZ;
 
+#ifdef HAVE_ETHTOOL_OPS_EXT
+	SET_ETHTOOL_OPS(netdev, &mlx5e_ethtool_ops);
+	set_ethtool_ops_ext(netdev, &mlx5e_ethtool_ops_ext);
+#else
 	netdev->ethtool_ops	  = &mlx5e_ethtool_ops;
+#endif
 
 	netdev->vlan_features    |= NETIF_F_SG;
 	netdev->vlan_features    |= NETIF_F_IP_CSUM;
@@ -4454,81 +4960,151 @@ static void mlx5e_build_nic_netdev(struc
 	netdev->vlan_features    |= NETIF_F_TSO;
 	netdev->vlan_features    |= NETIF_F_TSO6;
 	netdev->vlan_features    |= NETIF_F_RXCSUM;
+#ifdef HAVE_NETIF_F_RXHASH
 	netdev->vlan_features    |= NETIF_F_RXHASH;
+#endif
 
 	if (!!MLX5_CAP_ETH(mdev, lro_cap))
 		netdev->vlan_features    |= NETIF_F_LRO;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	netdev->hw_features       = netdev->vlan_features;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_TX;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_RX;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_FILTER;
 
+#if defined(HAVE_NETDEV_FEATURES_T) || defined(HAVE_NDO_GSO_CHECK)
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	if (mlx5e_vxlan_allowed(mdev) || MLX5_CAP_ETH(mdev, tunnel_stateless_gre)) {
+#else
+	if (MLX5_CAP_ETH(mdev, tunnel_stateless_gre)) {
+#endif
+#ifdef HAVE_NETIF_F_GSO_PARTIAL
 		netdev->hw_features     |= NETIF_F_GSO_PARTIAL;
+#endif
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 		netdev->hw_enc_features |= NETIF_F_IP_CSUM;
 		netdev->hw_enc_features |= NETIF_F_IPV6_CSUM;
 		netdev->hw_enc_features |= NETIF_F_TSO;
 		netdev->hw_enc_features |= NETIF_F_TSO6;
+#ifdef HAVE_NETIF_F_GSO_PARTIAL
 		netdev->hw_enc_features |= NETIF_F_GSO_PARTIAL;
+#endif
+#endif
 	}
+#endif
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	if (mlx5e_vxlan_allowed(mdev)) {
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL
 		netdev->hw_features     |= NETIF_F_GSO_UDP_TUNNEL |
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL_CSUM
 					   NETIF_F_GSO_UDP_TUNNEL_CSUM;
+#else
+					   0;
+#endif
+#endif
+
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL
 		netdev->hw_enc_features |= NETIF_F_GSO_UDP_TUNNEL |
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL_CSUM
 					   NETIF_F_GSO_UDP_TUNNEL_CSUM;
+#else
+					   0;
+#endif
+#endif
+#endif
+
+#ifdef HAVE_NETIF_F_GSO_PARTIAL
 		netdev->gso_partial_features = NETIF_F_GSO_UDP_TUNNEL_CSUM;
+#endif
 	}
+#endif
 
 	if (MLX5_CAP_ETH(mdev, tunnel_stateless_gre)) {
+#ifdef HAVE_NETIF_F_GSO_GRE_CSUM
 		netdev->hw_features     |= NETIF_F_GSO_GRE |
 					   NETIF_F_GSO_GRE_CSUM;
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 		netdev->hw_enc_features |= NETIF_F_GSO_GRE |
 					   NETIF_F_GSO_GRE_CSUM;
+#endif
+#endif
+#ifdef HAVE_NETIF_F_GSO_PARTIAL
 		netdev->gso_partial_features |= NETIF_F_GSO_GRE |
 						NETIF_F_GSO_GRE_CSUM;
+#endif
 	}
 
 	mlx5_query_port_fcs(mdev, &fcs_supported, &fcs_enabled);
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (fcs_supported)
 		netdev->hw_features |= NETIF_F_RXALL;
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (MLX5_CAP_ETH(mdev, scatter_fcs))
 		netdev->hw_features |= NETIF_F_RXFCS;
+#endif
 
 	netdev->features          = netdev->hw_features;
+#else
+	netdev->features       = netdev->vlan_features;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_TX;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_RX;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_FILTER;
+#ifdef HAVE_SET_NETDEV_HW_FEATURES
+	set_netdev_hw_features(netdev, netdev->features);
+#endif
+#endif
 	if (!priv->channels.params.lro_en)
 		netdev->features  &= ~NETIF_F_LRO;
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (fcs_enabled)
 		netdev->features  &= ~NETIF_F_RXALL;
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (!priv->channels.params.scatter_fcs_en)
 		netdev->features  &= ~NETIF_F_RXFCS;
+#endif
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 #define FT_CAP(f) MLX5_CAP_FLOWTABLE(mdev, flow_table_properties_nic_receive.f)
 	if (FT_CAP(flow_modify_en) &&
 	    FT_CAP(modify_root) &&
 	    FT_CAP(identified_miss_table_mode) &&
 	    FT_CAP(flow_table_modify)) {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 		netdev->hw_features      |= NETIF_F_HW_TC;
+#endif
 #ifdef CONFIG_RFS_ACCEL
 		netdev->hw_features	 |= NETIF_F_NTUPLE;
 #endif
 	}
+#endif
 
 	netdev->features         |= NETIF_F_HIGHDMA;
 
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	netdev->priv_flags       |= IFF_UNICAST_FLT;
+#endif
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+	set_netdev_ops_ext(netdev, &mlx5e_netdev_ops_ext);
+#endif
 
 	mlx5e_set_netdev_dev_addr(netdev);
 
+#ifdef HAVE_SWITCHDEV_OPS
 #if IS_ENABLED(CONFIG_NET_SWITCHDEV) && IS_ENABLED(CONFIG_MLX5_ESWITCH)
 	if (MLX5_VPORT_MANAGER(mdev))
 		netdev->switchdev_ops = &mlx5e_switchdev_ops;
 #endif
+#endif
 
 	mlx5e_ipsec_build_netdev(priv);
 }
@@ -4572,8 +5148,10 @@ static void mlx5e_nic_cleanup(struct mlx
 {
 	mlx5e_ipsec_cleanup(priv);
 
+#ifdef HAVE_NETDEV_XDP
 	if (priv->channels.params.xdp_prog)
 		bpf_prog_put(priv->channels.params.xdp_prog);
+#endif
 }
 
 static int mlx5e_init_nic_rx(struct mlx5e_priv *priv)
@@ -4607,7 +5185,9 @@ static int mlx5e_init_nic_rx(struct mlx5
 	if (err)
 		goto err_destroy_flow_steering;
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	mlx5e_vxlan_init(priv);
+#endif
 	return 0;
 
 err_destroy_flow_steering:
@@ -4625,7 +5205,9 @@ err_destroy_indirect_rqts:
 
 static void mlx5e_cleanup_nic_rx(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	mlx5e_vxlan_cleanup(priv);
+#endif
 	mlx5e_tc_cleanup(priv);
 	mlx5e_destroy_flow_steering(priv);
 	mlx5e_destroy_direct_tirs(priv);
@@ -4643,10 +5225,11 @@ static int mlx5e_init_nic_tx(struct mlx5
 		mlx5_core_warn(priv->mdev, "create tises failed, %d\n", err);
 		return err;
 	}
-
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	mlx5e_dcbnl_initialize(priv);
 #endif
+#endif
 	return 0;
 }
 
@@ -4654,7 +5237,9 @@ static void mlx5e_nic_enable(struct mlx5
 {
 	struct net_device *netdev = priv->netdev;
 	struct mlx5_core_dev *mdev = priv->mdev;
+#ifdef HAVE_NET_DEVICE_MIN_MAX_MTU
 	u16 max_mtu;
+#endif
 
 	mlx5e_init_l2_addr(priv);
 
@@ -4662,14 +5247,19 @@ static void mlx5e_nic_enable(struct mlx5
 	if (!netif_running(netdev))
 		mlx5_set_port_admin_status(mdev, MLX5_PORT_DOWN);
 
+#ifdef HAVE_NET_DEVICE_MIN_MAX_MTU
 	/* MTU range: 68 - hw-specific max */
 	netdev->min_mtu = ETH_MIN_MTU;
 	mlx5_query_port_max_mtu(priv->mdev, &max_mtu, 1);
 	netdev->max_mtu = MLX5E_HW2SW_MTU(priv, max_mtu);
+#endif
 	mlx5e_set_dev_port_mtu(priv);
 
 	mlx5_lag_add(mdev, netdev);
 
+	if (!is_valid_ether_addr(netdev->perm_addr))
+		memcpy(netdev->perm_addr, netdev->dev_addr, netdev->addr_len);
+
 	mlx5e_enable_async_events(priv);
 
 	if (MLX5_VPORT_MANAGER(priv->mdev))
@@ -4678,16 +5268,24 @@ static void mlx5e_nic_enable(struct mlx5
 	if (netdev->reg_state != NETREG_REGISTERED)
 		return;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	mlx5e_dcbnl_init_app(priv);
 #endif
+#endif
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	/* Device already registered: sync netdev system state */
 	if (mlx5e_vxlan_allowed(mdev)) {
 		rtnl_lock();
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 		udp_tunnel_get_rx_info(netdev);
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+		vxlan_get_rx_port(netdev);
+#endif
 		rtnl_unlock();
 	}
+#endif /* HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON */
 
 	queue_work(priv->wq, &priv->set_rx_mode_work);
 
@@ -4704,10 +5302,12 @@ static void mlx5e_nic_disable(struct mlx
 {
 	struct mlx5_core_dev *mdev = priv->mdev;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	if (priv->netdev->reg_state == NETREG_REGISTERED)
 		mlx5e_dcbnl_delete_app(priv);
 #endif
+#endif
 
 	rtnl_lock();
 	if (netif_running(priv->netdev))
@@ -4751,17 +5351,27 @@ struct net_device *mlx5e_create_netdev(s
 	struct net_device *netdev;
 	struct mlx5e_priv *priv;
 
+#ifdef HAVE_NEW_TX_RING_SCHEME
 	netdev = alloc_etherdev_mqs(sizeof(struct mlx5e_priv),
 				    nch * profile->max_tc + MLX5E_MAX_RL_QUEUES,
 				    nch);
+#else
+	netdev = alloc_etherdev_mq(sizeof(struct mlx5e_priv),
+				   nch * profile->max_tc + MLX5E_MAX_RL_QUEUES);
+#ifdef HAVE_NETIF_SET_REAL_NUM_RX_QUEUES
+	netif_set_real_num_rx_queues(netdev, nch);
+#endif
+#endif
 	if (!netdev) {
 		mlx5_core_err(mdev, "alloc_etherdev_mqs() failed\n");
 		return NULL;
 	}
 
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #ifdef CONFIG_RFS_ACCEL
 	netdev->rx_cpu_rmap = mdev->rmap;
 #endif
+#endif
 
 	profile->init(mdev, netdev, profile, ppriv);
 
@@ -4933,9 +5543,11 @@ static void *mlx5e_add(struct mlx5_core_
 	if (err)
 		goto err_unregister_netdev;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	mlx5e_dcbnl_init_app(priv);
 #endif
+#endif
 
 	return priv;
 
@@ -4956,9 +5568,12 @@ static void mlx5e_remove(struct mlx5_cor
 	struct mlx5e_priv *priv = vpriv;
 	void *ppriv = priv->ppriv;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	mlx5e_dcbnl_delete_app(priv);
 #endif
+#endif
+
 	mlx5e_sysfs_remove(priv->netdev);
 	unregister_netdev(priv->netdev);
 	mlx5e_detach(mdev, vpriv);
@@ -4986,7 +5601,9 @@ static struct mlx5_interface mlx5e_inter
 void mlx5e_init(void)
 {
 	mlx5e_ipsec_build_inverse_table();
+#ifdef __ETHTOOL_DECLARE_LINK_MODE_MASK
 	mlx5e_build_ptys2ethtool_map();
+#endif
 	mlx5_register_interface(&mlx5e_interface);
 }
 
