From: Tariq Toukan <tariqt@mellanox.com>
Subject: [PATCH] BACKPORT: mlx4

Change-Id: I877fa91653bde599736b2019f81c1327063d4f70
Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
---
 drivers/infiniband/hw/mlx4/alias_GUID.c            |  16 +
 drivers/infiniband/hw/mlx4/cm.c                    |  27 +
 drivers/infiniband/hw/mlx4/main.c                  |  23 +
 drivers/infiniband/hw/mlx4/main_exp.c              |  54 ++
 drivers/infiniband/hw/mlx4/mlx4_ib_exp.h           |   7 +
 drivers/infiniband/hw/mlx4/mr_exp.c                |  17 +
 drivers/net/ethernet/mellanox/mlx4/cmd.c           |  67 +-
 drivers/net/ethernet/mellanox/mlx4/en_clock.c      |  40 +
 drivers/net/ethernet/mellanox/mlx4/en_cq.c         |  13 +
 drivers/net/ethernet/mellanox/mlx4/en_dcb_nl.c     |  39 +
 drivers/net/ethernet/mellanox/mlx4/en_ethtool.c    | 879 ++++++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx4/en_main.c       |   6 +
 drivers/net/ethernet/mellanox/mlx4/en_netdev.c     | 776 +++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx4/en_rx.c         | 271 ++++++-
 drivers/net/ethernet/mellanox/mlx4/en_tx.c         |  85 ++
 drivers/net/ethernet/mellanox/mlx4/eq.c            |   9 +
 drivers/net/ethernet/mellanox/mlx4/fw.c            |  13 +-
 drivers/net/ethernet/mellanox/mlx4/intf.c          |   4 +
 drivers/net/ethernet/mellanox/mlx4/main.c          |  72 +-
 drivers/net/ethernet/mellanox/mlx4/mlx4.h          |   4 +
 drivers/net/ethernet/mellanox/mlx4/mlx4_en.h       | 317 +++++++-
 drivers/net/ethernet/mellanox/mlx4/mlx4_stats.h    |   9 +
 drivers/net/ethernet/mellanox/mlx4/pd.c            |   6 +
 .../net/ethernet/mellanox/mlx4/resource_tracker.c  |  10 +
 include/linux/mlx4/cmd.h                           |   3 +
 include/linux/mlx4/cq.h                            |   4 +
 include/linux/mlx4/cq_exp.h                        |   2 +
 include/linux/mlx4/device.h                        |   9 +-
 include/linux/mlx4/driver.h                        |   4 +
 29 files changed, 2754 insertions(+), 32 deletions(-)

--- a/drivers/infiniband/hw/mlx4/alias_GUID.c
+++ b/drivers/infiniband/hw/mlx4/alias_GUID.c
@@ -310,7 +310,11 @@ static void aliasguid_query_handler(int
 	if (status) {
 		pr_debug("(port: %d) failed: status = %d\n",
 			 cb_ctx->port, status);
+#ifdef HAVE_KTIME_GET_BOOT_NS
 		rec->time_to_run = ktime_get_boot_ns() + 1 * NSEC_PER_SEC;
+#else
+		rec->time_to_run = ktime_get_real_ns() + 1 * NSEC_PER_SEC;
+#endif
 		goto out;
 	}
 
@@ -416,7 +420,11 @@ next_entry:
 			 be64_to_cpu((__force __be64)rec->guid_indexes),
 			 be64_to_cpu((__force __be64)applied_guid_indexes),
 			 be64_to_cpu((__force __be64)declined_guid_indexes));
+#ifdef HAVE_KTIME_GET_BOOT_NS
 		rec->time_to_run = ktime_get_boot_ns() +
+#else
+		rec->time_to_run = ktime_get_real_ns() +
+#endif
 			resched_delay_sec * NSEC_PER_SEC;
 	} else {
 		rec->status = MLX4_GUID_INFO_STATUS_SET;
@@ -627,7 +635,11 @@ void mlx4_ib_invalidate_all_guid_record(
 		queued(not on the timer) the cancel will fail. That is not a problem
 		because we just want the work started.
 		*/
+#ifdef HAVE___CANCEL_DELAYED_WORK
+		__cancel_delayed_work(&dev->sriov.alias_guid.
+#else
 		cancel_delayed_work(&dev->sriov.alias_guid.
+#endif
 				      ports_guid[port - 1].alias_guid_work);
 		queue_delayed_work(dev->sriov.alias_guid.ports_guid[port - 1].wq,
 				   &dev->sriov.alias_guid.ports_guid[port - 1].alias_guid_work,
@@ -708,7 +720,11 @@ static int get_low_record_time_index(str
 		}
 	}
 	if (resched_delay_sec) {
+#ifdef HAVE_KTIME_GET_BOOT_NS
 		u64 curr_time = ktime_get_boot_ns();
+#else
+		u64 curr_time = ktime_get_real_ns();
+#endif
 
 		*resched_delay_sec = (low_record_time < curr_time) ? 0 :
 			div_u64((low_record_time - curr_time), NSEC_PER_SEC);
--- a/drivers/infiniband/hw/mlx4/cm.c
+++ b/drivers/infiniband/hw/mlx4/cm.c
@@ -242,7 +242,12 @@ static void sl_id_map_add(struct ib_devi
 static struct id_map_entry *
 id_map_alloc(struct ib_device *ibdev, int slave_id, u32 sl_cm_id)
 {
+#ifndef HAVE_IDR_ALLOC_CYCLIC
+	int ret, id;
+	static int next_id;
+#else
 	int ret;
+#endif
 	struct id_map_entry *ent;
 	struct mlx4_ib_sriov *sriov = &to_mdev(ibdev)->sriov;
 
@@ -258,6 +263,27 @@ id_map_alloc(struct ib_device *ibdev, in
 	ent->dev = to_mdev(ibdev);
 	INIT_DELAYED_WORK(&ent->timeout, id_map_ent_timeout);
 
+#ifndef HAVE_IDR_ALLOC_CYCLIC
+	do {
+		spin_lock(&to_mdev(ibdev)->sriov.id_map_lock);
+		ret = idr_get_new_above(&sriov->pv_id_table, ent,
+					next_id, &id);
+		if (!ret) {
+			next_id = max(id + 1, 0);
+			ent->pv_cm_id = (u32)id;
+			sl_id_map_add(ibdev, ent);
+		}
+
+		spin_unlock(&sriov->id_map_lock);
+	} while (ret == -EAGAIN && idr_pre_get(&sriov->pv_id_table, GFP_KERNEL));
+	/*the function idr_get_new_above can return -ENOSPC, so don't insert in that case.*/
+	if (!ret) {
+		spin_lock(&sriov->id_map_lock);
+		list_add_tail(&ent->list, &sriov->cm_list);
+		spin_unlock(&sriov->id_map_lock);
+		return ent;
+	}
+#else
 	idr_preload(GFP_KERNEL);
 	spin_lock(&to_mdev(ibdev)->sriov.id_map_lock);
 
@@ -273,6 +299,7 @@ id_map_alloc(struct ib_device *ibdev, in
 
 	if (ret >= 0)
 		return ent;
+#endif
 
 	/*error flow*/
 	kfree(ent);
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@ -41,7 +41,9 @@
 #include <linux/if_vlan.h>
 #include <net/ipv6.h>
 #include <net/addrconf.h>
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 
 #include <rdma/ib_smi.h>
 #include <rdma/ib_user_verbs.h>
@@ -58,6 +60,16 @@
 #include "mlx4_ib.h"
 #include <rdma/mlx4-abi.h>
 
+#ifdef DRV_NAME
+#undef DRV_NAME
+#endif
+#ifdef DRV_VERSION
+#undef DRV_VERSION
+#endif
+#ifdef DRV_RELDATE
+#undef DRV_RELDATE
+#endif
+
 #define DRV_NAME	MLX4_IB_DRV_NAME
 #define DRV_VERSION	"4.0-2.0.0"
 #define DRV_RELDATE	"28 Mar 2017"
@@ -172,6 +184,7 @@ static struct net_device *mlx4_ib_get_ne
 	dev = mlx4_get_protocol_dev(ibdev->dev, MLX4_PROT_ETH, port_num);
 
 	if (dev) {
+#ifdef HAVE_BONDING_H
 		if (mlx4_is_bonded(ibdev->dev)) {
 			struct net_device *upper = NULL;
 
@@ -184,6 +197,7 @@ static struct net_device *mlx4_ib_get_ne
 					dev = active;
 			}
 		}
+#endif
 	}
 	if (dev)
 		dev_hold(dev);
@@ -2969,6 +2983,9 @@ static void *mlx4_ib_add(struct mlx4_dev
 	ibdev->ib_dev.exp_create_qp	= mlx4_ib_exp_create_qp;
 	ibdev->ib_dev.exp_query_device	= mlx4_ib_exp_query_device;
 	ibdev->ib_dev.exp_ioctl		= mlx4_ib_exp_ioctl;
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE
+	ibdev->ib_dev.exp_get_unmapped_area = mlx4_ib_exp_get_unmapped_area;
+#endif
 	ibdev->ib_dev.rereg_user_mr	= mlx4_ib_rereg_user_mr;
 	ibdev->ib_dev.dereg_mr		= mlx4_ib_dereg_mr;
 	ibdev->ib_dev.alloc_mr		= mlx4_ib_alloc_mr;
@@ -3165,9 +3182,11 @@ static void *mlx4_ib_add(struct mlx4_dev
 	}
 
 	ibdev->ib_active = true;
+#ifdef HAVE_DEVLINK_H
 	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)
 		devlink_port_type_ib_set(mlx4_get_devlink_port(dev, i),
 					 &ibdev->ib_dev);
+#endif
 
 	if (mlx4_is_mfunc(ibdev->dev))
 		init_pkeys(ibdev);
@@ -3300,10 +3319,14 @@ static void mlx4_ib_remove(struct mlx4_d
 	struct mlx4_ib_dev *ibdev = ibdev_ptr;
 	int dev_idx, ret;
 	int p;
+#ifdef HAVE_DEVLINK_H
 	int i;
+#endif
 
+#ifdef HAVE_DEVLINK_H
 	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)
 		devlink_port_type_clear(mlx4_get_devlink_port(dev, i));
+#endif
 	ibdev->ib_active = false;
 	flush_workqueue(wq);
 
--- a/drivers/infiniband/hw/mlx4/main_exp.c
+++ b/drivers/infiniband/hw/mlx4/main_exp.c
@@ -190,6 +190,60 @@ int mlx4_ib_exp_ioctl(struct ib_ucontext
 	return ret;
 }
 
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE 
+unsigned long mlx4_ib_exp_get_unmapped_area(struct file *file,
+					    unsigned long addr,
+					    unsigned long len, unsigned long pgoff,
+					    unsigned long flags)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	unsigned long start_addr;
+	unsigned long page_size_order;
+	unsigned long  command;
+
+	mm = current->mm;
+	if (addr)
+		return current->mm->get_unmapped_area(file, addr, len,
+						pgoff, flags);
+
+	/* Last 8 bits hold the  command others are data per that command */
+	command = pgoff & MLX4_IB_EXP_MMAP_CMD_MASK;
+	if (command != MLX4_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES)
+		return current->mm->get_unmapped_area(file, addr, len,
+						pgoff, flags);
+	page_size_order = pgoff >> MLX4_IB_EXP_MMAP_CMD_BITS;
+	/* code is based on the huge-pages get_unmapped_area code */
+	start_addr = mm->free_area_cache;
+
+	if (len <= mm->cached_hole_size)
+		start_addr = TASK_UNMAPPED_BASE;
+
+
+full_search:
+	addr = ALIGN(start_addr, 1 << page_size_order);
+
+	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
+		/* At this point:  (!vma || addr < vma->vm_end). */
+		if (TASK_SIZE - len < addr) {
+			/*
+			 * Start a new search - just in case we missed
+			 * some holes.
+			 */
+			if (start_addr != TASK_UNMAPPED_BASE) {
+				start_addr = TASK_UNMAPPED_BASE;
+				goto full_search;
+			}
+			return -ENOMEM;
+		}
+
+		if (!vma || addr + len <= vma->vm_start)
+			return addr;
+		addr = ALIGN(vma->vm_end, 1 << page_size_order);
+	}
+}
+#endif
+
 int mlx4_ib_exp_uar_mmap(struct ib_ucontext *context, struct vm_area_struct *vma,
 			    unsigned long  command)
 {
--- a/drivers/infiniband/hw/mlx4/mlx4_ib_exp.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib_exp.h
@@ -133,4 +133,11 @@ int mlx4_ib_set_qp_user_uar(struct ib_pd
 			  int is_exp);
 struct ib_mr *mlx4_ib_phys_addr(struct ib_pd *pd, u64 length, u64 virt_addr,
 				int access_flags);
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE
+unsigned long mlx4_ib_exp_get_unmapped_area(struct file *file,
+					    unsigned long addr,
+					    unsigned long len, unsigned long pgoff,
+					    unsigned long flags);
+#endif
+
 #endif
--- a/drivers/infiniband/hw/mlx4/mr_exp.c
+++ b/drivers/infiniband/hw/mlx4/mr_exp.c
@@ -266,8 +266,13 @@ static ssize_t shared_mr_proc_write(stru
 
 static int shared_mr_mmap(struct file *filep, struct vm_area_struct *vma)
 {
+#ifdef HAVE_PDE_DATA
 	struct mlx4_shared_mr_info *smr_info =
 		(struct mlx4_shared_mr_info *)PDE_DATA(filep->f_path.dentry->d_inode);
+#else
+	struct proc_dir_entry *pde = PDE(filep->f_path.dentry->d_inode);
+	struct mlx4_shared_mr_info *smr_info = (struct mlx4_shared_mr_info *)pde->data;
+#endif
 
 	/* Prevent any mapping not on start of area */
 	if (vma->vm_pgoff != 0)
@@ -299,8 +304,10 @@ int prepare_shared_mr(struct mlx4_ib_mr
 	struct proc_dir_entry *mr_proc_entry;
 	mode_t mode = S_IFREG;
 	char name_buff[128];
+#ifdef HAVE_PROC_SET_USER
 	kuid_t uid;
 	kgid_t gid;
+#endif
 
 	/* start address and length must be aligned to page size in order
 	  * to map a full page and preventing leakage of data.
@@ -325,9 +332,14 @@ int prepare_shared_mr(struct mlx4_ib_mr
 		return -ENODEV;
 	}
 
+#ifdef HAVE_PROC_SET_USER
 	current_uid_gid(&uid, &gid);
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	/* now creating an extra entry having a uniqe suffix counter */
 	mr->smr_info->counter = atomic64_inc_return(&shared_mr_count);
@@ -343,8 +355,13 @@ int prepare_shared_mr(struct mlx4_ib_mr
 	}
 
 	mr->smr_info->counter_used = 1;
+#ifdef HAVE_PROC_SET_USER
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	return 0;
 }
--- a/drivers/net/ethernet/mellanox/mlx4/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx4/cmd.c
@@ -2116,8 +2116,10 @@ static int mlx4_master_activate_admin_st
 	int port, err;
 	struct mlx4_vport_state *vp_admin;
 	struct mlx4_vport_oper_state *vp_oper;
+#ifdef HAVE_ETH_P_8021AD
 	struct mlx4_slave_state *slave_state =
 		&priv->mfunc.master.slave_state[slave];
+#endif
 	struct mlx4_active_ports actv_ports = mlx4_get_active_ports(
 			&priv->dev, slave);
 	int min_port = find_first_bit(actv_ports.ports,
@@ -2159,13 +2161,16 @@ static int mlx4_master_activate_admin_st
 					return err;
 				}
 
+#ifdef HAVE_ETH_P_8021AD
 				if (vp_admin->vlan_proto != htons(ETH_P_8021AD) ||
 				    slave_state->vst_qinq_supported) {
+#endif
 					vp_oper->state.vlan_proto   = vp_admin->vlan_proto;
 					vp_oper->state.default_vlan = vp_admin->default_vlan;
 					vp_oper->state.default_qos  = vp_admin->default_qos;
+#ifdef HAVE_ETH_P_8021AD
 				}
-
+#endif
 
 				mlx4_dbg((&(priv->dev)), "alloc vlan %d idx  %d slave %d port %d\n",
 					 (int)(vp_oper->state.default_vlan),
@@ -3139,7 +3144,9 @@ int mlx4_set_vf_vlan(struct mlx4_dev *de
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_vport_state *vf_admin;
+#ifdef HAVE_ETH_P_8021AD
 	struct mlx4_slave_state *slave_state;
+#endif
 	struct mlx4_vport_oper_state *vf_oper;
 	int slave;
 
@@ -3150,6 +3157,7 @@ int mlx4_set_vf_vlan(struct mlx4_dev *de
 	if ((vlan > 4095) || (qos > 7))
 		return -EINVAL;
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	if (proto == htons(ETH_P_8021AD) &&
 	    !(dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_SVLAN_BY_QP))
 		return -EPROTONOSUPPORT;
@@ -3161,17 +3169,23 @@ int mlx4_set_vf_vlan(struct mlx4_dev *de
 	if ((proto == htons(ETH_P_8021AD)) &&
 	    ((vlan == 0) || (vlan == MLX4_VGT)))
 		return -EINVAL;
+#else
+	if (proto != htons(ETH_P_8021Q))
+		return -EPROTONOSUPPORT;
+#endif
 
 	slave = mlx4_get_slave_indx(dev, vf);
 	if (slave < 0)
 		return -EINVAL;
 
+#ifdef HAVE_ETH_P_8021AD
 	slave_state = &priv->mfunc.master.slave_state[slave];
 	if ((proto == htons(ETH_P_8021AD)) && (slave_state->active) &&
 	    (!slave_state->vst_qinq_supported)) {
 		mlx4_err(dev, "vf %d does not support VST QinQ mode\n", vf);
 		return -EPROTONOSUPPORT;
 	}
+#endif
 	port = mlx4_slaves_closest_port(dev, slave, port);
 	vf_admin = &priv->mfunc.master.vf_admin[slave].vport[port];
 	vf_oper = &priv->mfunc.master.vf_oper[slave].vport[port];
@@ -3203,9 +3217,13 @@ int mlx4_set_vf_vlan(struct mlx4_dev *de
 	/* Try to activate new vf state without restart,
 	 * this option is not supported while moving to VST QinQ mode.
 	 */
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	if ((proto == htons(ETH_P_8021AD) &&
 	     vf_oper->state.vlan_proto != proto) ||
 	    mlx4_master_immediate_activate_vlan_qos(priv, slave, port))
+#else
+	if (mlx4_master_immediate_activate_vlan_qos(priv, slave, port))
+#endif
 		mlx4_info(dev,
 			  "updating vf %d port %d config will take effect on next VF restart\n",
 			  vf, port);
@@ -3357,16 +3375,31 @@ int mlx4_get_vf_config(struct mlx4_dev *
 
 	ivf->vlan		= s_info->default_vlan;
 	ivf->qos		= s_info->default_qos;
+#ifdef HAVE_VF_VLAN_PROTO
 	ivf->vlan_proto		= s_info->vlan_proto;
+#endif
 
+#ifdef HAVE_TX_RATE_LIMIT
 	if (mlx4_is_vf_vst_and_prio_qos(dev, port, s_info))
 		ivf->max_tx_rate = s_info->tx_rate;
 	else
 		ivf->max_tx_rate = 0;
 
 	ivf->min_tx_rate	= 0;
+#else
+#ifdef HAVE_VF_TX_RATE
+	if (mlx4_is_vf_vst_and_prio_qos(dev, port, s_info))
+		ivf->tx_rate = s_info->tx_rate;
+	else
+		ivf->tx_rate = 0;
+#endif
+#endif
+#ifdef HAVE_VF_INFO_SPOOFCHK
 	ivf->spoofchk		= s_info->spoofchk;
+#endif
+#ifdef HAVE_LINKSTATE
 	ivf->linkstate		= s_info->link_state;
+#endif
 
 	return 0;
 }
@@ -3875,3 +3908,35 @@ ssize_t mlx4_get_vf_rate(struct mlx4_dev
 	return len;
 }
 EXPORT_SYMBOL_GPL(mlx4_get_vf_rate);
+
+#if (defined(HAVE_NETIF_F_HW_VLAN_STAG_RX) && !defined(HAVE_VF_VLAN_PROTO))
+ssize_t mlx4_get_vf_vlan_info(struct mlx4_dev *dev, int port, int vf, char *buf)
+{
+	int slave;
+	ssize_t len = 0;
+	struct mlx4_vport_state *s_info;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	if (!mlx4_is_master(dev) ||
+	    !(dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_SVLAN_BY_QP))
+		return -EOPNOTSUPP;
+
+	slave = mlx4_get_slave_indx(dev, vf);
+	if (slave < 0)
+		return -EINVAL;
+
+	s_info = &priv->mfunc.master.vf_admin[slave].vport[port];
+	if (s_info->default_vlan)
+		len += sprintf(&buf[len], "vlan %d", s_info->default_vlan);
+	if (s_info->default_qos)
+		len += sprintf(&buf[len], ", qos %d", s_info->default_qos);
+	if (s_info->vlan_proto == htons(ETH_P_8021AD))
+		len += sprintf(&buf[len], ", vlan protocol 802.1ad");
+	else if (s_info->default_vlan != MLX4_VGT)
+		len += sprintf(&buf[len], ", vlan protocol 802.1Q");
+	len += sprintf(&buf[len], "\n");
+
+      return len;
+}
+EXPORT_SYMBOL_GPL(mlx4_get_vf_vlan_info);
+#endif
--- a/drivers/net/ethernet/mellanox/mlx4/en_clock.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_clock.c
@@ -81,11 +81,13 @@ void mlx4_en_fill_hwtstamps(struct mlx4_
  **/
 void mlx4_en_remove_timestamp(struct mlx4_en_dev *mdev)
 {
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	if (mdev->ptp_clock) {
 		ptp_clock_unregister(mdev->ptp_clock);
 		mdev->ptp_clock = NULL;
 		mlx4_info(mdev, "removed PHC\n");
 	}
+#endif
 }
 
 void mlx4_en_ptp_overflow_check(struct mlx4_en_dev *mdev)
@@ -102,6 +104,7 @@ void mlx4_en_ptp_overflow_check(struct m
 	}
 }
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 /**
  * mlx4_en_phc_adjfreq - adjust the frequency of the hardware clock
  * @ptp: ptp clock structure
@@ -165,7 +168,11 @@ static int mlx4_en_phc_adjtime(struct pt
  * it into a struct timespec.
  **/
 static int mlx4_en_phc_gettime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			       struct timespec *ts)
+#else
 			       struct timespec64 *ts)
+#endif
 {
 	struct mlx4_en_dev *mdev = container_of(ptp, struct mlx4_en_dev,
 						ptp_clock_info);
@@ -176,7 +183,11 @@ static int mlx4_en_phc_gettime(struct pt
 	ns = timecounter_read(&mdev->clock);
 	write_unlock_irqrestore(&mdev->clock_lock, flags);
 
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	*ts = ns_to_timespec(ns);
+#else
 	*ts = ns_to_timespec64(ns);
+#endif
 
 	return 0;
 }
@@ -190,11 +201,19 @@ static int mlx4_en_phc_gettime(struct pt
  * wall timer value.
  **/
 static int mlx4_en_phc_settime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			       const struct timespec *ts)
+#else
 			       const struct timespec64 *ts)
+#endif
 {
 	struct mlx4_en_dev *mdev = container_of(ptp, struct mlx4_en_dev,
 						ptp_clock_info);
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	u64 ns = timespec_to_ns(ts);
+#else
 	u64 ns = timespec64_to_ns(ts);
+#endif
 	unsigned long flags;
 
 	/* reset the timecounter */
@@ -227,14 +246,23 @@ static const struct ptp_clock_info mlx4_
 	.n_alarm	= 0,
 	.n_ext_ts	= 0,
 	.n_per_out	= 0,
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	.n_pins		= 0,
+#endif
+
 	.pps		= 0,
 	.adjfreq	= mlx4_en_phc_adjfreq,
 	.adjtime	= mlx4_en_phc_adjtime,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	.gettime	= mlx4_en_phc_gettime,
+	.settime	= mlx4_en_phc_settime,
+#else
 	.gettime64	= mlx4_en_phc_gettime,
 	.settime64	= mlx4_en_phc_settime,
+#endif
 	.enable		= mlx4_en_phc_enable,
 };
+#endif
 
 #define MLX4_EN_WRAP_AROUND_SEC	10ULL
 
@@ -261,14 +289,20 @@ void mlx4_en_init_timestamp(struct mlx4_
 {
 	struct mlx4_dev *dev = mdev->dev;
 	unsigned long flags;
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
 	u64 ns, zero = 0;
+#else
+	u64 ns;
+#endif
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	/* mlx4_en_init_timestamp is called for each netdev.
 	 * mdev->ptp_clock is common for all ports, skip initialization if
 	 * was done for other port.
 	 */
 	if (mdev->ptp_clock)
 		return;
+#endif
 
 	rwlock_init(&mdev->clock_lock);
 
@@ -288,10 +322,15 @@ void mlx4_en_init_timestamp(struct mlx4_
 	/* Calculate period in seconds to call the overflow watchdog - to make
 	 * sure counter is checked at least once every wrap around.
 	 */
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
 	ns = cyclecounter_cyc2ns(&mdev->cycles, mdev->cycles.mask, zero, &zero);
+#else
+	ns = cyclecounter_cyc2ns(&mdev->cycles, mdev->cycles.mask);
+#endif
 	do_div(ns, NSEC_PER_SEC / 2 / HZ);
 	mdev->overflow_period = ns;
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	/* Configure the PHC */
 	mdev->ptp_clock_info = mlx4_en_ptp_clock_info;
 	snprintf(mdev->ptp_clock_info.name, 16, "mlx4 ptp");
@@ -304,5 +343,6 @@ void mlx4_en_init_timestamp(struct mlx4_
 	} else if (mdev->ptp_clock) {
 		mlx4_info(mdev, "registered PHC clock\n");
 	}
+#endif
 
 }
--- a/drivers/net/ethernet/mellanox/mlx4/en_cq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_cq.c
@@ -158,13 +158,16 @@ int mlx4_en_activate_cq(struct mlx4_en_p
 			err = 0;
 		}
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 		cq->irq_desc =
 			irq_to_desc(mlx4_eq_get_irq(mdev->dev,
 						    cq->vector));
+#endif
 	} else {
 		/* For TX we use the same irq per
 		ring we assigned for the RX    */
 		struct mlx4_en_cq *rx_cq;
+#ifdef HAVE_XDP_BUFF
 		int xdp_index;
 
 		/* The xdp tx irq must align with the rx ring that forwards to
@@ -174,6 +177,7 @@ int mlx4_en_activate_cq(struct mlx4_en_p
 		xdp_index = (priv->xdp_ring_num - priv->tx_ring_num) + cq_idx;
 		if (xdp_index >= 0)
 			cq_idx = xdp_index;
+#endif
 		cq_idx = cq_idx % priv->rx_ring_num;
 		rx_cq = priv->rx_cq[cq_idx];
 		cq->vector = rx_cq->vector;
@@ -196,10 +200,17 @@ int mlx4_en_activate_cq(struct mlx4_en_p
 	cq->mcq.event = mlx4_en_cq_event;
 
 	if (cq->is_tx)
+#ifdef HAVE_NETIF_TX_NAPI_ADD
 		netif_tx_napi_add(cq->dev, &cq->napi,
 				  vgtp_cq ? mlx4_en_vgtp_poll_tx_cq :
 				  mlx4_en_poll_tx_cq,
 				  NAPI_POLL_WEIGHT);
+#else
+		netif_napi_add(cq->dev, &cq->napi,
+			       vgtp_cq ? mlx4_en_vgtp_poll_tx_cq :
+			       mlx4_en_poll_tx_cq,
+			       NAPI_POLL_WEIGHT);
+#endif
 	else
 		netif_napi_add(cq->dev, &cq->napi, mlx4_en_poll_rx_cq, 64);
 
@@ -235,10 +246,12 @@ void mlx4_en_deactivate_cq(struct mlx4_e
 	struct mlx4_en_cq *cq = *pcq;
 
 	napi_disable(&cq->napi);
+#ifdef HAVE_NAPI_HASH_ADD
 	if (!cq->is_tx) {
 		napi_hash_del(&cq->napi);
 		synchronize_rcu();
 	}
+#endif
 	netif_napi_del(&cq->napi);
 
 	mlx4_cq_free(priv->mdev->dev, &cq->mcq);
--- a/drivers/net/ethernet/mellanox/mlx4/en_dcb_nl.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_dcb_nl.c
@@ -138,7 +138,11 @@ static void mlx4_en_dcbnl_set_pfc_cfg(st
 	priv->cee_config.pfc_state = true;
 }
 
+#ifdef NDO_GETNUMTCS_RETURNS_INT
 static int mlx4_en_dcbnl_getnumtcs(struct net_device *netdev, int tcid, u8 *num)
+#else
+static u8 mlx4_en_dcbnl_getnumtcs(struct net_device *netdev, int tcid, u8 *num)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 
@@ -248,7 +252,11 @@ static u8 mlx4_en_dcbnl_set_state(struct
  * otherwise returns 0 as the invalid user priority bitmap to
  * indicate an error.
  */
+#ifdef NDO_GETAPP_RETURNS_INT
 static int mlx4_en_dcbnl_getapp(struct net_device *netdev, u8 idtype, u16 id)
+#else
+static u8 mlx4_en_dcbnl_getapp(struct net_device *netdev, u8 idtype, u16 id)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 	struct dcb_app app = {
@@ -261,8 +269,13 @@ static int mlx4_en_dcbnl_getapp(struct n
 	return dcb_getapp(netdev, &app);
 }
 
+#ifdef NDO_SETAPP_RETURNS_INT
 static int mlx4_en_dcbnl_setapp(struct net_device *netdev, u8 idtype,
 				u16 id, u8 up)
+#else
+static u8 mlx4_en_dcbnl_setapp(struct net_device *netdev, u8 idtype,
+			       u16 id, u8 up)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 	struct dcb_app app;
@@ -562,7 +575,11 @@ err:
 }
 
 #define MLX4_RATELIMIT_UNITS_IN_KB 100000 /* rate-limit HW unit in Kbps */
+#ifndef CONFIG_SYSFS_MAXRATE
 static int mlx4_en_dcbnl_ieee_getmaxrate(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_getmaxrate(struct net_device *dev,
+#endif
 				   struct ieee_maxrate *maxrate)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -575,7 +592,11 @@ static int mlx4_en_dcbnl_ieee_getmaxrate
 	return 0;
 }
 
+#ifndef CONFIG_SYSFS_MAXRATE
 static int mlx4_en_dcbnl_ieee_setmaxrate(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_setmaxrate(struct net_device *dev,
+#endif
 		struct ieee_maxrate *maxrate)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -603,7 +624,11 @@ static int mlx4_en_dcbnl_ieee_setmaxrate
 #define RPG_ENABLE_BIT	31
 #define CN_TAG_BIT	30
 
+#ifndef CONFIG_SYSFS_QCN
 static int mlx4_en_dcbnl_ieee_getqcn(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_getqcn(struct net_device *dev,
+#endif
 				     struct ieee_qcn *qcn)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -666,7 +691,11 @@ static int mlx4_en_dcbnl_ieee_getqcn(str
 	return 0;
 }
 
+#ifndef CONFIG_SYSFS_QCN
 static int mlx4_en_dcbnl_ieee_setqcn(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_setqcn(struct net_device *dev,
+#endif
 				     struct ieee_qcn *qcn)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -729,7 +758,11 @@ static int mlx4_en_dcbnl_ieee_setqcn(str
 	return 0;
 }
 
+#ifndef CONFIG_SYSFS_QCN
 static int mlx4_en_dcbnl_ieee_getqcnstats(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_getqcnstats(struct net_device *dev,
+#endif
 					  struct ieee_qcn_stats *qcn_stats)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -775,11 +808,15 @@ static int mlx4_en_dcbnl_ieee_getqcnstat
 const struct dcbnl_rtnl_ops mlx4_en_dcbnl_ops = {
 	.ieee_getets		= mlx4_en_dcbnl_ieee_getets,
 	.ieee_setets		= mlx4_en_dcbnl_ieee_setets,
+#ifdef HAVE_IEEE_GET_SET_MAXRATE
 	.ieee_getmaxrate	= mlx4_en_dcbnl_ieee_getmaxrate,
 	.ieee_setmaxrate	= mlx4_en_dcbnl_ieee_setmaxrate,
+#endif
+#ifdef HAVE_IEEE_GETQCN
 	.ieee_getqcn		= mlx4_en_dcbnl_ieee_getqcn,
 	.ieee_setqcn		= mlx4_en_dcbnl_ieee_setqcn,
 	.ieee_getqcnstats	= mlx4_en_dcbnl_ieee_getqcnstats,
+#endif
 	.ieee_getpfc		= mlx4_en_dcbnl_ieee_getpfc,
 	.ieee_setpfc		= mlx4_en_dcbnl_ieee_setpfc,
 
@@ -815,7 +852,9 @@ const struct dcbnl_rtnl_ops mlx4_en_dcbn
 
 	.getdcbx	= mlx4_en_dcbnl_getdcbx,
 	.setdcbx	= mlx4_en_dcbnl_setdcbx,
+#ifdef HAVE_IEEE_GETQCN
 	.ieee_getqcn	= mlx4_en_dcbnl_ieee_getqcn,
 	.ieee_setqcn	= mlx4_en_dcbnl_ieee_setqcn,
 	.ieee_getqcnstats = mlx4_en_dcbnl_ieee_getqcnstats,
+#endif
 };
--- a/drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
@@ -47,6 +47,46 @@
 #define EN_ETHTOOL_SHORT_MASK cpu_to_be16(0xffff)
 #define EN_ETHTOOL_WORD_MASK  cpu_to_be32(0xffffffff)
 
+#ifndef HAVE_ETHTOOL_FLOW_UNION
+union mlx4_ethtool_flow_union {
+	struct ethtool_tcpip4_spec		tcp_ip4_spec;
+	struct ethtool_tcpip4_spec		udp_ip4_spec;
+	struct ethtool_tcpip4_spec		sctp_ip4_spec;
+	struct ethtool_ah_espip4_spec		ah_ip4_spec;
+	struct ethtool_ah_espip4_spec		esp_ip4_spec;
+	struct ethtool_usrip4_spec		usr_ip4_spec;
+	struct ethhdr				ether_spec;
+	__u8					hdata[52];
+};
+
+struct mlx4_ethtool_flow_ext {
+	__u8		padding[2];
+	unsigned char	h_dest[ETH_ALEN];
+	__be16		vlan_etype;
+	__be16		vlan_tci;
+	__be32		data[2];
+};
+
+struct mlx4_ethtool_rx_flow_spec {
+	__u32		flow_type;
+	union mlx4_ethtool_flow_union h_u;
+	struct mlx4_ethtool_flow_ext h_ext;
+	union mlx4_ethtool_flow_union m_u;
+	struct mlx4_ethtool_flow_ext m_ext;
+	__u64		ring_cookie;
+	__u32		location;
+};
+
+struct mlx4_ethtool_rxnfc {
+	__u32				cmd;
+	__u32				flow_type;
+	__u64				data;
+	struct mlx4_ethtool_rx_flow_spec	fs;
+	__u32				rule_cnt;
+	__u32				rule_locs[0];
+};
+#endif
+
 static int mlx4_en_change_inline_scatter_thold(struct net_device *dev,
 					       int thold)
 {
@@ -170,6 +210,119 @@ mlx4_en_get_drvinfo(struct net_device *d
 		sizeof(drvinfo->bus_info));
 }
 
+#ifdef LEGACY_ETHTOOL_OPS
+#ifdef HAVE_GET_SET_FLAGS
+int mlx4_en_set_flags(struct net_device *dev, u32 data)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+
+	if (DEV_FEATURE_CHANGED(dev, data, NETIF_F_HW_VLAN_CTAG_RX)) {
+		en_info(priv, "Turn %s RX vlan strip offload\n",
+			(data & NETIF_F_HW_VLAN_CTAG_RX) ? "ON" : "OFF");
+
+		if (data & NETIF_F_HW_VLAN_CTAG_RX)
+			priv->hwtstamp_config.flags |= NETIF_F_HW_VLAN_CTAG_RX;
+		else
+			priv->hwtstamp_config.flags &= ~NETIF_F_HW_VLAN_CTAG_RX;
+
+		mlx4_en_reset_config(dev, priv->hwtstamp_config, data);
+	}
+
+	if (DEV_FEATURE_CHANGED(dev, data, NETIF_F_HW_VLAN_CTAG_TX)) {
+		en_info(priv, "Turn %s TX vlan strip offload\n",
+				(data & NETIF_F_HW_VLAN_CTAG_TX) ? "ON" : "OFF");
+
+		if (data & NETIF_F_HW_VLAN_CTAG_TX)
+			dev->features |= NETIF_F_HW_VLAN_CTAG_TX;
+		else
+			dev->features &= ~NETIF_F_HW_VLAN_CTAG_TX;
+	}
+
+	if (data & ETH_FLAG_LRO)
+		dev->features |= NETIF_F_LRO;
+	else
+		dev->features &= ~NETIF_F_LRO;
+
+	return 0;
+}
+
+u32 mlx4_en_get_flags(struct net_device *dev)
+{
+	return ethtool_op_get_flags(dev) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_RX) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_TX);
+}
+#endif
+
+#ifdef HAVE_GET_SET_TSO
+static u32 mlx4_en_get_tso(struct net_device *dev)
+{
+       return (dev->features & NETIF_F_TSO) != 0;
+}
+
+static int mlx4_en_set_tso(struct net_device *dev, u32 data)
+{
+       struct mlx4_en_priv *priv = netdev_priv(dev);
+
+       if (data) {
+               if (!priv->mdev->LSO_support)
+                       return -EPERM;
+               dev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+#ifndef HAVE_VLAN_GRO_RECEIVE
+               dev->vlan_features |= (NETIF_F_TSO | NETIF_F_TSO6);
+#else
+               if (priv->vlgrp) {
+                       int i;
+                       struct net_device *vdev;
+                       for (i = 0; i < VLAN_N_VID; i++) {
+                               vdev = vlan_group_get_device(priv->vlgrp, i);
+                               if (vdev) {
+                                       vdev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+                                       vlan_group_set_device(priv->vlgrp, i, vdev);
+                               }
+                       }
+               }
+#endif
+       } else {
+               dev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+#ifndef HAVE_VLAN_GRO_RECEIVE
+               dev->vlan_features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+#else
+               if (priv->vlgrp) {
+                       int i;
+                       struct net_device *vdev;
+                       for (i = 0; i < VLAN_N_VID; i++) {
+                               vdev = vlan_group_get_device(priv->vlgrp, i);
+                               if (vdev) {
+                                       vdev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+                                       vlan_group_set_device(priv->vlgrp, i, vdev);
+                               }
+                       }
+               }
+#endif
+       }
+       return 0;
+}
+#endif
+
+#ifdef HAVE_GET_SET_RX_CSUM
+static u32 mlx4_en_get_rx_csum(struct net_device *dev)
+{
+       return dev->features & NETIF_F_RXCSUM;
+}
+
+static int mlx4_en_set_rx_csum(struct net_device *dev, u32 data)
+{
+       if (!data) {
+               dev->features &= ~NETIF_F_RXCSUM;
+               return 0;
+       }
+       dev->features |= NETIF_F_RXCSUM;
+       return 0;
+}
+#endif
+#endif
+
 static const char mlx4_en_priv_flags[][ETH_GSTRING_LEN] = {
 	"blueflame",
 	"phv-bit",
@@ -182,6 +335,15 @@ static const char mlx4_en_priv_flags[][E
 	"mlx4_flow_steering_tcp",
 	"mlx4_flow_steering_udp",
 	"disable_mc_loopback",
+#ifndef HAVE_NETIF_F_RXFCS
+	"rx-fcs",
+#endif
+#ifndef HAVE_NETIF_F_RXALL
+	"rx-all",
+#endif
+#ifndef HAVE_ETH_SS_RSS_HASH_FUNCS
+	"mlx4_rss_xor_hash_function",
+#endif
 };
 
 static const char main_strings[][ETH_GSTRING_LEN] = {
@@ -194,6 +356,9 @@ static const char main_strings[][ETH_GST
 	"tx_heartbeat_errors", "tx_window_errors",
 
 	/* port statistics */
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	"rx_lro_aggregated", "rx_lro_flushed", "rx_lro_no_desc",
+#endif
 	"tso_packets",
 	"xmit_more",
 	"queue_stopped", "wake_queue", "tx_timeout", "rx_alloc_failed",
@@ -454,7 +619,11 @@ static int mlx4_en_get_sset_count(struct
 	case ETH_SS_STATS:
 		return bitmap_iterator_count(&it) +
 			(priv->tx_ring_num * 2) +
+#ifdef MLX4_EN_BUSY_POLL
+			(priv->rx_ring_num * 6) +
+#else
 			(priv->rx_ring_num * 3) +
+#endif
 			(vgtp_count * 2 + vgtp_on * 1);
 	case ETH_SS_TEST:
 		return MLX4_EN_NUM_SELF_TEST - !(priv->mdev->dev->caps.flags
@@ -466,6 +635,26 @@ static int mlx4_en_get_sset_count(struct
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+static void mlx4_en_update_lro_stats(struct mlx4_en_priv *priv)
+{
+	int i;
+
+	priv->port_stats.lro_aggregated = 0;
+	priv->port_stats.lro_flushed = 0;
+	priv->port_stats.lro_no_desc = 0;
+
+	for (i = 0; i < priv->rx_ring_num; i++) {
+		struct net_lro_stats *lro_stats =
+			&priv->rx_ring[i]->lro.lro_mgr.stats;
+
+		priv->port_stats.lro_aggregated += lro_stats->aggregated;
+		priv->port_stats.lro_flushed += lro_stats->flushed;
+		priv->port_stats.lro_no_desc += lro_stats->no_desc;
+	}
+}
+#endif
+
 static void mlx4_en_get_ethtool_stats(struct net_device *dev,
 		struct ethtool_stats *stats, uint64_t *data)
 {
@@ -478,6 +667,10 @@ static void mlx4_en_get_ethtool_stats(st
 
 	spin_lock_bh(&priv->stats_lock);
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	mlx4_en_update_lro_stats(priv);
+#endif
+
 	for (i = 0; i < NUM_MAIN_STATS; i++, bitmap_iterator_inc(&it))
 		if (bitmap_iterator_test(&it))
 			data[index++] = ((unsigned long *)&dev->stats)[i];
@@ -546,6 +739,11 @@ static void mlx4_en_get_ethtool_stats(st
 		data[index++] = priv->rx_ring[i]->packets;
 		data[index++] = priv->rx_ring[i]->bytes;
 		data[index++] = priv->rx_ring[i]->dropped;
+#ifdef MLX4_EN_BUSY_POLL
+		data[index++] = priv->rx_ring[i]->yields;
+		data[index++] = priv->rx_ring[i]->misses;
+		data[index++] = priv->rx_ring[i]->cleaned;
+#endif
 	}
 	spin_unlock_bh(&priv->stats_lock);
 
@@ -651,6 +849,14 @@ static void mlx4_en_get_strings(struct n
 				"rx%d_bytes", i);
 			sprintf(data + (index++) * ETH_GSTRING_LEN,
 				"rx%d_dropped", i);
+#ifdef MLX4_EN_BUSY_POLL
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_napi_yield", i);
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_misses", i);
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_cleaned", i);
+#endif
 		}
 		break;
 	case ETH_SS_PRIV_FLAGS:
@@ -675,6 +881,7 @@ static u32 mlx4_en_autoneg_get(struct ne
 	return autoneg;
 }
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static void ptys2ethtool_update_supported_port(unsigned long *mask,
 					       struct mlx4_ptys_reg *ptys_reg)
 {
@@ -700,6 +907,37 @@ static void ptys2ethtool_update_supporte
 		__set_bit(ETHTOOL_LINK_MODE_Backplane_BIT, mask);
 	}
 }
+#endif
+
+static u32 ptys_get_supported_port(struct mlx4_ptys_reg *ptys_reg)
+{
+	u32 eth_proto = be32_to_cpu(ptys_reg->eth_proto_cap);
+
+	if (eth_proto & (MLX4_PROT_MASK(MLX4_10GBASE_T)
+			 | MLX4_PROT_MASK(MLX4_1000BASE_T)
+			 | MLX4_PROT_MASK(MLX4_100BASE_TX))) {
+			return SUPPORTED_TP;
+	}
+
+	if (eth_proto & (MLX4_PROT_MASK(MLX4_10GBASE_CR)
+			 | MLX4_PROT_MASK(MLX4_10GBASE_SR)
+			 | MLX4_PROT_MASK(MLX4_56GBASE_SR4)
+			 | MLX4_PROT_MASK(MLX4_40GBASE_CR4)
+			 | MLX4_PROT_MASK(MLX4_40GBASE_SR4)
+			 | MLX4_PROT_MASK(MLX4_1000BASE_CX_SGMII))) {
+			return SUPPORTED_FIBRE;
+	}
+
+	if (eth_proto & (MLX4_PROT_MASK(MLX4_56GBASE_KR4)
+			 | MLX4_PROT_MASK(MLX4_40GBASE_KR4)
+			 | MLX4_PROT_MASK(MLX4_20GBASE_KR2)
+			 | MLX4_PROT_MASK(MLX4_10GBASE_KR)
+			 | MLX4_PROT_MASK(MLX4_10GBASE_KX4)
+			 | MLX4_PROT_MASK(MLX4_1000BASE_KX))) {
+			return SUPPORTED_Backplane;
+	}
+	return 0;
+}
 
 static u32 ptys_get_active_port(struct mlx4_ptys_reg *ptys_reg)
 {
@@ -744,8 +982,12 @@ static u32 ptys_get_active_port(struct m
 enum ethtool_report {
 	SUPPORTED = 0,
 	ADVERTISED = 1,
+#ifndef HAVE_ETHTOOL_xLINKSETTINGS
+	SPEED = 2,
+#endif
 };
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 struct ptys2ethtool_config {
 	__ETHTOOL_DECLARE_LINK_MODE_MASK(supported);
 	__ETHTOOL_DECLARE_LINK_MODE_MASK(advertised);
@@ -780,8 +1022,10 @@ static unsigned long *ptys2ethtool_link_
 			__set_bit(modes[i], cfg->advertised);		\
 		}							\
 	})
+#endif
 
 /* Translates mlx4 link mode to equivalent ethtool Link modes/speed */
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static struct ptys2ethtool_config ptys2ethtool_map[MLX4_LINK_MODES_SZ];
 
 void __init mlx4_en_init_ptys2ethtool_map(void)
@@ -822,7 +1066,102 @@ void __init mlx4_en_init_ptys2ethtool_ma
 	MLX4_BUILD_PTYS2ETHTOOL_CONFIG(MLX4_56GBASE_SR4, SPEED_56000,
 				       ETHTOOL_LINK_MODE_56000baseSR4_Full_BIT);
 };
+#endif
+
+static u32 deprecated_ptys2ethtool_map[MLX4_LINK_MODES_SZ][3] = {
+	[MLX4_100BASE_TX] = {
+		SUPPORTED_100baseT_Full,
+		ADVERTISED_100baseT_Full,
+		SPEED_100
+		},
+
+	[MLX4_1000BASE_T] = {
+		SUPPORTED_1000baseT_Full,
+		ADVERTISED_1000baseT_Full,
+		SPEED_1000
+		},
+	[MLX4_1000BASE_CX_SGMII] = {
+		SUPPORTED_1000baseKX_Full,
+		ADVERTISED_1000baseKX_Full,
+		SPEED_1000
+		},
+	[MLX4_1000BASE_KX] = {
+		SUPPORTED_1000baseKX_Full,
+		ADVERTISED_1000baseKX_Full,
+		SPEED_1000
+		},
+
+	[MLX4_10GBASE_T] = {
+		SUPPORTED_10000baseT_Full,
+		ADVERTISED_10000baseT_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_CX4] = {
+		SUPPORTED_10000baseKX4_Full,
+		ADVERTISED_10000baseKX4_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_KX4] = {
+		SUPPORTED_10000baseKX4_Full,
+		ADVERTISED_10000baseKX4_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_KR] = {
+		SUPPORTED_10000baseKR_Full,
+		ADVERTISED_10000baseKR_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_CR] = {
+		SUPPORTED_10000baseKR_Full,
+		ADVERTISED_10000baseKR_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_SR] = {
+		SUPPORTED_10000baseKR_Full,
+		ADVERTISED_10000baseKR_Full,
+		SPEED_10000
+		},
+
+	[MLX4_20GBASE_KR2] = {
+		SUPPORTED_20000baseMLD2_Full | SUPPORTED_20000baseKR2_Full,
+		ADVERTISED_20000baseMLD2_Full | ADVERTISED_20000baseKR2_Full,
+		SPEED_20000
+		},
+
+	[MLX4_40GBASE_CR4] = {
+		SUPPORTED_40000baseCR4_Full,
+		ADVERTISED_40000baseCR4_Full,
+		SPEED_40000
+		},
+	[MLX4_40GBASE_KR4] = {
+		SUPPORTED_40000baseKR4_Full,
+		ADVERTISED_40000baseKR4_Full,
+		SPEED_40000
+		},
+	[MLX4_40GBASE_SR4] = {
+		SUPPORTED_40000baseSR4_Full,
+		ADVERTISED_40000baseSR4_Full,
+		SPEED_40000
+		},
+
+	[MLX4_56GBASE_KR4] = {
+		SUPPORTED_56000baseKR4_Full,
+		ADVERTISED_56000baseKR4_Full,
+		SPEED_56000
+		},
+	[MLX4_56GBASE_CR4] = {
+		SUPPORTED_56000baseCR4_Full,
+		ADVERTISED_56000baseCR4_Full,
+		SPEED_56000
+		},
+	[MLX4_56GBASE_SR4] = {
+		SUPPORTED_56000baseSR4_Full,
+		ADVERTISED_56000baseSR4_Full,
+		SPEED_56000
+		},
+};
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static void ptys2ethtool_update_link_modes(unsigned long *link_modes,
 					   u32 eth_proto,
 					   enum ethtool_report report)
@@ -836,7 +1175,21 @@ static void ptys2ethtool_update_link_mod
 				  __ETHTOOL_LINK_MODE_MASK_NBITS);
 	}
 }
+#endif
+
+static u32 ptys2ethtool_link_modes(u32 eth_proto, enum ethtool_report report)
+{
+	int i;
+	u32 link_modes = 0;
+
+	for (i = 0; i < MLX4_LINK_MODES_SZ; i++) {
+		if (eth_proto & MLX4_PROT_MASK(i))
+			link_modes |= deprecated_ptys2ethtool_map[i][report];
+	}
+	return link_modes;
+}
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static u32 ethtool2ptys_link_modes(const unsigned long *link_modes,
 				   enum ethtool_report report)
 {
@@ -853,6 +1206,19 @@ static u32 ethtool2ptys_link_modes(const
 	}
 	return ptys_modes;
 }
+#endif
+
+static u32 deprecated_ethtool2ptys_link_modes(u32 link_modes, enum ethtool_report report)
+{
+	int i;
+	u32 ptys_modes = 0;
+
+	for (i = 0; i < MLX4_LINK_MODES_SZ; i++) {
+		if (deprecated_ptys2ethtool_map[i][report] & link_modes)
+			ptys_modes |= 1 << i;
+	}
+	return ptys_modes;
+}
 
 /* Convert actual speed (SPEED_XXX) to ptys link modes */
 static u32 speed2ptys_link_modes(u32 speed)
@@ -861,12 +1227,17 @@ static u32 speed2ptys_link_modes(u32 spe
 	u32 ptys_modes = 0;
 
 	for (i = 0; i < MLX4_LINK_MODES_SZ; i++) {
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 		if (ptys2ethtool_map[i].speed == speed)
+#else
+		if (deprecated_ptys2ethtool_map[i][SPEED] == speed)
+#endif
 			ptys_modes |= 1 << i;
 	}
 	return ptys_modes;
 }
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static int
 ethtool_get_ptys_link_ksettings(struct net_device *dev,
 				struct ethtool_link_ksettings *link_ksettings)
@@ -954,7 +1325,85 @@ ethtool_get_ptys_link_ksettings(struct n
 
 	return ret;
 }
+#endif
+
+static int ethtool_get_ptys_settings(struct net_device *dev,
+				     struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	struct mlx4_ptys_reg ptys_reg;
+	u32 eth_proto;
+	int ret;
+
+	memset(&ptys_reg, 0, sizeof(ptys_reg));
+	ptys_reg.local_port = priv->port;
+	ptys_reg.proto_mask = MLX4_PTYS_EN;
+	ret = mlx4_ACCESS_PTYS_REG(priv->mdev->dev,
+				   MLX4_ACCESS_REG_QUERY, &ptys_reg);
+	if (ret) {
+		en_warn(priv, "Failed to run mlx4_ACCESS_PTYS_REG status(%x)",
+			ret);
+		return ret;
+	}
+	en_dbg(DRV, priv, "ptys_reg.proto_mask       %x\n",
+	       ptys_reg.proto_mask);
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_cap    %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_cap));
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_admin  %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_admin));
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_oper   %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_oper));
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_lp_adv %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_lp_adv));
+
+	cmd->supported = 0;
+	cmd->advertising = 0;
+
+	cmd->supported |= ptys_get_supported_port(&ptys_reg);
+
+	eth_proto = be32_to_cpu(ptys_reg.eth_proto_cap);
+	cmd->supported |= ptys2ethtool_link_modes(eth_proto, SUPPORTED);
+
+	eth_proto = be32_to_cpu(ptys_reg.eth_proto_admin);
+	cmd->advertising |= ptys2ethtool_link_modes(eth_proto, ADVERTISED);
+
+	cmd->supported |= SUPPORTED_Pause | SUPPORTED_Asym_Pause;
+	cmd->advertising |= (priv->prof->tx_pause) ? ADVERTISED_Pause : 0;
+
+	cmd->advertising |= (priv->prof->tx_pause ^ priv->prof->rx_pause) ?
+		ADVERTISED_Asym_Pause : 0;
+
+	cmd->port = ptys_get_active_port(&ptys_reg);
+	cmd->transceiver = (SUPPORTED_TP & cmd->supported) ?
+		XCVR_EXTERNAL : XCVR_INTERNAL;
+
+	if (mlx4_en_autoneg_get(dev)) {
+		cmd->supported |= SUPPORTED_Autoneg;
+		cmd->advertising |= ADVERTISED_Autoneg;
+	}
+
+	cmd->autoneg = (priv->port_state.flags & MLX4_EN_PORT_ANC) ?
+		AUTONEG_ENABLE : AUTONEG_DISABLE;
+
+	eth_proto = be32_to_cpu(ptys_reg.eth_proto_lp_adv);
+	cmd->lp_advertising = ptys2ethtool_link_modes(eth_proto, ADVERTISED);
+
+	cmd->lp_advertising |= (priv->port_state.flags & MLX4_EN_PORT_ANC) ?
+			ADVERTISED_Autoneg : 0;
+
+	cmd->phy_address = 0;
+	cmd->mdio_support = 0;
+	cmd->maxtxpkt = 0;
+	cmd->maxrxpkt = 0;
+	cmd->eth_tp_mdix = ETH_TP_MDI_INVALID;
+#if defined(ETH_TP_MDI_AUTO)
+	cmd->eth_tp_mdix_ctrl = ETH_TP_MDI_AUTO;
+#endif
+
+	return ret;
+}
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static void
 ethtool_get_default_link_ksettings(
 	struct net_device *dev, struct ethtool_link_ksettings *link_ksettings)
@@ -989,7 +1438,36 @@ ethtool_get_default_link_ksettings(
 		link_ksettings->base.port = -1;
 	}
 }
+#endif
 
+static void ethtool_get_default_settings(struct net_device *dev,
+					 struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	int trans_type;
+
+	cmd->autoneg = AUTONEG_DISABLE;
+	cmd->supported = SUPPORTED_10000baseT_Full;
+	cmd->advertising = ADVERTISED_10000baseT_Full;
+	trans_type = priv->port_state.transceiver;
+
+	if (trans_type > 0 && trans_type <= 0xC) {
+		cmd->port = PORT_FIBRE;
+		cmd->transceiver = XCVR_EXTERNAL;
+		cmd->supported |= SUPPORTED_FIBRE;
+		cmd->advertising |= ADVERTISED_FIBRE;
+	} else if (trans_type == 0x80 || trans_type == 0) {
+		cmd->port = PORT_TP;
+		cmd->transceiver = XCVR_INTERNAL;
+		cmd->supported |= SUPPORTED_TP;
+		cmd->advertising |= ADVERTISED_TP;
+	} else  {
+		cmd->port = -1;
+		cmd->transceiver = -1;
+	}
+}
+
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static int
 mlx4_en_get_link_ksettings(struct net_device *dev,
 			   struct ethtool_link_ksettings *link_ksettings)
@@ -1018,6 +1496,34 @@ mlx4_en_get_link_ksettings(struct net_de
 	}
 	return 0;
 }
+#endif
+
+static int mlx4_en_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	int ret = -EINVAL;
+
+	if (mlx4_en_QUERY_PORT(priv->mdev, priv->port))
+		return -ENOMEM;
+
+	en_dbg(DRV, priv, "query port state.flags ANC(%x) ANE(%x)\n",
+	       priv->port_state.flags & MLX4_EN_PORT_ANC,
+	       priv->port_state.flags & MLX4_EN_PORT_ANE);
+
+	if (priv->mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ETH_PROT_CTRL)
+		ret = ethtool_get_ptys_settings(dev, cmd);
+	if (ret) /* ETH PROT CRTL is not supported or PTYS CMD failed */
+		ethtool_get_default_settings(dev, cmd);
+
+	if (netif_carrier_ok(dev)) {
+		ethtool_cmd_speed_set(cmd, priv->port_state.link_speed);
+		cmd->duplex = DUPLEX_FULL;
+	} else {
+		ethtool_cmd_speed_set(cmd, SPEED_UNKNOWN);
+		cmd->duplex = DUPLEX_UNKNOWN;
+	}
+	return 0;
+}
 
 /* Calculate PTYS admin according ethtool speed (SPEED_XXX) */
 static __be32 speed_set_ptys_admin(struct mlx4_en_priv *priv, u32 speed,
@@ -1038,6 +1544,7 @@ static __be32 speed_set_ptys_admin(struc
 	return proto_admin;
 }
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static int
 mlx4_en_set_link_ksettings(struct net_device *dev,
 			   const struct ethtool_link_ksettings *link_ksettings)
@@ -1110,6 +1617,72 @@ mlx4_en_set_link_ksettings(struct net_de
 	mutex_unlock(&priv->mdev->state_lock);
 	return 0;
 }
+#endif
+
+static int mlx4_en_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	struct mlx4_ptys_reg ptys_reg;
+	__be32 proto_admin;
+	int ret;
+
+	u32 ptys_adv = deprecated_ethtool2ptys_link_modes(cmd->advertising, ADVERTISED);
+	int speed = ethtool_cmd_speed(cmd);
+
+       en_dbg(DRV, priv, "Set Speed=%d adv=0x%x autoneg=%d duplex=%d\n",
+              speed, cmd->advertising, cmd->autoneg, cmd->duplex);
+
+       if (!(priv->mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ETH_PROT_CTRL) ||
+           (cmd->duplex == DUPLEX_HALF))
+               return -EINVAL;
+
+	memset(&ptys_reg, 0, sizeof(ptys_reg));
+	ptys_reg.local_port = priv->port;
+	ptys_reg.proto_mask = MLX4_PTYS_EN;
+	ret = mlx4_ACCESS_PTYS_REG(priv->mdev->dev,
+				   MLX4_ACCESS_REG_QUERY, &ptys_reg);
+	if (ret) {
+		en_warn(priv, "Failed to QUERY mlx4_ACCESS_PTYS_REG status(%x)\n",
+			ret);
+		return 0;
+	}
+
+	proto_admin = cmd->autoneg == AUTONEG_ENABLE ?
+		cpu_to_be32(ptys_adv) :
+		speed_set_ptys_admin(priv, speed,
+				     ptys_reg.eth_proto_cap);
+
+	proto_admin &= ptys_reg.eth_proto_cap;
+	if (!proto_admin) {
+		en_warn(priv, "Not supported link mode(s) requested, check supported link modes.\n");
+		return -EINVAL; /* nothing to change due to bad input */
+	}
+
+	if (proto_admin == ptys_reg.eth_proto_admin)
+		return 0; /* Nothing to change */
+
+	en_dbg(DRV, priv, "mlx4_ACCESS_PTYS_REG SET: ptys_reg.eth_proto_admin = 0x%x\n",
+	       be32_to_cpu(proto_admin));
+
+	ptys_reg.eth_proto_admin = proto_admin;
+	ret = mlx4_ACCESS_PTYS_REG(priv->mdev->dev, MLX4_ACCESS_REG_WRITE,
+				   &ptys_reg);
+	if (ret) {
+		en_warn(priv, "Failed to write mlx4_ACCESS_PTYS_REG eth_proto_admin(0x%x) status(0x%x)",
+			be32_to_cpu(ptys_reg.eth_proto_admin), ret);
+		return ret;
+	}
+
+	mutex_lock(&priv->mdev->state_lock);
+	if (priv->port_up) {
+		en_warn(priv, "Port link mode changed, restarting port...\n");
+		mlx4_en_stop_port(dev, 1);
+		if (mlx4_en_start_port(dev))
+			en_err(priv, "Failed restarting port %d\n", priv->port);
+	}
+	mutex_unlock(&priv->mdev->state_lock);
+	return 0;
+}
 
 static int mlx4_en_get_coalesce(struct net_device *dev,
 			      struct ethtool_coalesce *coal)
@@ -1279,18 +1852,27 @@ static void mlx4_en_get_ringparam(struct
 	param->tx_pending = priv->tx_ring[0]->size;
 }
 
+#if defined(HAVE_RXFH_INDIR_SIZE) || defined(HAVE_RXFH_INDIR_SIZE_EXT)
+#if defined(HAVE_RXFH_INDIR_SIZE) && !defined(HAVE_RXFH_INDIR_SIZE_EXT)
+u32 mlx4_en_get_rxfh_indir_size(struct net_device *dev)
+#elif defined(HAVE_RXFH_INDIR_SIZE_EXT) && !defined(HAVE_RXFH_INDIR_SIZE)
 static u32 mlx4_en_get_rxfh_indir_size(struct net_device *dev)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 
 	return rounddown_pow_of_two(priv->rx_ring_num);
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static u32 mlx4_en_get_rxfh_key_size(struct net_device *netdev)
 {
 	return MLX4_EN_RSS_KEY_SIZE;
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 static int mlx4_en_check_rxfh_func(struct net_device *dev, u8 hfunc)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1312,9 +1894,19 @@ static int mlx4_en_check_rxfh_func(struc
 
 	return -EINVAL;
 }
-
+#endif
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 static int mlx4_en_get_rxfh(struct net_device *dev, u32 *ring_index, u8 *key,
 			    u8 *hfunc)
+#else
+static int mlx4_en_get_rxfh(struct net_device *dev, u32 *ring_index, u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx4_en_get_rxfh_indir(struct net_device *dev, u32 *ring_index)
+#elif defined(CONFIG_SYSFS_INDIR_SETTING)
+int mlx4_en_get_rxfh_indir(struct net_device *dev, u32 *ring_index)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u32 n = mlx4_en_get_rxfh_indir_size(dev);
@@ -1329,15 +1921,30 @@ static int mlx4_en_get_rxfh(struct net_d
 			break;
 		ring_index[i] = i % rss_rings;
 	}
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key)
 		memcpy(key, priv->rss_key, MLX4_EN_RSS_KEY_SIZE);
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (hfunc)
 		*hfunc = priv->rss_hash_fn;
+#endif
+#endif
 	return err;
 }
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static int mlx4_en_set_rxfh(struct net_device *dev, const u32 *ring_index,
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 			    const u8 *key, const u8 hfunc)
+#else
+			    const u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx4_en_set_rxfh_indir(struct net_device *dev, const u32 *ring_index)
+#elif defined(CONFIG_SYSFS_INDIR_SETTING)
+int mlx4_en_set_rxfh_indir(struct net_device *dev, const u32 *ring_index)
+#endif
+
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u32 n = mlx4_en_get_rxfh_indir_size(dev);
@@ -1367,11 +1974,13 @@ static int mlx4_en_set_rxfh(struct net_d
 	if (!is_power_of_2(rss_rings))
 		return -EINVAL;
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	if (hfunc != ETH_RSS_HASH_NO_CHANGE) {
 		err = mlx4_en_check_rxfh_func(dev, hfunc);
 		if (err)
 			return err;
 	}
+#endif
 
 	mutex_lock(&mdev->state_lock);
 	if (priv->port_up) {
@@ -1381,10 +1990,14 @@ static int mlx4_en_set_rxfh(struct net_d
 
 	if (ring_index)
 		priv->prof->rss_rings = rss_rings;
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key)
 		memcpy(priv->rss_key, key, MLX4_EN_RSS_KEY_SIZE);
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (hfunc !=  ETH_RSS_HASH_NO_CHANGE)
 		priv->rss_hash_fn = hfunc;
+#endif
+#endif
 
 	if (port_up) {
 		err = mlx4_en_start_port(dev);
@@ -1400,7 +2013,11 @@ static int mlx4_en_set_rxfh(struct net_d
 	((field) == 0 || (field) == (__force typeof(field))-1)
 
 static int mlx4_en_validate_flow(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 				 struct ethtool_rxnfc *cmd)
+#else
+				 struct mlx4_ethtool_rxnfc *cmd)
+#endif
 {
 	struct ethtool_usrip4_spec *l3_mask;
 	struct ethtool_tcpip4_spec *l4_mask;
@@ -1409,11 +2026,13 @@ static int mlx4_en_validate_flow(struct
 	if (cmd->fs.location >= MAX_NUM_OF_FS_RULES)
 		return -EINVAL;
 
+#ifdef HAVE_ETHTOOL_FLOW_EXT_H_DEST
 	if (cmd->fs.flow_type & FLOW_MAC_EXT) {
 		/* dest mac mask must be ff:ff:ff:ff:ff:ff */
 		if (!is_broadcast_ether_addr(cmd->fs.m_ext.h_dest))
 			return -EINVAL;
 	}
+#endif
 
 	switch (cmd->fs.flow_type & ~(FLOW_EXT | FLOW_MAC_EXT)) {
 	case TCP_V4_FLOW:
@@ -1454,6 +2073,7 @@ static int mlx4_en_validate_flow(struct
 		return -EINVAL;
 	}
 
+#ifdef HAVE_ETHTOOL_FLOW_EXT
 	if ((cmd->fs.flow_type & FLOW_EXT)) {
 		if (cmd->fs.m_ext.vlan_etype ||
 		    !((cmd->fs.m_ext.vlan_tci & cpu_to_be16(VLAN_VID_MASK)) ==
@@ -1468,11 +2088,16 @@ static int mlx4_en_validate_flow(struct
 
 		}
 	}
+#endif
 
 	return 0;
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_ethtool_add_mac_rule(struct ethtool_rxnfc *cmd,
+#else
+static int mlx4_en_ethtool_add_mac_rule(struct mlx4_ethtool_rxnfc *cmd,
+#endif
 					struct list_head *rule_list_h,
 					struct mlx4_spec_list *spec_l2,
 					unsigned char *mac)
@@ -1484,11 +2109,13 @@ static int mlx4_en_ethtool_add_mac_rule(
 	memcpy(spec_l2->eth.dst_mac_msk, &mac_msk, ETH_ALEN);
 	memcpy(spec_l2->eth.dst_mac, mac, ETH_ALEN);
 
+#ifdef HAVE_ETHTOOL_FLOW_EXT
 	if ((cmd->fs.flow_type & FLOW_EXT) &&
 	    (cmd->fs.m_ext.vlan_tci & cpu_to_be16(VLAN_VID_MASK))) {
 		spec_l2->eth.vlan_id = cmd->fs.h_ext.vlan_tci;
 		spec_l2->eth.vlan_id_msk = cpu_to_be16(VLAN_VID_MASK);
 	}
+#endif
 
 	list_add_tail(&spec_l2->list, rule_list_h);
 
@@ -1496,7 +2123,11 @@ static int mlx4_en_ethtool_add_mac_rule(
 }
 
 static int mlx4_en_ethtool_add_mac_rule_by_ipv4(struct mlx4_en_priv *priv,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 						struct ethtool_rxnfc *cmd,
+#else
+						struct mlx4_ethtool_rxnfc *cmd,
+#endif
 						struct list_head *rule_list_h,
 						struct mlx4_spec_list *spec_l2,
 						__be32 ipv4_dst)
@@ -1505,9 +2136,11 @@ static int mlx4_en_ethtool_add_mac_rule_
 	unsigned char mac[ETH_ALEN];
 
 	if (!ipv4_is_multicast(ipv4_dst)) {
+#ifdef HAVE_ETHTOOL_FLOW_EXT_H_DEST
 		if (cmd->fs.flow_type & FLOW_MAC_EXT)
 			memcpy(&mac, cmd->fs.h_ext.h_dest, ETH_ALEN);
 		else
+#endif
 			memcpy(&mac, priv->dev->dev_addr, ETH_ALEN);
 	} else {
 		ip_eth_mc_map(ipv4_dst, mac);
@@ -1520,7 +2153,11 @@ static int mlx4_en_ethtool_add_mac_rule_
 }
 
 static int add_ip_rule(struct mlx4_en_priv *priv,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 		       struct ethtool_rxnfc *cmd,
+#else
+		       struct mlx4_ethtool_rxnfc *cmd,
+#endif
 		       struct list_head *list_h)
 {
 	int err;
@@ -1558,7 +2195,11 @@ free_spec:
 }
 
 static int add_tcp_udp_rule(struct mlx4_en_priv *priv,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 			     struct ethtool_rxnfc *cmd,
+#else
+			     struct mlx4_ethtool_rxnfc *cmd,
+#endif
 			     struct list_head *list_h, int proto)
 {
 	int err;
@@ -1626,7 +2267,11 @@ free_spec:
 }
 
 static int mlx4_en_ethtool_to_net_trans_rule(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 					     struct ethtool_rxnfc *cmd,
+#else
+					     struct mlx4_ethtool_rxnfc *cmd,
+#endif
 					     struct list_head *rule_list_h)
 {
 	int err;
@@ -1666,7 +2311,11 @@ static int mlx4_en_ethtool_to_net_trans_
 }
 
 static int mlx4_en_flow_replace(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 				struct ethtool_rxnfc *cmd)
+#else
+				struct mlx4_ethtool_rxnfc *cmd)
+#endif
 {
 	int err;
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1742,7 +2391,11 @@ out_free_list:
 }
 
 static int mlx4_en_flow_detach(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 			       struct ethtool_rxnfc *cmd)
+#else
+			       struct mlx4_ethtool_rxnfc *cmd)
+#endif
 {
 	int err = 0;
 	struct ethtool_flow_id *rule;
@@ -1771,7 +2424,12 @@ out:
 
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_get_flow(struct net_device *dev, struct ethtool_rxnfc *cmd,
+#else
+static int mlx4_en_get_flow(struct net_device *dev,
+			    struct mlx4_ethtool_rxnfc *cmd,
+#endif
 			    int loc)
 {
 	int err = 0;
@@ -1803,13 +2461,24 @@ static int mlx4_en_get_num_flows(struct
 
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd,
+#else
+static int mlx4_en_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *c,
+#endif
+#ifdef HAVE_ETHTOOL_OPS_GET_RXNFC_U32_RULE_LOCS
 			     u32 *rule_locs)
+#else
+			     void *rule_locs)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
 	int err = 0;
 	int i = 0, priority = 0;
+#ifndef HAVE_ETHTOOL_FLOW_UNION
+	struct mlx4_ethtool_rxnfc *cmd = (struct mlx4_ethtool_rxnfc *)c;
+#endif
 
 	if ((cmd->cmd == ETHTOOL_GRXCLSRLCNT ||
 	     cmd->cmd == ETHTOOL_GRXCLSRULE ||
@@ -1832,7 +2501,11 @@ static int mlx4_en_get_rxnfc(struct net_
 		while ((!err || err == -ENOENT) && priority < cmd->rule_cnt) {
 			err = mlx4_en_get_flow(dev, cmd, i);
 			if (!err)
+#ifdef HAVE_ETHTOOL_OPS_GET_RXNFC_U32_RULE_LOCS
 				rule_locs[priority++] = i;
+#else
+				((u32 *)(rule_locs))[priority++] = i;
+#endif
 			i++;
 		}
 		err = 0;
@@ -1845,11 +2518,18 @@ static int mlx4_en_get_rxnfc(struct net_
 	return err;
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd)
+#else
+static int mlx4_en_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *c)
+#endif
 {
 	int err = 0;
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
+#ifndef HAVE_ETHTOOL_FLOW_UNION
+	struct mlx4_ethtool_rxnfc *cmd = (struct mlx4_ethtool_rxnfc *)c;
+#endif
 
 	if (mdev->dev->caps.steering_mode !=
 	    MLX4_STEERING_MODE_DEVICE_MANAGED || !priv->port_up)
@@ -1870,7 +2550,11 @@ static int mlx4_en_set_rxnfc(struct net_
 	return err;
 }
 
+#ifndef CONFIG_SYSFS_NUM_CHANNELS
 static void mlx4_en_get_channels(struct net_device *dev,
+#else
+void mlx4_en_get_channels(struct net_device *dev,
+#endif
 				 struct ethtool_channels *channel)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1884,7 +2568,11 @@ static void mlx4_en_get_channels(struct
 	channel->tx_count = priv->tx_ring_num / MLX4_EN_NUM_UP;
 }
 
+#ifndef CONFIG_SYSFS_NUM_CHANNELS
 static int mlx4_en_set_channels(struct net_device *dev,
+#else
+int mlx4_en_set_channels(struct net_device *dev,
+#endif
 				struct ethtool_channels *channel)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1900,11 +2588,13 @@ static int mlx4_en_set_channels(struct n
 	    !channel->tx_count || !channel->rx_count)
 		return -EINVAL;
 
+#ifdef HAVE_XDP_BUFF
 	if (channel->tx_count * MLX4_EN_NUM_UP <= priv->xdp_ring_num) {
 		en_err(priv, "Minimum %d tx channels required with XDP on\n",
 		       priv->xdp_ring_num / MLX4_EN_NUM_UP + 1);
 		return -EINVAL;
 	}
+#endif
 
 	tmp = kzalloc(sizeof(*tmp), GFP_KERNEL);
 	if (!tmp)
@@ -1927,8 +2617,12 @@ static int mlx4_en_set_channels(struct n
 
 	mlx4_en_safe_replace_resources(priv, tmp);
 
+#ifdef HAVE_XDP_BUFF
 	netif_set_real_num_tx_queues(dev, priv->tx_ring_num -
 							priv->xdp_ring_num);
+#else
+	netif_set_real_num_tx_queues(dev, priv->tx_ring_num);
+#endif
 	netif_set_real_num_rx_queues(dev, priv->rx_ring_num);
 
 	if (netdev_get_num_tc(dev))
@@ -1950,6 +2644,7 @@ out:
 	return err;
 }
 
+#if defined(HAVE_GET_TS_INFO) || defined(HAVE_GET_TS_INFO_EXT)
 static int mlx4_en_get_ts_info(struct net_device *dev,
 			       struct ethtool_ts_info *info)
 {
@@ -1975,12 +2670,15 @@ static int mlx4_en_get_ts_info(struct ne
 			(1 << HWTSTAMP_FILTER_NONE) |
 			(1 << HWTSTAMP_FILTER_ALL);
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 		if (mdev->ptp_clock)
 			info->phc_index = ptp_clock_index(mdev->ptp_clock);
+#endif
 	}
 
 	return ret;
 }
+#endif
 
 static int mlx4_en_set_priv_flags(struct net_device *dev, u32 flags)
 {
@@ -1993,12 +2691,18 @@ static int mlx4_en_set_priv_flags(struct
 	bool is_enabled_new = !!(flags & MLX4_EN_PRIV_FLAGS_INLINE_SCATTER);
 	bool is_enabled_old = !!(priv->pflags &
 				 MLX4_EN_PRIV_FLAGS_INLINE_SCATTER);
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 	bool qcn_disable_new = !!(flags & MLX4_EN_PRIV_FLAGS_DISABLE_32_14_4_E);
 	bool qcn_disable_old = !!(priv->pflags & MLX4_EN_PRIV_FLAGS_DISABLE_32_14_4_E);
 #endif
+#endif
 	bool ld_new = !!(flags & MLX4_EN_PRIV_FLAGS_DISABLE_MC_LOOPBACK);
 	bool ld_old = !!(priv->pflags & MLX4_EN_PRIV_FLAGS_DISABLE_MC_LOOPBACK);
+#ifndef HAVE_ETH_SS_RSS_HASH_FUNCS
+	bool rss_func_new = !!(flags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR);
+	bool rss_func_old = !!(priv->pflags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR);
+#endif
 	int i;
 	int ret = 0;
 
@@ -2025,6 +2729,39 @@ static int mlx4_en_set_priv_flags(struct
 			is_enabled_new ? "Enabled" : "Disabled");
 	}
 
+#ifndef HAVE_ETH_SS_RSS_HASH_FUNCS
+	if (rss_func_new != rss_func_old) {
+		int err = 0;
+		bool port_up = false;
+		if (rss_func_new) {
+			priv->pflags |= MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+			dev->features &= ~NETIF_F_RXHASH;
+#endif
+		} else {
+			priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+			dev->features |= NETIF_F_RXHASH;
+#endif
+		}
+
+		mutex_lock(&mdev->state_lock);
+		if (priv->port_up) {
+			port_up = true;
+			en_warn(priv,
+			"Port line mode changed, restarting port...\n");
+			mlx4_en_stop_port(dev, 1);
+		}
+		if (port_up) {
+			err = mlx4_en_start_port(dev);
+			if (err)
+				en_err(priv, "Failed restarting port %d\n",
+				       priv->port);
+		}
+		mutex_unlock(&mdev->state_lock);
+	}
+#endif
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 	if (qcn_disable_new != qcn_disable_old) {
 		ret = mlx4_disable_32_14_4_e_write(mdev->dev, qcn_disable_new,
@@ -2043,6 +2780,67 @@ static int mlx4_en_set_priv_flags(struct
 			qcn_disable_new ? "ON" : "OFF");
 	}
 #endif
+#endif
+
+#ifndef HAVE_NETIF_F_RXFCS
+	if ((flags ^ priv->pflags) & MLX4_EN_PRIV_FLAGS_RXFCS) {
+		int err = 0;
+		bool port_up = false;
+		u8 rxfcs_value = (flags & MLX4_EN_PRIV_FLAGS_RXFCS) ? 1 : 0;
+
+		if (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_IGNORE_FCS)
+		    || mlx4_is_mfunc(mdev->dev))
+			return -EOPNOTSUPP;
+
+		en_info(priv, "Turn %s RX-FCS\n", rxfcs_value ? "ON" : "OFF");
+
+		if (rxfcs_value)
+			priv->pflags |= MLX4_EN_PRIV_FLAGS_RXFCS;
+		else
+			priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RXFCS;
+
+		mutex_lock(&mdev->state_lock);
+		if (priv->port_up) {
+			port_up = true;
+			en_warn(priv,
+			"Port link mode changed, restarting port...\n");
+			mlx4_en_stop_port(dev, 1);
+		}
+		if (port_up) {
+			err = mlx4_en_start_port(dev);
+			if (err)
+				en_err(priv, "Failed restarting port %d\n",
+				       priv->port);
+		}
+		mutex_unlock(&mdev->state_lock);
+
+		if (err)
+			return err;
+	}
+#endif
+
+#ifndef HAVE_NETIF_F_RXALL
+	if ((flags ^ priv->pflags) & MLX4_EN_PRIV_FLAGS_RXALL) {
+		int ret = 0;
+		u8 rxall_value = (flags & MLX4_EN_PRIV_FLAGS_RXALL) ? 1 : 0;
+
+		if (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_IGNORE_FCS)
+		    || mlx4_is_mfunc(mdev->dev))
+			return -EOPNOTSUPP;
+
+		en_info(priv, "Turn %s RX-ALL\n", rxall_value ? "ON" : "OFF");
+
+		if (rxall_value)
+			priv->pflags |= MLX4_EN_PRIV_FLAGS_RXALL;
+		else
+			priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RXALL;
+
+		ret = mlx4_SET_PORT_fcs_check(mdev->dev,
+					      priv->port, rxall_value);
+		if (ret)
+			return ret;
+	}
+#endif
 
 	if (ld_new != ld_old) {
 		if (!(mdev->dev->caps.flags2 &
@@ -2112,6 +2910,7 @@ static u32 mlx4_en_get_priv_flags(struct
 	return priv->pflags;
 }
 
+#ifdef HAVE_GET_SET_TUNABLE
 static int mlx4_en_get_tunable(struct net_device *dev,
 			       const struct ethtool_tunable *tuna,
 			       void *data)
@@ -2160,7 +2959,9 @@ static int mlx4_en_set_tunable(struct ne
 
 	return ret;
 }
+#endif
 
+#if defined(HAVE_GET_MODULE_EEPROM) || defined(HAVE_GET_MODULE_EEPROM_EXT)
 static int mlx4_en_get_module_info(struct net_device *dev,
 				   struct ethtool_modinfo *modinfo)
 {
@@ -2241,7 +3042,9 @@ static int mlx4_en_get_module_eeprom(str
 	}
 	return 0;
 }
+#endif
 
+#if defined(HAVE_SET_PHYS_ID) || defined(HAVE_SET_PHYS_ID_EXT)
 static int mlx4_en_set_phys_id(struct net_device *dev,
 			       enum ethtool_phys_id_state state)
 {
@@ -2267,17 +3070,46 @@ static int mlx4_en_set_phys_id(struct ne
 	err = mlx4_SET_PORT_BEACON(mdev->dev, priv->port, beacon_duration);
 	return err;
 }
+#endif
 
 const struct ethtool_ops mlx4_en_ethtool_ops = {
 	.get_drvinfo = mlx4_en_get_drvinfo,
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 	.get_link_ksettings = mlx4_en_get_link_ksettings,
 	.set_link_ksettings = mlx4_en_set_link_ksettings,
+#endif
+	.get_settings = mlx4_en_get_settings,
+	.set_settings = mlx4_en_set_settings,
+#ifdef LEGACY_ETHTOOL_OPS
+#ifdef HAVE_GET_SET_FLAGS
+	.get_flags = mlx4_en_get_flags,
+	.set_flags = mlx4_en_set_flags,
+#endif
+#ifdef HAVE_GET_SET_TSO
+	.get_tso = mlx4_en_get_tso,
+	.set_tso = mlx4_en_set_tso,
+#endif
+#ifdef HAVE_GET_SET_SG
+	.get_sg = ethtool_op_get_sg,
+	.set_sg = ethtool_op_set_sg,
+#endif
+#ifdef HAVE_GET_SET_RX_CSUM
+	.get_rx_csum = mlx4_en_get_rx_csum,
+	.set_rx_csum = mlx4_en_set_rx_csum,
+#endif
+#ifdef HAVE_GET_SET_TX_CSUM
+	.get_tx_csum = ethtool_op_get_tx_csum,
+	.set_tx_csum = ethtool_op_set_tx_ipv6_csum,
+#endif
+#endif
 	.get_link = ethtool_op_get_link,
 	.get_strings = mlx4_en_get_strings,
 	.get_sset_count = mlx4_en_get_sset_count,
 	.get_ethtool_stats = mlx4_en_get_ethtool_stats,
 	.self_test = mlx4_en_self_test,
+#if defined(HAVE_SET_PHYS_ID) && !defined(HAVE_SET_PHYS_ID_EXT)
 	.set_phys_id = mlx4_en_set_phys_id,
+#endif
 	.get_wol = mlx4_en_get_wol,
 	.set_wol = mlx4_en_set_wol,
 	.get_msglevel = mlx4_en_get_msglevel,
@@ -2290,22 +3122,59 @@ const struct ethtool_ops mlx4_en_ethtool
 	.set_ringparam = mlx4_en_set_ringparam,
 	.get_rxnfc = mlx4_en_get_rxnfc,
 	.set_rxnfc = mlx4_en_set_rxnfc,
+#if defined(HAVE_RXFH_INDIR_SIZE) && !defined(HAVE_RXFH_INDIR_SIZE_EXT)
 	.get_rxfh_indir_size = mlx4_en_get_rxfh_indir_size,
+#endif
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	.get_rxfh_key_size = mlx4_en_get_rxfh_key_size,
 	.get_rxfh = mlx4_en_get_rxfh,
 	.set_rxfh = mlx4_en_set_rxfh,
+#elif defined(HAVE_GET_SET_RXFH_INDIR) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
+	.get_rxfh_indir = mlx4_en_get_rxfh_indir,
+	.set_rxfh_indir = mlx4_en_set_rxfh_indir,
+#endif
+#ifdef HAVE_GET_SET_CHANNELS
 	.get_channels = mlx4_en_get_channels,
 	.set_channels = mlx4_en_set_channels,
+#endif
+#if defined(HAVE_GET_TS_INFO) && !defined(HAVE_GET_TS_INFO_EXT)
 	.get_ts_info = mlx4_en_get_ts_info,
+#endif
 	.set_priv_flags = mlx4_en_set_priv_flags,
 	.get_priv_flags = mlx4_en_get_priv_flags,
+#ifdef HAVE_GET_SET_TUNABLE
 	.get_tunable		= mlx4_en_get_tunable,
 	.set_tunable		= mlx4_en_set_tunable,
+#endif
+#ifdef HAVE_GET_MODULE_EEPROM
 	.get_module_info = mlx4_en_get_module_info,
 	.get_module_eeprom = mlx4_en_get_module_eeprom
+#endif
 };
 
-
-
-
-
+#ifdef HAVE_ETHTOOL_OPS_EXT
+const struct ethtool_ops_ext mlx4_en_ethtool_ops_ext = {
+	.size = sizeof(struct ethtool_ops_ext),
+#ifdef HAVE_RXFH_INDIR_SIZE_EXT
+	.get_rxfh_indir_size = mlx4_en_get_rxfh_indir_size,
+#endif
+#ifdef HAVE_GET_SET_RXFH_INDIR_EXT
+	.get_rxfh_indir = mlx4_en_get_rxfh_indir,
+	.set_rxfh_indir = mlx4_en_set_rxfh_indir,
+#endif
+#ifdef HAVE_GET_SET_CHANNELS_EXT
+	.get_channels = mlx4_en_get_channels,
+	.set_channels = mlx4_en_set_channels,
+#endif
+#ifdef HAVE_GET_TS_INFO_EXT
+	.get_ts_info = mlx4_en_get_ts_info,
+#endif
+#ifdef HAVE_SET_PHYS_ID_EXT
+	.set_phys_id = mlx4_en_set_phys_id,
+#endif
+#ifdef HAVE_GET_MODULE_EEPROM_EXT
+	.get_module_info = mlx4_en_get_module_info,
+	.get_module_eeprom = mlx4_en_get_module_eeprom,
+#endif
+};
+#endif
--- a/drivers/net/ethernet/mellanox/mlx4/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_main.c
@@ -78,6 +78,7 @@ MLX4_EN_PARM_INT(inline_thold, MAX_INLIN
 #define MAX_PFC_TX     0xff
 #define MAX_PFC_RX     0xff
 
+#if defined(HAVE_VA_FORMAT) && !defined(CONFIG_X86_XEN)
 void en_print(const char *level, const struct mlx4_en_priv *priv,
 	      const char *format, ...)
 {
@@ -97,6 +98,7 @@ void en_print(const char *level, const s
 		       priv->port, &vaf);
 	va_end(args);
 }
+#endif
 
 void mlx4_en_update_loopback_state(struct net_device *dev,
 				   netdev_features_t features)
@@ -259,12 +261,14 @@ static void mlx4_en_activate(struct mlx4
 			mdev->pndev[i] = NULL;
 	}
 
+#ifdef HAVE_NETDEV_BONDING_INFO
 	/* register notifier */
 	mdev->nb.notifier_call = mlx4_en_netdev_event;
 	if (register_netdevice_notifier(&mdev->nb)) {
 		mdev->nb.notifier_call = NULL;
 		mlx4_err(mdev, "Failed to create notifier\n");
 	}
+#endif
 }
 
 static void *mlx4_en_add(struct mlx4_dev *dev)
@@ -386,7 +390,9 @@ static void mlx4_en_verify_params(void)
 static int __init mlx4_en_init(void)
 {
 	mlx4_en_verify_params();
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 	mlx4_en_init_ptys2ethtool_map();
+#endif
 
 	return mlx4_register_interface(&mlx4_en_interface);
 }
--- a/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
@@ -31,7 +31,9 @@
  *
  */
 
+#ifdef HAVE_XDP_BUFF
 #include <linux/bpf.h>
+#endif
 #include <linux/etherdevice.h>
 #include <linux/tcp.h>
 #include <linux/if_vlan.h>
@@ -39,9 +41,12 @@
 #include <linux/slab.h>
 #include <linux/hash.h>
 #include <net/ip.h>
-#include <net/busy_poll.h>
+#if (defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)) || defined(HAVE_UDP_TUNNEL_GET_RX_INFO)
 #include <net/vxlan.h>
+#endif
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 
 #include <linux/mlx4/driver.h>
 #include <linux/mlx4/device.h>
@@ -51,6 +56,10 @@
 #include "mlx4_en.h"
 #include "en_port.h"
 
+#ifdef MLX4_EN_BUSY_POLL
+#include <net/busy_poll.h>
+#endif
+
 static int udev_dev_port_dev_id = 0;
 module_param(udev_dev_port_dev_id, int, 0444);
 MODULE_PARM_DESC(udev_dev_port_dev_id, "Work with dev_id or dev_port when"
@@ -100,6 +109,8 @@ int mlx4_en_setup_tc(struct net_device *
 	return 0;
 }
 
+#ifdef HAVE_NDO_SETUP_TC
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
 static int __mlx4_en_setup_tc(struct net_device *dev, u32 handle, __be16 proto,
 			      struct tc_to_netdev *tc)
 {
@@ -108,8 +119,39 @@ static int __mlx4_en_setup_tc(struct net
 
 	return mlx4_en_setup_tc(dev, tc->tc);
 }
+#endif
+#endif
+
+#ifdef MLX4_EN_BUSY_POLL
+/* must be called with local_bh_disable()d */
+static int mlx4_en_low_latency_recv(struct napi_struct *napi)
+{
+	struct mlx4_en_cq *cq = container_of(napi, struct mlx4_en_cq, napi);
+	struct net_device *dev = cq->dev;
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	struct mlx4_en_rx_ring *rx_ring = priv->rx_ring[cq->ring];
+	int done;
+
+	if (!priv->port_up)
+		return LL_FLUSH_FAILED;
+
+	if (!mlx4_en_cq_lock_poll(cq))
+		return LL_FLUSH_BUSY;
+
+	done = mlx4_en_process_rx_cq(dev, cq, 4);
+	if (likely(done))
+		rx_ring->cleaned += done;
+	else
+		rx_ring->misses++;
+
+	mlx4_en_cq_unlock_poll(cq);
+
+	return done;
+}
+#endif
 
 #ifdef CONFIG_RFS_ACCEL
+#ifdef HAVE_NDO_RX_FLOW_STEER
 
 struct mlx4_en_filter {
 	struct list_head next;
@@ -289,11 +331,12 @@ mlx4_en_filter_find(struct mlx4_en_priv
 {
 	struct mlx4_en_filter *filter;
 	struct mlx4_en_filter *ret = NULL;
+	COMPAT_HL_NODE
 
-	hlist_for_each_entry(filter,
-			     filter_hash_bucket(priv, src_ip, dst_ip,
-						src_port, dst_port),
-			     filter_chain) {
+	compat_hlist_for_each_entry(filter,
+				    filter_hash_bucket(priv, src_ip, dst_ip,
+						       src_port, dst_port),
+				    filter_chain) {
 		if (filter->src_ip == src_ip &&
 		    filter->dst_ip == dst_ip &&
 		    filter->ip_proto == ip_proto &&
@@ -419,6 +462,18 @@ static void mlx4_en_filter_rfs_expire(st
 		mlx4_en_filter_free(filter);
 }
 #endif
+#endif
+
+#ifdef HAVE_VLAN_GRO_RECEIVE
+static void mlx4_en_vlan_rx_register(struct net_device *dev,
+				     struct vlan_group *grp)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+
+	en_dbg(HW, priv, "Registering VLAN group:%p\n", grp);
+	priv->vlgrp = grp;
+}
+#endif
 
 static void mlx4_en_remove_tx_rings_per_vlan(struct mlx4_en_priv *priv, int vid)
 {
@@ -597,8 +652,14 @@ uc_steer_add_err:
 	return err;
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 static int mlx4_en_vlan_rx_add_vid(struct net_device *dev,
 				   __be16 proto, u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+static int mlx4_en_vlan_rx_add_vid(struct net_device *dev, unsigned short vid)
+#else
+static void mlx4_en_vlan_rx_add_vid(struct net_device *dev, unsigned short vid)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
@@ -631,7 +692,10 @@ static int mlx4_en_vlan_rx_add_vid(struc
 
 out:
 	mutex_unlock(&mdev->state_lock);
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return err;
+#endif
 }
 
 static int mlx4_en_vgtp_kill_vid(struct net_device *dev, unsigned short vid)
@@ -650,8 +714,14 @@ static int mlx4_en_vgtp_kill_vid(struct
 	return 0;
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 static int mlx4_en_vlan_rx_kill_vid(struct net_device *dev,
 				    __be16 proto, u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+static int mlx4_en_vlan_rx_kill_vid(struct net_device *dev, unsigned short vid)
+#else
+static void mlx4_en_vlan_rx_kill_vid(struct net_device *dev, unsigned short vid)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
@@ -692,7 +762,10 @@ static int mlx4_en_vlan_rx_kill_vid(stru
 out:
 	mutex_unlock(&mdev->state_lock);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return err;
+#endif
 }
 
 int mlx4_en_vgtp_alloc_res(struct mlx4_en_priv *priv)
@@ -912,9 +985,10 @@ static int mlx4_en_replace_mac(struct ml
 		struct mlx4_mac_entry *entry;
 		struct hlist_node *tmp;
 		u64 prev_mac_u64 = mlx4_mac_to_u64(prev_mac);
+		COMPAT_HL_NODE
 
 		bucket = &priv->mac_hash[prev_mac[MLX4_EN_MAC_HASH_IDX]];
-		hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
+		compat_hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
 			if (ether_addr_equal_64bits(entry->mac, prev_mac)) {
 				mlx4_en_uc_steer_release(priv, entry->mac,
 							 qpn, entry->reg_id);
@@ -1007,17 +1081,30 @@ static void mlx4_en_clear_list(struct ne
 static void mlx4_en_cache_mclist(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	struct netdev_hw_addr *ha;
+#else
+	struct dev_mc_list *mclist;
+#endif
+
 	struct mlx4_en_mc_list *tmp;
 
 	mlx4_en_clear_list(dev);
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	netdev_for_each_mc_addr(ha, dev) {
+#else
+	for (mclist = dev->mc_list; mclist; mclist = mclist->next) {
+#endif
 		tmp = kzalloc(sizeof(struct mlx4_en_mc_list), GFP_ATOMIC);
 		if (!tmp) {
 			mlx4_en_clear_list(dev);
 			return;
 		}
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 		memcpy(tmp->addr, ha->addr, ETH_ALEN);
+#else
+		memcpy(tmp->addr, mclist->dmi_addr, ETH_ALEN);
+#endif
 		list_add_tail(&tmp->list, &priv->mc_list);
 	}
 }
@@ -1367,6 +1454,7 @@ static void mlx4_en_do_uc_filter(struct
 	unsigned int i;
 	int removed = 0;
 	u32 prev_flags;
+	COMPAT_HL_NODE
 
 	/* Note that we do not need to protect our mac_hash traversal with rcu,
 	 * since all modification code is protected by mdev->state_lock
@@ -1375,7 +1463,7 @@ static void mlx4_en_do_uc_filter(struct
 	/* find what to remove */
 	for (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i) {
 		bucket = &priv->mac_hash[i];
-		hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
+		compat_hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
 			found = false;
 			netdev_for_each_uc_addr(ha, dev) {
 				if (ether_addr_equal_64bits(entry->mac,
@@ -1419,7 +1507,7 @@ static void mlx4_en_do_uc_filter(struct
 	netdev_for_each_uc_addr(ha, dev) {
 		found = false;
 		bucket = &priv->mac_hash[ha->addr[MLX4_EN_MAC_HASH_IDX]];
-		hlist_for_each_entry(entry, bucket, hlist) {
+		compat_hlist_for_each_entry(entry, bucket, hlist) {
 			if (ether_addr_equal_64bits(entry->mac, ha->addr)) {
 				found = true;
 				break;
@@ -1509,7 +1597,11 @@ static void mlx4_en_do_set_rx_mode(struc
 		}
 	}
 
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	if (dev->priv_flags & IFF_UNICAST_FLT)
+#else
+	if (mdev->dev->caps.steering_mode != MLX4_STEERING_MODE_A0)
+#endif
 		mlx4_en_do_uc_filter(priv, dev, mdev);
 
 	promisc = (dev->flags & IFF_PROMISC) ||
@@ -1594,10 +1686,11 @@ static void mlx4_en_delete_rss_steer_rul
 	struct hlist_head *bucket;
 	struct hlist_node *tmp;
 	struct mlx4_mac_entry *entry;
+	COMPAT_HL_NODE
 
 	for (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i) {
 		bucket = &priv->mac_hash[i];
-		hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
+		compat_hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
 			mac = mlx4_mac_to_u64(entry->mac);
 			en_dbg(DRV, priv, "Registering MAC:%pM for deleting\n",
 			       entry->mac);
@@ -1638,17 +1731,28 @@ static void mlx4_en_tx_timeout(struct ne
 	queue_work(mdev->workqueue, &priv->watchdog_task);
 }
 
-
+#if (defined(HAVE_NDO_GET_STATS64) && defined(HAVE_NETDEV_STATS_TO_STATS64))
 static struct rtnl_link_stats64 *
 mlx4_en_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#else
+static struct net_device_stats *mlx4_en_get_stats(struct net_device *dev)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 
 	spin_lock_bh(&priv->stats_lock);
+#if (defined(HAVE_NDO_GET_STATS64) && defined(HAVE_NETDEV_STATS_TO_STATS64))
 	netdev_stats_to_stats64(stats, &dev->stats);
+#else
+	memcpy(&priv->ret_stats, &dev->stats, sizeof(priv->ret_stats));
+#endif
 	spin_unlock_bh(&priv->stats_lock);
 
+#if (defined(HAVE_NDO_GET_STATS64) && defined(HAVE_NETDEV_STATS_TO_STATS64))
 	return stats;
+#else
+	return &priv->ret_stats;
+#endif
 }
 
 static void mlx4_en_set_default_moderation(struct mlx4_en_priv *priv)
@@ -1852,6 +1956,7 @@ static void mlx4_en_free_affinity_hint(s
 	free_cpumask_var(priv->rx_ring[ring_idx]->affinity_mask);
 }
 
+#ifdef HAVE_XDP_BUFF
 static void mlx4_en_init_recycle_ring(struct mlx4_en_priv *priv,
 				      int tx_ring_idx)
 {
@@ -1869,6 +1974,7 @@ static void mlx4_en_init_recycle_ring(st
 		tx_ring->recycle_ring = NULL;
 	}
 }
+#endif
 
 int mlx4_en_start_port(struct net_device *dev)
 {
@@ -1908,6 +2014,8 @@ int mlx4_en_start_port(struct net_device
 	for (i = 0; i < priv->rx_ring_num; i++) {
 		cq = priv->rx_cq[i];
 
+		mlx4_en_cq_init_lock(cq);
+
 		err = mlx4_en_init_affinity_hint(priv, i);
 		if (err) {
 			en_err(priv, "Failed preparing IRQ affinity hint\n");
@@ -1992,7 +2100,9 @@ int mlx4_en_start_port(struct net_device
 		}
 		tx_ring->tx_queue = netdev_get_tx_queue(dev, i);
 
+#ifdef HAVE_XDP_BUFF
 		mlx4_en_init_recycle_ring(priv, i);
+#endif
 
 		/* Arm CQ for TX completions */
 		mlx4_en_arm_cq(priv, cq);
@@ -2066,8 +2176,14 @@ int mlx4_en_start_port(struct net_device
 	/* Schedule multicast task to populate multicast list */
 	queue_work(mdev->workqueue, &priv->rx_mode_task);
 
+#if defined(HAVE_VXLAN_DYNAMIC_PORT) || defined(HAVE_UDP_TUNNEL_GET_RX_INFO)
 	if (priv->mdev->dev->caps.tunnel_offload_mode == MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)
+#ifdef HAVE_UDP_TUNNEL_GET_RX_INFO
 		udp_tunnel_get_rx_info(dev);
+#elif defined(HAVE_VXLAN_DYNAMIC_PORT)
+		vxlan_get_rx_port(dev);
+#endif
+#endif
 
 	priv->port_up = true;
 
@@ -2201,6 +2317,13 @@ void mlx4_en_stop_port(struct net_device
 	for (i = 0; i < priv->rx_ring_num; i++) {
 		struct mlx4_en_cq *cq = priv->rx_cq[i];
 
+		local_bh_disable();
+		while (!mlx4_en_cq_lock_napi(cq)) {
+			pr_info("CQ %d locked\n", i);
+			mdelay(1);
+		}
+		local_bh_enable();
+
 		napi_synchronize(&cq->napi);
 		mlx4_en_deactivate_rx_ring(priv, priv->rx_ring[i]);
 		mlx4_en_deactivate_cq(priv, &priv->rx_cq[i]);
@@ -2411,6 +2534,83 @@ struct en_port_attribute en_port_attr_tx
 						       mlx4_en_show_tx_rate,
 						       NULL);
 
+#if (defined(HAVE_NETIF_F_HW_VLAN_STAG_RX) && !defined(HAVE_VF_VLAN_PROTO))
+static ssize_t mlx4_en_show_vf_vlan_info(struct en_port *en_p,
+					 struct en_port_attribute *attr,
+					 char *buf)
+{
+	return mlx4_get_vf_vlan_info(en_p->dev, en_p->port_num,
+				     en_p->vport_num, buf);
+}
+
+static ssize_t mlx4_en_store_vf_vlan_info(struct en_port *en_p,
+					  struct en_port_attribute *attr,
+					  const char *buf, size_t count)
+{
+	int err, num_args, i = 0;
+	u16 vlan;
+	u16 qos;
+	__be16 vlan_proto;
+	char save;
+
+	const char *tmp_tok[7] = {NULL};
+
+	do {
+		int len;
+
+		len = strcspn(buf, " \n");
+		/* nul-terminate and break to tokens */
+		save = buf[len];
+		((char *)buf)[len] = '\0';
+		tmp_tok[i++] = buf;
+		buf += len+1;
+	} while (save == ' ' && i < 7);
+
+	num_args = i;
+	if (num_args < 2 || num_args > 6)
+		return -EINVAL;
+	i = 0;
+	if (strcmp(tmp_tok[i], "vlan") != 0)
+		return -EINVAL;
+
+	if (sscanf(tmp_tok[i+1], "%hu", &vlan) != 1 || vlan > VLAN_MAX_VALUE)
+		return -EINVAL;
+	qos = 0;
+	vlan_proto = htons(ETH_P_8021Q);
+
+	i += 2;
+	if ((i+1 < num_args) && !strcmp(tmp_tok[i], "qos") ) {
+		if (sscanf(tmp_tok[i+1], "%hu", &qos) != 1 ||
+		    qos > 7)
+			return -EINVAL;
+		i += 2;
+	}
+	if ((i+1 < num_args) && strcmp(tmp_tok[i], "proto") == 0) {
+		if ((strcmp(tmp_tok[i+1], "802.1Q") == 0) ||
+		    (strcmp(tmp_tok[i+1], "802.1q") == 0))
+			vlan_proto = htons(ETH_P_8021Q);
+		else if ((strcmp(tmp_tok[i+1], "802.1AD") == 0) ||
+			 (strcmp(tmp_tok[i+1], "802.1ad") == 0))
+			vlan_proto = htons(ETH_P_8021AD);
+		else {
+			return -EINVAL;
+		}
+		i += 2;
+	}
+	if (i < num_args)
+		return -EINVAL;
+
+	err = mlx4_set_vf_vlan(en_p->dev, en_p->port_num, en_p->vport_num,
+			       vlan, qos, vlan_proto);
+	return err ? err : count;
+}
+
+struct en_port_attribute en_port_attr_vlan_info = __ATTR(vlan_info,
+							 S_IRUGO | S_IWUSR,
+							 mlx4_en_show_vf_vlan_info,
+							 mlx4_en_store_vf_vlan_info);
+#endif
+
 static ssize_t en_port_show(struct kobject *kobj,
 			    struct attribute *attr, char *buf)
 {
@@ -2508,6 +2708,9 @@ static struct attribute *vf_attrs[] = {
 	&en_port_attr_link_state.attr,
 	&en_port_attr_vlan_set.attr,
 	&en_port_attr_tx_rate.attr,
+#if (defined(HAVE_NETIF_F_HW_VLAN_STAG_RX) && !defined(HAVE_VF_VLAN_PROTO))
+	&en_port_attr_vlan_info.attr,
+#endif
 	NULL
 };
 
@@ -2516,6 +2719,115 @@ static struct kobj_type en_port_type = {
 	.default_attrs = vf_attrs,
 };
 
+#ifdef CONFIG_SYSFS_FDB
+static ssize_t mlx4_en_show_fdb(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	struct net_device *netdev = to_net_dev(dev);
+	ssize_t len = 0;
+	struct netdev_hw_addr *ha;
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
+	struct netdev_hw_addr *mc;
+#else
+	struct dev_addr_list *mc;
+#endif
+
+	netif_addr_lock_bh(netdev);
+
+	netdev_for_each_uc_addr(ha, netdev) {
+		len += sprintf(&buf[len], "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			       ha->addr[0], ha->addr[1], ha->addr[2],
+			       ha->addr[3], ha->addr[4], ha->addr[5]);
+	}
+	netdev_for_each_mc_addr(mc, netdev) {
+		len += sprintf(&buf[len], "%02x:%02x:%02x:%02x:%02x:%02x\n",
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
+			mc->addr[0], mc->addr[1], mc->addr[2],
+			mc->addr[3], mc->addr[4], mc->addr[5]);
+#else
+			mc->da_addr[0], mc->da_addr[1], mc->da_addr[2],
+			mc->da_addr[3], mc->da_addr[4], mc->da_addr[5]);
+#endif
+	}
+
+	netif_addr_unlock_bh(netdev);
+
+	return len;
+}
+
+static ssize_t mlx4_en_set_fdb(struct device *dev,
+			       struct device_attribute *attr,
+			       const char *buf, size_t count)
+{
+	struct net_device *netdev = to_net_dev(dev);
+	struct mlx4_en_priv *priv = netdev_priv(netdev);
+	unsigned char mac[ETH_ALEN];
+	unsigned int tmp[ETH_ALEN];
+	int add = 0;
+	int err, i;
+
+	if (count < sizeof("-01:02:03:04:05:06"))
+		return -EINVAL;
+
+	if (!priv->mdev)
+		return -EOPNOTSUPP;
+
+	switch (buf[0]) {
+	case '-':
+		break;
+	case '+':
+		add = 1;
+		break;
+	default:
+		return -EINVAL;
+	}
+	err = sscanf(&buf[1], "%02x:%02x:%02x:%02x:%02x:%02x",
+		     &tmp[0], &tmp[1], &tmp[2], &tmp[3], &tmp[4], &tmp[5]);
+
+	if (err != ETH_ALEN)
+		return -EINVAL;
+	for (i = 0; i < ETH_ALEN; ++i)
+		mac[i] = tmp[i] & 0xff;
+
+	/* make sure all the other fdb actions are done,
+	 * otherwise no way to know the current state.
+	 */
+	flush_work(&priv->rx_mode_task);
+	if (add) {
+		if (!mlx4_is_available_mac(priv->mdev->dev, priv->port)) {
+			mlx4_warn(priv->mdev,
+				  "Cannot add mac:%pM, no free macs.\n", mac);
+			return -EINVAL;
+		}
+	}
+
+	rtnl_lock();
+	if (is_unicast_ether_addr(mac)) {
+		if (add)
+			err = dev_uc_add_excl(netdev, mac);
+		else
+			err = dev_uc_del(netdev, mac);
+	} else if (is_multicast_ether_addr(mac)) {
+		if (add)
+			err = dev_mc_add_excl(netdev, mac);
+		else
+			err = dev_mc_del(netdev, mac);
+	} else {
+		rtnl_unlock();
+		return -EINVAL;
+	}
+	rtnl_unlock();
+
+	en_dbg(DRV, priv, "Port:%d: %s %pM\n", priv->port,
+	       add ? "adding" : "removing", mac);
+
+	return err ? err : count;
+}
+
+static DEVICE_ATTR(fdb, S_IRUGO | 002, mlx4_en_show_fdb, mlx4_en_set_fdb);
+#endif
+
 static void mlx4_en_clear_stats(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -2607,9 +2919,11 @@ static void mlx4_en_free_resources(struc
 {
 	int i;
 
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #ifdef CONFIG_RFS_ACCEL
 	priv->dev->rx_cpu_rmap = NULL;
 #endif
+#endif
 
 	for (i = 0; i < priv->tx_ring_num; i++) {
 		if (priv->tx_ring && priv->tx_ring[i])
@@ -2660,9 +2974,11 @@ static int mlx4_en_alloc_resources(struc
 			goto err;
 	}
 
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #ifdef CONFIG_RFS_ACCEL
 	priv->dev->rx_cpu_rmap = mlx4_get_cpu_rmap(priv->mdev->dev, priv->port);
 #endif
+#endif
 
 	return 0;
 
@@ -2760,6 +3076,30 @@ void mlx4_en_safe_replace_resources(stru
 	mlx4_en_update_priv(priv, tmp);
 }
 
+#ifdef CONFIG_SYSFS_FDB
+/* returns the details of the mac table. used only in multi_function mode */
+static ssize_t mlx4_en_show_fdb_details(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct net_device *netdev = to_net_dev(dev);
+	struct mlx4_en_priv *priv = netdev_priv(netdev);
+	int free_macs = mlx4_get_port_free_macs(priv->mdev->dev, priv->port);
+	int max_macs = mlx4_get_port_max_macs(priv->mdev->dev, priv->port);
+	int total = mlx4_get_port_total_macs(priv->mdev->dev, priv->port);
+	ssize_t len = 0;
+
+	/* in VF the macs that allocated before it been opened are count */
+	total = min(max_macs, total);
+	len += sprintf(&buf[len],
+		       "FDB details: device %s: max: %d, used: %d, free macs: %d\n",
+		       netdev->name, max_macs, total, free_macs);
+
+	return len;
+}
+static DEVICE_ATTR(fdb_det, S_IRUGO, mlx4_en_show_fdb_details, NULL);
+#endif
+
 void mlx4_en_destroy_netdev(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -2767,10 +3107,23 @@ void mlx4_en_destroy_netdev(struct net_d
 
 	en_dbg(DRV, priv, "Destroying netdev on port:%d\n", priv->port);
 
+#ifdef CONFIG_COMPAT_EN_SYSFS
+	if (priv->sysfs_group_initialized)
+		mlx4_en_sysfs_remove(dev);
+#endif
+
+#ifdef CONFIG_SYSFS_FDB
+	if (priv->sysfs_fdb_created) {
+		device_remove_file(&dev->dev, &dev_attr_fdb_det);
+		device_remove_file(&dev->dev, &dev_attr_fdb);
+	}
+#endif
 	/* Unregister device - this will close the port if it was up */
 	if (priv->registered) {
+#ifdef HAVE_DEVLINK_H
 		devlink_port_type_clear(mlx4_get_devlink_port(mdev->dev,
 							      priv->port));
+#endif
 		unregister_netdev(dev);
 	}
 
@@ -2805,8 +3158,10 @@ void mlx4_en_destroy_netdev(struct net_d
 	}
 
 #ifdef CONFIG_RFS_ACCEL
+#ifdef HAVE_NDO_RX_FLOW_STEER
 	mlx4_en_cleanup_filters(priv);
 #endif
+#endif
 
 	mlx4_en_free_resources(priv);
 
@@ -2834,11 +3189,13 @@ static int mlx4_en_change_mtu(struct net
 		en_err(priv, "Bad MTU size:%d.\n", new_mtu);
 		return -EPERM;
 	}
+#ifdef HAVE_XDP_BUFF
 	if (priv->xdp_ring_num && MLX4_EN_EFF_MTU(new_mtu) > FRAG_SZ0) {
 		en_err(priv, "MTU size:%d requires frags but XDP running\n",
 		       new_mtu);
 		return -EOPNOTSUPP;
 	}
+#endif
 
 	if (priv->prof->inline_scatter_thold >= MIN_INLINE_SCATTER) {
 		en_err(priv, "Please disable RX Copybreak by setting to 0\n");
@@ -2926,6 +3283,7 @@ static int mlx4_en_hwtstamp_set(struct n
 			    sizeof(config)) ? -EFAULT : 0;
 }
 
+#ifdef SIOCGHWTSTAMP
 static int mlx4_en_hwtstamp_get(struct net_device *dev, struct ifreq *ifr)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -2933,19 +3291,23 @@ static int mlx4_en_hwtstamp_get(struct n
 	return copy_to_user(ifr->ifr_data, &priv->hwtstamp_config,
 			    sizeof(priv->hwtstamp_config)) ? -EFAULT : 0;
 }
+#endif
 
 static int mlx4_en_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 {
 	switch (cmd) {
 	case SIOCSHWTSTAMP:
 		return mlx4_en_hwtstamp_set(dev, ifr);
+#ifdef SIOCGHWTSTAMP
 	case SIOCGHWTSTAMP:
 		return mlx4_en_hwtstamp_get(dev, ifr);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
 }
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 static netdev_features_t mlx4_en_fix_features(struct net_device *netdev,
 					      netdev_features_t features)
 {
@@ -2964,20 +3326,31 @@ static netdev_features_t mlx4_en_fix_fea
 
 	return features;
 }
+#endif
 
-static int mlx4_en_set_features(struct net_device *netdev,
-		netdev_features_t features)
+#ifndef CONFIG_SYSFS_LOOPBACK
+static
+#endif
+int mlx4_en_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			 u32 features)
+#else
+			 netdev_features_t features)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 	bool reset = false;
 	int ret = 0;
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_RXFCS)) {
 		en_info(priv, "Turn %s RX-FCS\n",
 			(features & NETIF_F_RXFCS) ? "ON" : "OFF");
 		reset = true;
 	}
+#endif
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_RXALL)) {
 		u8 ignore_fcs_value = (features & NETIF_F_RXALL) ? 1 : 0;
 
@@ -2988,6 +3361,7 @@ static int mlx4_en_set_features(struct n
 		if (ret)
 			return ret;
 	}
+#endif
 
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_HW_VLAN_CTAG_RX)) {
 		en_info(priv, "Turn %s RX vlan strip offload\n",
@@ -2999,9 +3373,11 @@ static int mlx4_en_set_features(struct n
 		en_info(priv, "Turn %s TX vlan strip offload\n",
 			(features & NETIF_F_HW_VLAN_CTAG_TX) ? "ON" : "OFF");
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_HW_VLAN_STAG_TX))
 		en_info(priv, "Turn %s TX S-VLAN strip offload\n",
 			(features & NETIF_F_HW_VLAN_STAG_TX) ? "ON" : "OFF");
+#endif
 
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_LOOPBACK)) {
 		en_info(priv, "Turn %s loopback\n",
@@ -3027,16 +3403,24 @@ static int mlx4_en_set_vf_mac(struct net
 	return mlx4_set_vf_mac(mdev->dev, en_priv->port, queue, mac);
 }
 
+#ifdef HAVE_VF_VLAN_PROTO
 static int mlx4_en_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos,
 			       __be16 vlan_proto)
+#else
+static int mlx4_en_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos)
+#endif
 {
 	struct mlx4_en_priv *en_priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = en_priv->mdev;
+#ifndef HAVE_VF_VLAN_PROTO
+	__be16 vlan_proto = htons(ETH_P_8021Q);
+#endif
 
 	return mlx4_set_vf_vlan(mdev->dev, en_priv->port, vf, vlan, qos,
 				vlan_proto);
 }
 
+#ifdef HAVE_TX_RATE_LIMIT
 static int mlx4_en_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate,
 			       int max_tx_rate)
 {
@@ -3046,7 +3430,17 @@ static int mlx4_en_set_vf_rate(struct ne
 	return mlx4_set_vf_rate(mdev->dev, en_priv->port, vf, min_tx_rate,
 				max_tx_rate);
 }
+#elif defined(HAVE_VF_TX_RATE)
+static int mlx4_en_set_vf_tx_rate(struct net_device *dev, int vf, int rate)
+{
+	struct mlx4_en_priv *en_priv = netdev_priv(dev);
+	struct mlx4_en_dev *mdev = en_priv->mdev;
+
+	return mlx4_set_vf_rate(mdev->dev, en_priv->port, vf, 0, rate);
+}
+#endif
 
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK)
 static int mlx4_en_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx4_en_priv *en_priv = netdev_priv(dev);
@@ -3054,6 +3448,7 @@ static int mlx4_en_set_vf_spoofchk(struc
 
 	return mlx4_set_vf_spoofchk(mdev->dev, en_priv->port, vf, setting);
 }
+#endif
 
 static int mlx4_en_get_vf_config(struct net_device *dev, int vf, struct ifla_vf_info *ivf)
 {
@@ -3063,6 +3458,7 @@ static int mlx4_en_get_vf_config(struct
 	return mlx4_get_vf_config(mdev->dev, en_priv->port, vf, ivf);
 }
 
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
 static int mlx4_en_set_vf_link_state(struct net_device *dev, int vf, int link_state)
 {
 	struct mlx4_en_priv *en_priv = netdev_priv(dev);
@@ -3070,7 +3466,9 @@ static int mlx4_en_set_vf_link_state(str
 
 	return mlx4_set_vf_link_state(mdev->dev, en_priv->port, vf, link_state);
 }
+#endif
 
+#ifdef HAVE_NDO_GET_VF_STATS
 static int mlx4_en_get_vf_stats(struct net_device *dev, int vf,
 				struct ifla_vf_stats *vf_stats)
 {
@@ -3079,10 +3477,16 @@ static int mlx4_en_get_vf_stats(struct n
 
 	return mlx4_get_vf_stats(mdev->dev, en_priv->port, vf, vf_stats);
 }
+#endif
 
+#if defined(HAVE_NETDEV_NDO_GET_PHYS_PORT_ID) || defined(HAVE_NETDEV_EXT_NDO_GET_PHYS_PORT_ID)
 #define PORT_ID_BYTE_LEN 8
 static int mlx4_en_get_phys_port_id(struct net_device *dev,
+#ifdef HAVE_NETDEV_PHYS_ITEM_ID
 				    struct netdev_phys_item_id *ppid)
+#else
+				    struct netdev_phys_port_id *ppid)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_dev *mdev = priv->mdev->dev;
@@ -3099,6 +3503,7 @@ static int mlx4_en_get_phys_port_id(stru
 	}
 	return 0;
 }
+#endif
 
 static void mlx4_en_add_vxlan_offloads(struct work_struct *work)
 {
@@ -3118,13 +3523,21 @@ out:
 		return;
 	}
 
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 	/* set offloads */
 	priv->dev->hw_enc_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
 				      NETIF_F_RXCSUM |
 				      NETIF_F_TSO | NETIF_F_TSO6 |
 				      NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 				      NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 				      NETIF_F_GSO_PARTIAL;
+#else
+				      0;
+#endif
+#endif
 }
 
 static void mlx4_en_del_vxlan_offloads(struct work_struct *work)
@@ -3132,13 +3545,21 @@ static void mlx4_en_del_vxlan_offloads(s
 	int ret;
 	struct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,
 						 vxlan_del_task);
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 	/* unset offloads */
 	priv->dev->hw_enc_features &= ~(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
 					NETIF_F_RXCSUM |
 					NETIF_F_TSO | NETIF_F_TSO6 |
 					NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 					NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 					NETIF_F_GSO_PARTIAL);
+#else
+					0);
+#endif
+#endif
 
 	ret = mlx4_SET_PORT_VXLAN(priv->mdev->dev, priv->port,
 				  VXLAN_STEER_BY_OUTER_MAC, 0);
@@ -3148,6 +3569,8 @@ static void mlx4_en_del_vxlan_offloads(s
 	priv->vxlan_port = 0;
 }
 
+#if defined(HAVE_VXLAN_DYNAMIC_PORT) || defined(HAVE_UDP_TUNNEL_GET_RX_INFO)
+#ifdef HAVE_UDP_TUNNEL_GET_RX_INFO
 static void mlx4_en_add_vxlan_port(struct  net_device *dev,
 				   struct udp_tunnel_info *ti)
 {
@@ -3199,13 +3622,64 @@ static void mlx4_en_del_vxlan_port(struc
 
 	queue_work(priv->mdev->workqueue, &priv->vxlan_del_task);
 }
+#else
+static void mlx4_en_add_vxlan_port(struct  net_device *dev,
+				   sa_family_t sa_family, __be16 port)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	__be16 current_port;
+
+	if (priv->mdev->dev->caps.tunnel_offload_mode != MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)
+		return;
+
+	if (sa_family == AF_INET6)
+		return;
+
+	current_port = priv->vxlan_port;
+	if (current_port && current_port != port) {
+		en_warn(priv, "vxlan port %d configured, can't add port %d\n",
+			ntohs(current_port), ntohs(port));
+		return;
+	}
+
+	priv->vxlan_port = port;
+	queue_work(priv->mdev->workqueue, &priv->vxlan_add_task);
+}
+
+static void mlx4_en_del_vxlan_port(struct  net_device *dev,
+				   sa_family_t sa_family, __be16 port)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	__be16 current_port;
+
+	if (priv->mdev->dev->caps.tunnel_offload_mode != MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)
+		return;
+
+	if (sa_family == AF_INET6)
+		return;
+
+	current_port = priv->vxlan_port;
+	if (current_port != port) {
+		en_dbg(DRV, priv, "vxlan port %d isn't configured, ignoring\n", ntohs(port));
+		return;
+	}
+
+	queue_work(priv->mdev->workqueue, &priv->vxlan_del_task);
+}
+#endif
+#endif
 
+#ifdef HAVE_NETDEV_FEATURES_T
 static netdev_features_t mlx4_en_features_check(struct sk_buff *skb,
 						struct net_device *dev,
 						netdev_features_t features)
 {
+#ifdef HAVE_VLAN_FEATURES_CHECK
 	features = vlan_features_check(skb, features);
+#endif
+#ifdef HAVE_VXLAN_FEATURES_CHECK
 	features = vxlan_features_check(skb, features);
+#endif
 
 	/* The ConnectX-3 doesn't support outer IPv6 checksums but it does
 	 * support inner IPv6 checksums and segmentation so  we need to
@@ -3223,7 +3697,16 @@ static netdev_features_t mlx4_en_feature
 
 	return features;
 }
+#else
+#if defined HAVE_VXLAN_GSO_CHECK && ((defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)) || defined(HAVE_UDP_TUNNEL_GET_RX_INFO))
+static bool mlx4_en_gso_check(struct sk_buff *skb, struct net_device *dev)
+{
+	return vxlan_gso_check(skb);
+}
+#endif
+#endif
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 static int mlx4_en_set_tx_maxrate(struct net_device *dev, int queue_index, u32 maxrate)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -3250,7 +3733,9 @@ static int mlx4_en_set_tx_maxrate(struct
 			     &params);
 	return err;
 }
+#endif
 
+#ifdef HAVE_XDP_BUFF
 static int mlx4_xdp_set(struct net_device *dev, struct bpf_prog *prog)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -3354,38 +3839,106 @@ static int mlx4_xdp(struct net_device *d
 		return -EINVAL;
 	}
 }
+#endif
 
 static struct net_device_ops mlx4_netdev_base_ops = {
 	.ndo_open		= mlx4_en_open,
 	.ndo_stop		= mlx4_en_close,
 	.ndo_start_xmit		= mlx4_en_xmit,
 	.ndo_select_queue	= mlx4_en_select_queue,
+#if (defined(HAVE_NDO_GET_STATS64) && defined(HAVE_NETDEV_STATS_TO_STATS64))
 	.ndo_get_stats64	= mlx4_en_get_stats64,
+#else
+	.ndo_get_stats		= mlx4_en_get_stats,
+#endif
 	.ndo_set_rx_mode	= mlx4_en_set_rx_mode,
 	.ndo_set_mac_address	= mlx4_en_set_mac,
 	.ndo_validate_addr	= eth_validate_addr,
 	.ndo_change_mtu		= mlx4_en_change_mtu,
 	.ndo_do_ioctl		= mlx4_en_ioctl,
 	.ndo_tx_timeout		= mlx4_en_tx_timeout,
+#ifdef HAVE_VLAN_GRO_RECEIVE
+	.ndo_vlan_rx_register	= mlx4_en_vlan_rx_register,
+#endif
 	.ndo_vlan_rx_add_vid	= mlx4_en_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid	= mlx4_en_vlan_rx_kill_vid,
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= mlx4_en_netpoll,
 #endif
+#if (defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_features	= mlx4_en_set_features,
+#endif
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	.ndo_fix_features	= mlx4_en_fix_features,
+#endif
+#ifdef HAVE_NDO_SETUP_TC
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
 	.ndo_setup_tc		= __mlx4_en_setup_tc,
+#else /* HAVE_NDO_SETUP_TC_4_PARAMS */
+	.ndo_setup_tc           = mlx4_en_setup_tc,
+#endif
+#endif
+#ifdef HAVE_NDO_RX_FLOW_STEER
 #ifdef CONFIG_RFS_ACCEL
 	.ndo_rx_flow_steer	= mlx4_en_filter_rfs,
 #endif
+#endif
+#ifdef MLX4_EN_BUSY_POLL
+#ifndef HAVE_NETDEV_EXTENDED_NDO_BUSY_POLL
+	.ndo_busy_poll		= mlx4_en_low_latency_recv,
+#endif
+#endif
+#ifdef HAVE_NETDEV_NDO_GET_PHYS_PORT_ID
 	.ndo_get_phys_port_id	= mlx4_en_get_phys_port_id,
+#endif
+#if (defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)) || defined(HAVE_UDP_TUNNEL_GET_RX_INFO)
+#ifdef HAVE_ADD_VXLAN_PORT_UDP_TUNNEL
 	.ndo_udp_tunnel_add	= mlx4_en_add_vxlan_port,
 	.ndo_udp_tunnel_del	= mlx4_en_del_vxlan_port,
+#else
+	.ndo_add_vxlan_port	= mlx4_en_add_vxlan_port,
+	.ndo_del_vxlan_port	= mlx4_en_del_vxlan_port,
+#endif
+#endif
+#ifdef HAVE_NETDEV_FEATURES_T
 	.ndo_features_check	= mlx4_en_features_check,
+#else
+#if defined HAVE_VXLAN_GSO_CHECK && ((defined(HAVE_VXLAN_ENABLED) && defined(HAVE_VXLAN_DYNAMIC_PORT)) || defined(HAVE_UDP_TUNNEL_GET_RX_INFO))
+	.ndo_gso_check          = mlx4_en_gso_check,
+#endif
+#endif
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	.ndo_set_tx_maxrate	= mlx4_en_set_tx_maxrate,
+#endif
+#ifdef HAVE_XDP_BUFF
 	.ndo_xdp		= mlx4_xdp,
+#endif
+};
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+static const struct net_device_ops_ext mlx4_netdev_ops_ext = {
+	.size		  = sizeof(struct net_device_ops_ext),
+	.ndo_set_features = mlx4_en_set_features,
+#ifdef HAVE_NETDEV_EXT_NDO_GET_PHYS_PORT_ID
+	.ndo_get_phys_port_id = mlx4_en_get_phys_port_id,
+#endif
 };
 
+static const struct net_device_ops_ext mlx4_netdev_ops_master_ext = {
+	.size			= sizeof(struct net_device_ops_ext),
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK
+	.ndo_set_vf_spoofchk	= mlx4_en_set_vf_spoofchk,
+#endif
+#if defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
+	.ndo_set_vf_link_state	= mlx4_en_set_vf_link_state,
+#endif
+#ifdef HAVE_NETDEV_EXT_NDO_GET_PHYS_PORT_ID
+	.ndo_get_phys_port_id	= mlx4_en_get_phys_port_id,
+#endif
+	.ndo_set_features	= mlx4_en_set_features,
+};
+#endif
+
 struct mlx4_en_bond {
 	struct work_struct work;
 	struct mlx4_en_priv *priv;
@@ -3393,6 +3946,7 @@ struct mlx4_en_bond {
 	struct mlx4_port_map port_map;
 };
 
+#ifdef HAVE_NETDEV_BONDING_INFO
 static void mlx4_en_bond_work(struct work_struct *work)
 {
 	struct mlx4_en_bond *bond = container_of(work,
@@ -3559,6 +4113,7 @@ int mlx4_en_netdev_event(struct notifier
 
 	return NOTIFY_DONE;
 }
+#endif
 
 void mlx4_en_update_pfc_stats_bitmap(struct mlx4_dev *dev,
 				     struct mlx4_en_stats_bitmap *stats_bitmap,
@@ -3654,10 +4209,21 @@ static void mlx4_en_set_netdev_ops(struc
 	if (mlx4_is_master(priv->mdev->dev)) {
 		priv->dev_ops.ndo_set_vf_mac = mlx4_en_set_vf_mac;
 		priv->dev_ops.ndo_set_vf_vlan = mlx4_en_set_vf_vlan;
+#ifdef HAVE_TX_RATE_LIMIT
 		priv->dev_ops.ndo_set_vf_rate = mlx4_en_set_vf_rate;
+#elif defined(HAVE_VF_TX_RATE)
+		priv->dev_ops.ndo_set_vf_tx_rate =  mlx4_en_set_vf_tx_rate;
+
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 		priv->dev_ops.ndo_set_vf_spoofchk = mlx4_en_set_vf_spoofchk;
+#endif
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE
 		priv->dev_ops.ndo_set_vf_link_state = mlx4_en_set_vf_link_state;
+#endif
+#ifdef HAVE_NDO_GET_VF_STATS
 		priv->dev_ops.ndo_get_vf_stats = mlx4_en_get_vf_stats;
+#endif
 		priv->dev_ops.ndo_get_vf_config = mlx4_en_get_vf_config;
 		priv->dev_ops.ndo_do_ioctl = mlx4_en_ioctl;
 	}
@@ -3666,11 +4232,27 @@ static void mlx4_en_set_netdev_ops(struc
 		priv->dev_ops.ndo_start_xmit = mlx4_en_vgtp_xmit;
 
 	priv->dev->netdev_ops = &priv->dev_ops;
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+	if (mlx4_is_master(priv->mdev->dev)) {
+		set_netdev_ops_ext(priv->dev, &mlx4_netdev_ops_master_ext);
+	} else {
+		set_netdev_ops_ext(priv->dev, &mlx4_netdev_ops_ext);
+	}
+#endif
 }
 
 int mlx4_en_init_netdev(struct mlx4_en_dev *mdev, int port,
 			struct mlx4_en_port_profile *prof)
 {
+#ifndef HAVE_NETDEV_RSS_KEY_FILL
+	static const __be32 rsskey[MLX4_EN_RSS_KEY_SIZE / sizeof(__be32)] = {
+		cpu_to_be32(0xD181C62C), cpu_to_be32(0xF7F4DB5B),
+		cpu_to_be32(0x1983A2FC), cpu_to_be32(0x943E1ADB),
+		cpu_to_be32(0xD9389E6B), cpu_to_be32(0xD1039C2C),
+		cpu_to_be32(0xA74499AD), cpu_to_be32(0x593D56D9),
+		cpu_to_be32(0xF3253C06), cpu_to_be32(0x2ADC1FFC) };
+#endif
 	struct net_device *dev;
 	struct mlx4_en_priv *priv;
 	int i;
@@ -3693,14 +4275,25 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	}
 	if (udev_dev_port_dev_id == 0) {
 		/* in mode 0 update dev_port */
+#ifdef HAVE_NET_DEVICE_DEV_PORT
 		dev->dev_port = port - 1;
+#elif defined(HAVE_NETDEV_EXTENDED_DEV_PORT)
+		netdev_extended(dev)->dev_port = port - 1;
+#else
+		/* fallback to dev_id when dev_port does not exist */
+		dev->dev_id = port - 1;
+#endif
 	} else if (udev_dev_port_dev_id == 1) {
 		/* in mode 1 update only dev_id */
 		dev->dev_id = port - 1;
 	} else if (udev_dev_port_dev_id == 2) {
 		/* in mode 2 update both of dev_id and dev_port */
 		dev->dev_id = port - 1;
+#ifdef HAVE_NET_DEVICE_DEV_PORT
 		dev->dev_port = port - 1;
+#elif defined(HAVE_NETDEV_EXTENDED_DEV_PORT)
+		netdev_extended(dev)->dev_port = port - 1;
+#endif
 	}
 
 	/*
@@ -3739,7 +4332,11 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	priv->tx_ring_num = prof->tx_ring_num;
 	priv->num_up = prof->num_up;
 	priv->tx_work_limit = MLX4_EN_DEFAULT_TX_WORK;
+#ifdef HAVE_NETDEV_RSS_KEY_FILL
 	netdev_rss_key_fill(priv->rss_key, sizeof(priv->rss_key));
+#else
+	memcpy(priv->rss_key, rsskey, sizeof(priv->rss_key));
+#endif
 
 	priv->tx_ring = kzalloc(sizeof(struct mlx4_en_tx_ring *) * MAX_TX_RINGS,
 				GFP_KERNEL);
@@ -3758,6 +4355,7 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	priv->cqe_size = mdev->dev->caps.cqe_size;
 	priv->mac_index = -1;
 	priv->msg_enable = MLX4_EN_MSG_LEVEL;
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 	if (!mlx4_is_slave(priv->mdev->dev)) {
 		u8 config = 0;
@@ -3794,6 +4392,7 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 		}
 	}
 #endif
+#endif
 
 	for (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i)
 		INIT_HLIST_HEAD(&priv->mac_hash[i]);
@@ -3876,24 +4475,41 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	netif_set_real_num_tx_queues(dev, priv->tx_ring_num);
 	netif_set_real_num_rx_queues(dev, priv->rx_ring_num);
 
+#ifdef HAVE_ETHTOOL_OPS_EXT
+	SET_ETHTOOL_OPS(dev, &mlx4_en_ethtool_ops);
+	set_ethtool_ops_ext(dev, &mlx4_en_ethtool_ops_ext);
+#else
 	dev->ethtool_ops = &mlx4_en_ethtool_ops;
+#endif
+
+#ifdef MLX4_EN_BUSY_POLL
+#ifdef HAVE_NETDEV_EXTENDED_NDO_BUSY_POLL
+	netdev_extended(dev)->ndo_busy_poll = mlx4_en_low_latency_recv;
+#endif
+#endif
 
 	/*
 	 * Set driver features
 	 */
+#ifdef HAVE_NETDEV_HW_FEATURES
 	dev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
 	if (mdev->LSO_support)
 		dev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;
 
 	dev->vlan_features = dev->hw_features;
 
+#ifdef HAVE_NETIF_F_RXHASH
 	dev->hw_features |= NETIF_F_RXCSUM | NETIF_F_RXHASH;
+#else
+	dev->hw_features |= NETIF_F_RXCSUM;
+#endif
 	dev->features = dev->hw_features | NETIF_F_HIGHDMA |
 			NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |
 			NETIF_F_HW_VLAN_CTAG_FILTER;
 	dev->hw_features |= NETIF_F_LOOPBACK |
 			NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	if (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_SKIP_OUTER_VLAN)) {
 		dev->features |= NETIF_F_HW_VLAN_STAG_RX |
 			NETIF_F_HW_VLAN_STAG_FILTER;
@@ -3921,54 +4537,148 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 					   NETIF_F_HW_VLAN_STAG_TX |
 					   NETIF_F_HW_VLAN_STAG_RX);
 		}
+
 	} else {
 		if (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_PHV_EN &&
 		    !(mdev->dev->caps.flags2 &
 		      MLX4_DEV_CAP_FLAG2_SKIP_OUTER_VLAN))
 			dev->hw_features |= NETIF_F_HW_VLAN_STAG_TX;
 	}
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP)
 		dev->hw_features |= NETIF_F_RXFCS;
+#endif
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_IGNORE_FCS)
 		dev->hw_features |= NETIF_F_RXALL;
+#endif
 
 	if (mdev->dev->caps.steering_mode ==
 	    MLX4_STEERING_MODE_DEVICE_MANAGED &&
 	    mdev->dev->caps.dmfs_high_steer_mode != MLX4_STEERING_DMFS_A0_STATIC)
 		dev->hw_features |= NETIF_F_NTUPLE;
 
+#ifndef NETIF_F_SOFT_FEATURES
+	dev->hw_features |= NETIF_F_GSO | NETIF_F_GRO;
+	dev->features |= NETIF_F_GSO | NETIF_F_GRO;
+#endif
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	dev->hw_features |= NETIF_F_LRO;
+	dev->features |= NETIF_F_LRO;
+#endif
+#else
+	dev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+
+	if (mdev->LSO_support)
+		dev->features |= NETIF_F_TSO | NETIF_F_TSO6;
+
+	dev->vlan_features = dev->features;
+
+#ifdef HAVE_NETIF_F_RXHASH
+	dev->features |= NETIF_F_RXCSUM | NETIF_F_RXHASH;
+#else
+	dev->features |= NETIF_F_RXCSUM;
+#endif
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	dev->features |= NETIF_F_LRO;
+#endif
+#ifdef HAVE_SET_NETDEV_HW_FEATURES
+	set_netdev_hw_features(dev, dev->features);
+#endif
+	dev->features = dev->features | NETIF_F_HIGHDMA |
+			NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |
+			NETIF_F_HW_VLAN_CTAG_FILTER;
+#ifdef HAVE_NETDEV_EXTENDED_HW_FEATURES
+	netdev_extended(dev)->hw_features |= NETIF_F_LOOPBACK;
+	netdev_extended(dev)->hw_features |= NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_CTAG_TX;
+#endif
+
+	if (mdev->dev->caps.steering_mode ==
+		MLX4_STEERING_MODE_DEVICE_MANAGED)
+#ifdef HAVE_NETDEV_EXTENDED_HW_FEATURES
+		netdev_extended(dev)->hw_features |= NETIF_F_NTUPLE;
+#else
+		dev->features |= NETIF_F_NTUPLE;
+#endif
+
+#ifndef NETIF_F_SOFT_FEATURES
+	dev->features |= NETIF_F_GSO | NETIF_F_GRO;
+#endif
+#endif
+
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	if (mdev->dev->caps.steering_mode != MLX4_STEERING_MODE_A0)
 		dev->priv_flags |= IFF_UNICAST_FLT;
+#endif
 
 	/* Setting a default hash function value */
 	if (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_RSS_TOP) {
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		priv->rss_hash_fn = ETH_RSS_HASH_TOP;
+#else
+		priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+		dev->features |= NETIF_F_RXHASH;
+#endif
+#endif
 	} else if (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_RSS_XOR) {
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		priv->rss_hash_fn = ETH_RSS_HASH_XOR;
+#else
+		priv->pflags |= MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+		dev->features &= ~NETIF_F_RXHASH;
+#endif
+#endif
 	} else {
 		en_warn(priv,
 			"No RSS hash capabilities exposed, using Toeplitz\n");
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		priv->rss_hash_fn = ETH_RSS_HASH_TOP;
+#else
+		priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+		dev->features |= NETIF_F_RXHASH;
+#endif
+#endif
 	}
 
 	if (mdev->dev->caps.tunnel_offload_mode == MLX4_TUNNEL_OFFLOAD_MODE_VXLAN) {
+#ifdef HAVE_NETDEV_HW_FEATURES
 		dev->hw_features |= NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 				    NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 				    NETIF_F_GSO_PARTIAL;
+#else
+				    0;
+#endif
+#endif
 		dev->features    |= NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 				    NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 				    NETIF_F_GSO_PARTIAL;
 		dev->gso_partial_features = NETIF_F_GSO_UDP_TUNNEL_CSUM;
+#else
+				    0;
+#endif
 	}
 
 	mdev->pndev[port] = dev;
 	mdev->upper[port] = NULL;
 
+#ifdef HAVE_NET_DEVICE_MIN_MAX_MTU
 	/* MTU range: 46 - hw-specific max */
 	dev->min_mtu = MLX4_EN_MIN_MTU;
 	dev->max_mtu = priv->max_mtu;
+#endif
 
 	netif_carrier_off(dev);
 	mlx4_en_set_default_moderation(priv);
@@ -4037,8 +4747,31 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	}
 
 	priv->registered = 1;
+
+#ifdef CONFIG_SYSFS_FDB
+	if (mlx4_is_mfunc(priv->mdev->dev)) {
+		err = device_create_file(&dev->dev, &dev_attr_fdb);
+		if (err) {
+			en_err(priv, "Sysfs registration failed for port %d\n",
+			       port);
+			goto out;
+		}
+		err = device_create_file(&dev->dev, &dev_attr_fdb_det);
+		if (err) {
+			en_err(priv,
+			       "Sysfs (fdb_det) registration failed port %d\n",
+			       port);
+			device_remove_file(&dev->dev, &dev_attr_fdb);
+			goto out;
+		}
+	}
+	priv->sysfs_fdb_created = 1;
+#endif
+
+#ifdef HAVE_DEVLINK_H
 	devlink_port_type_eth_set(mlx4_get_devlink_port(mdev->dev, priv->port),
 				  dev);
+#endif
 
 	if (mlx4_is_master(priv->mdev->dev)) {
 		for (i = 0; i < priv->mdev->dev->persist->num_vfs; i++) {
@@ -4072,6 +4805,13 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 		}
 	}
 
+#ifdef CONFIG_COMPAT_EN_SYSFS
+	err = mlx4_en_sysfs_create(dev);
+	if (err)
+		goto out;
+	priv->sysfs_group_initialized = 1;
+#endif
+
 	return 0;
 
 out:
@@ -4092,8 +4832,12 @@ int mlx4_en_reset_config(struct net_devi
 
 	if (priv->hwtstamp_config.tx_type == ts_config.tx_type &&
 	    priv->hwtstamp_config.rx_filter == ts_config.rx_filter &&
+#ifdef HAVE_NETIF_F_RXFCS
 	    !DEV_FEATURE_CHANGED(dev, features, NETIF_F_HW_VLAN_CTAG_RX) &&
 	    !DEV_FEATURE_CHANGED(dev, features, NETIF_F_RXFCS))
+#else
+	    !DEV_FEATURE_CHANGED(dev, features, NETIF_F_HW_VLAN_CTAG_RX))
+#endif
 		return 0; /* Nothing to change */
 
 	if (DEV_FEATURE_CHANGED(dev, features, NETIF_F_HW_VLAN_CTAG_RX) &&
@@ -4136,18 +4880,26 @@ int mlx4_en_reset_config(struct net_devi
 		/* RX time-stamping is OFF, update the RX vlan offload
 		 * to the latest wanted state
 		 */
+#if defined(HAVE_NETDEV_WANTED_FEATURES) || defined(HAVE_NETDEV_EXTENDED_WANTED_FEATURES)
+#ifdef HAVE_NETDEV_WANTED_FEATURES
 		if (dev->wanted_features & NETIF_F_HW_VLAN_CTAG_RX)
+#else
+		if (netdev_extended(dev)->wanted_features & NETIF_F_HW_VLAN_CTAG_RX)
+#endif
 			dev->features |= NETIF_F_HW_VLAN_CTAG_RX;
 		else
 			dev->features &= ~NETIF_F_HW_VLAN_CTAG_RX;
+#endif
 	}
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (DEV_FEATURE_CHANGED(dev, features, NETIF_F_RXFCS)) {
 		if (features & NETIF_F_RXFCS)
 			dev->features |= NETIF_F_RXFCS;
 		else
 			dev->features &= ~NETIF_F_RXFCS;
 	}
+#endif
 
 	/* RX vlan offload and RX time-stamping can't co-exist !
 	 * Regardless of the caller's choice,
--- a/drivers/net/ethernet/mellanox/mlx4/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
@@ -31,8 +31,9 @@
  *
  */
 
-#include <net/busy_poll.h>
+#ifdef HAVE_XDP_BUFF
 #include <linux/bpf.h>
+#endif
 #include <linux/mlx4/cq.h>
 #include <linux/slab.h>
 #include <linux/mlx4/qp.h>
@@ -49,6 +50,10 @@
 
 #include "mlx4_en.h"
 
+#ifdef MLX4_EN_BUSY_POLL
+#include <net/busy_poll.h>
+#endif
+
 static int mlx4_alloc_pages(struct mlx4_en_priv *priv,
 			    struct mlx4_en_rx_alloc *page_alloc,
 			    const struct mlx4_en_frag_info *frag_info,
@@ -83,7 +88,12 @@ static int mlx4_alloc_pages(struct mlx4_
 	/* Not doing get_page() for each frag is a big win
 	 * on asymetric workloads. Note we can not use atomic_set().
 	 */
+#ifdef HAVE_MM_PAGE__COUNT
+	atomic_add(page_alloc->page_size / frag_info->frag_stride - 1,
+		   &page->_count);
+#else
 	page_ref_add(page, page_alloc->page_size / frag_info->frag_stride - 1);
+#endif
 	return 0;
 }
 
@@ -130,8 +140,12 @@ out:
 				priv->frag_info[i].dma_dir);
 			page = page_alloc[i].page;
 			/* Revert changes done by mlx4_alloc_pages */
+#ifdef HAVE_MM_PAGE__COUNT
+			atomic_set(&page->_count, 1);
+#else
 			page_ref_sub(page, page_alloc[i].page_size /
 					   priv->frag_info[i].frag_stride - 1);
+#endif
 			put_page(page);
 		}
 	}
@@ -169,7 +183,11 @@ static int mlx4_en_init_allocator(struct
 
 		en_dbg(DRV, priv, "  frag %d allocator: - size:%d frags:%d\n",
 		       i, ring->page_alloc[i].page_size,
+#ifdef HAVE_MM_PAGE__COUNT
+		       atomic_read(&ring->page_alloc[i].page->_count));
+#else
 		       page_ref_count(ring->page_alloc[i].page));
+#endif
 	}
 	return 0;
 
@@ -183,8 +201,12 @@ out:
 			       priv->frag_info[i].dma_dir);
 		page = page_alloc->page;
 		/* Revert changes done by mlx4_alloc_pages */
+#ifdef HAVE_MM_PAGE__COUNT
+		atomic_set(&page->_count, 1);
+#else
 		page_ref_sub(page, page_alloc->page_size /
 				   priv->frag_info[i].frag_stride - 1);
+#endif
 		put_page(page);
 		page_alloc->page = NULL;
 	}
@@ -281,6 +303,31 @@ static void mlx4_en_free_rx_desc(struct
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+static inline int mlx4_en_can_lro(__be16 status)
+{
+	static __be16 status_ipv4_ipok_tcp;
+	static __be16 status_all;
+
+	status_all		= cpu_to_be16(
+					MLX4_CQE_STATUS_IPV4    |
+					MLX4_CQE_STATUS_IPV4F   |
+					MLX4_CQE_STATUS_IPV6    |
+					MLX4_CQE_STATUS_IPV4OPT |
+					MLX4_CQE_STATUS_TCP     |
+					MLX4_CQE_STATUS_UDP     |
+					MLX4_CQE_STATUS_IPOK);
+
+	status_ipv4_ipok_tcp	= cpu_to_be16(
+					MLX4_CQE_STATUS_IPV4    |
+					MLX4_CQE_STATUS_IPOK    |
+					MLX4_CQE_STATUS_TCP);
+
+	status &= status_all;
+	return status == status_ipv4_ipok_tcp;
+}
+#endif
+
 static int mlx4_en_fill_rx_buffers(struct mlx4_en_priv *priv)
 {
 	struct mlx4_en_rx_ring *ring;
@@ -362,6 +409,42 @@ void mlx4_en_set_num_rx_rings(struct mlx
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+static int mlx4_en_get_frag_hdr(struct skb_frag_struct *frags, void **mac_hdr,
+				void **ip_hdr, void **tcpudp_hdr,
+				u64 *hdr_flags, void *priv)
+{
+	*mac_hdr = page_address(skb_frag_page(frags)) + frags->page_offset;
+	*ip_hdr = *mac_hdr + ETH_HLEN;
+	*tcpudp_hdr = (struct tcphdr *)(*ip_hdr + sizeof(struct iphdr));
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+
+	return 0;
+}
+
+static void mlx4_en_lro_init(struct mlx4_en_rx_ring *ring,
+			     struct mlx4_en_priv *priv)
+{
+	/* Commit 9d4dde5215779 reduced SKB's frags array to 17 from 18.
+	 * The lro_receive_frags routine aggregates priv->num_frags to this
+	 * array and only then check that total number of frags did not
+	 * passed the max_aggr, so need to align max_aggr to a multiple of
+	 * priv->num_frags, in order for LRO to avoid overflow.
+	 */
+	ring->lro.lro_mgr.max_aggr =
+		MAX_SKB_FRAGS - (MAX_SKB_FRAGS % priv->num_frags);
+
+	ring->lro.lro_mgr.max_desc		= MLX4_EN_LRO_MAX_DESC;
+	ring->lro.lro_mgr.lro_arr		= ring->lro.lro_desc;
+	ring->lro.lro_mgr.get_frag_header	= mlx4_en_get_frag_hdr;
+	ring->lro.lro_mgr.features		= LRO_F_NAPI;
+	ring->lro.lro_mgr.frag_align_pad	= NET_IP_ALIGN;
+	ring->lro.lro_mgr.dev			= priv->dev;
+	ring->lro.lro_mgr.ip_summed		= CHECKSUM_UNNECESSARY;
+	ring->lro.lro_mgr.ip_summed_aggr	= CHECKSUM_UNNECESSARY;
+}
+#endif
+
 int mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,
 			   struct mlx4_en_rx_ring **pring,
 			   u32 size, u16 stride, int node)
@@ -474,6 +557,9 @@ int mlx4_en_activate_rx_rings(struct mlx
 			ring_ind--;
 			goto err_allocator;
 		}
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+		mlx4_en_lro_init(ring, priv);
+#endif
 	}
 	err = mlx4_en_fill_rx_buffers(priv);
 	if (err)
@@ -544,6 +630,7 @@ void mlx4_en_destroy_rx_ring(struct mlx4
 {
 	struct mlx4_en_dev *mdev = priv->mdev;
 	struct mlx4_en_rx_ring *ring = *pring;
+#ifdef HAVE_XDP_BUFF
 	struct bpf_prog *old_prog;
 
 	old_prog = rcu_dereference_protected(
@@ -551,6 +638,7 @@ void mlx4_en_destroy_rx_ring(struct mlx4
 					lockdep_is_held(&mdev->state_lock));
 	if (old_prog)
 		bpf_prog_put(old_prog);
+#endif
 	mlx4_free_hwq_res(mdev->dev, &ring->wqres, size * stride + TXBB_SIZE);
 	vfree(ring->rx_info);
 	ring->rx_info = NULL;
@@ -581,10 +669,10 @@ void mlx4_en_deactivate_rx_ring(struct m
 static int mlx4_en_complete_rx_desc(struct mlx4_en_priv *priv,
 				    struct mlx4_en_rx_desc *rx_desc,
 				    struct mlx4_en_rx_alloc *frags,
-				    struct sk_buff *skb,
-				    int length)
+				    struct skb_frag_struct *skb_frags_rx,
+				    int length,
+				    int *truesize)
 {
-	struct skb_frag_struct *skb_frags_rx = skb_shinfo(skb)->frags;
 	struct mlx4_en_frag_info *frag_info;
 	int nr;
 	dma_addr_t dma;
@@ -605,7 +693,7 @@ static int mlx4_en_complete_rx_desc(stru
 		__skb_frag_set_page(&skb_frags_rx[nr], frags[nr].page);
 		skb_frag_size_set(&skb_frags_rx[nr], frag_info->frag_size);
 		skb_frags_rx[nr].page_offset = frags[nr].page_offset;
-		skb->truesize += frag_info->frag_stride;
+		*truesize += frag_info->frag_stride;
 		frags[nr].page = NULL;
 	}
 	/* Adjust size of last fragment to match actual length */
@@ -658,14 +746,19 @@ static struct sk_buff *mlx4_en_rx_skb(st
 
 		/* Move relevant fragments to skb */
 		used_frags = mlx4_en_complete_rx_desc(priv, rx_desc, frags,
-							skb, length);
+						      skb_shinfo(skb)->frags,
+						      length, &skb->truesize);
 		if (unlikely(!used_frags)) {
 			kfree_skb(skb);
 			return NULL;
 		}
 		skb_shinfo(skb)->nr_frags = used_frags;
 
+#ifdef HAVE_ETH_GET_HEADLEN
 		pull_len = eth_get_headlen(va, SMALL_PACKET_SIZE);
+#else
+		pull_len = HEADER_COPY_SIZE;
+#endif
 		/* Copy headers into the skb linear buffer */
 		memcpy(skb->data, va, pull_len);
 		skb->tail += pull_len;
@@ -821,10 +914,14 @@ int mlx4_en_process_rx_cq(struct net_dev
 	struct mlx4_en_rx_ring *ring = priv->rx_ring[cq->ring];
 	struct mlx4_en_rx_alloc *frags;
 	struct mlx4_en_rx_desc *rx_desc;
+#ifdef HAVE_XDP_BUFF
 	struct bpf_prog *xdp_prog;
 	int doorbell_pending;
+#endif
 	struct sk_buff *skb;
+#ifdef HAVE_XDP_BUFF
 	int tx_index;
+#endif
 	int index;
 	int nr;
 	unsigned int length;
@@ -832,7 +929,9 @@ int mlx4_en_process_rx_cq(struct net_dev
 	int ip_summed;
 	int factor = priv->cqe_factor;
 	u64 timestamp;
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 	bool l2_tunnel;
+#endif
 
 	if (unlikely(!priv->port_up))
 		return 0;
@@ -840,11 +939,13 @@ int mlx4_en_process_rx_cq(struct net_dev
 	if (unlikely(budget <= 0))
 		return polled;
 
+#ifdef HAVE_XDP_BUFF
 	/* Protect accesses to: ring->xdp_prog, priv->mac_hash list */
 	rcu_read_lock();
 	xdp_prog = rcu_dereference(ring->xdp_prog);
 	doorbell_pending = 0;
 	tx_index = (priv->tx_ring_num - priv->xdp_ring_num) + cq->ring;
+#endif
 
 	/* We assume a 1:1 mapping between CQEs and Rx descriptors, so Rx
 	 * descriptor offset can be deduced from the CQE index instead of
@@ -862,7 +963,11 @@ int mlx4_en_process_rx_cq(struct net_dev
 		/*
 		 * make sure we read the CQE after we read the ownership bit
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		/* Drop packet on bad receive or bad checksum */
 		if (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==
@@ -890,6 +995,7 @@ int mlx4_en_process_rx_cq(struct net_dev
 		if (priv->flags & MLX4_EN_FLAG_RX_FILTER_NEEDED) {
 			struct ethhdr *ethh;
 			dma_addr_t dma;
+			COMPAT_HL_NODE
 			/* Get pointer to first fragment since we haven't
 			 * skb yet and cast it to ethhdr struct
 			 */
@@ -907,7 +1013,7 @@ int mlx4_en_process_rx_cq(struct net_dev
 				/* Drop the packet, since HW loopback-ed it */
 				mac_hash = ethh->h_source[MLX4_EN_MAC_HASH_IDX];
 				bucket = &priv->mac_hash[mac_hash];
-				hlist_for_each_entry_rcu(entry, bucket, hlist) {
+				compat_hlist_for_each_entry_rcu(entry, bucket, hlist) {
 					if (ether_addr_equal_64bits(entry->mac,
 								    ethh->h_source))
 						goto next;
@@ -920,9 +1026,12 @@ int mlx4_en_process_rx_cq(struct net_dev
 		 */
 		ring->bytes += length;
 		ring->packets++;
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 		l2_tunnel = (dev->hw_enc_features & NETIF_F_RXCSUM) &&
 			(cqe->vlan_my_qpn & cpu_to_be32(MLX4_CQE_L2_TUNNEL));
+#endif
 
+#ifdef HAVE_XDP_BUFF
 		/* A bpf program gets first chance to drop the packet. It may
 		 * read bytes but not past the end of the frag.
 		 */
@@ -960,6 +1069,7 @@ xdp_drop:
 				goto next;
 			}
 		}
+#endif
 
 		if (likely(dev->features & NETIF_F_RXCSUM)) {
 			if (cqe->status & cpu_to_be16(MLX4_CQE_STATUS_TCP |
@@ -968,6 +1078,33 @@ xdp_drop:
 				    cqe->checksum == cpu_to_be16(0xffff)) {
 					ip_summed = CHECKSUM_UNNECESSARY;
 					ring->csum_ok++;
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+					/* traffic eligible for LRO */
+					if ((dev->features & NETIF_F_LRO) &&
+					    mlx4_en_can_lro(cqe->status) &&
+					    (ring->hwtstamp_rx_filter ==
+					     HWTSTAMP_FILTER_NONE) &&
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
+					    !l2_tunnel &&
+#endif
+					    !(be32_to_cpu(cqe->vlan_my_qpn) &
+					      (MLX4_CQE_CVLAN_PRESENT_MASK |
+					       MLX4_CQE_SVLAN_PRESENT_MASK))) {
+						int truesize = 0;
+						struct skb_frag_struct lro_frag[MLX4_EN_MAX_RX_FRAGS];
+
+						nr = mlx4_en_complete_rx_desc(priv, rx_desc, frags,
+									      lro_frag, length, &truesize);
+
+						if (unlikely(!nr))
+							goto next;
+
+						/* Push it up the stack (LRO) */
+						lro_receive_frags(&ring->lro.lro_mgr, lro_frag,
+								  length, truesize, NULL, 0);
+						goto next;
+					}
+#endif
 				} else {
 					ip_summed = CHECKSUM_NONE;
 					ring->csum_none++;
@@ -992,17 +1129,26 @@ xdp_drop:
 		 * - DIX Ethernet (type interpretation)
 		 * - TCP/IP (v4)
 		 * - without IP options
+		 * - no LLS polling in progress
 		 * - not an IP fragment
 		 */
 		if ((!(cqe->status & cpu_to_be16(MLX4_CQE_STATUS_UDP))) &&
+		    !mlx4_en_cq_busy_polling(cq) &&
+#ifndef HAVE_VLAN_GRO_RECEIVE
 		    (dev->features & NETIF_F_GRO) && (ip_summed != CHECKSUM_NONE)) {
+#else
+		    (dev->features & NETIF_F_GRO) &&
+		    (ip_summed != CHECKSUM_NONE) &&
+		    (!(be32_to_cpu(cqe->vlan_my_qpn) &
+		     MLX4_CQE_CVLAN_PRESENT_MASK))) {
+#endif
 			struct sk_buff *gro_skb = napi_get_frags(&cq->napi);
 			if (!gro_skb)
 				goto next;
 
 			nr = mlx4_en_complete_rx_desc(priv,
-				rx_desc, frags, gro_skb,
-				length);
+				rx_desc, frags, skb_shinfo(gro_skb)->frags,
+				length, &gro_skb->truesize);
 			if (!nr)
 				goto next;
 
@@ -1021,29 +1167,51 @@ xdp_drop:
 			gro_skb->data_len = length;
 			gro_skb->ip_summed = ip_summed;
 
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
+#ifdef HAVE_SK_BUFF_CSUM_LEVEL
 			if (l2_tunnel && ip_summed == CHECKSUM_UNNECESSARY)
 				gro_skb->csum_level = 1;
+#else
+			if (l2_tunnel)
+				gro_skb->encapsulation = 1;
+#endif
+#endif
 
 			if ((cqe->vlan_my_qpn &
 			    cpu_to_be32(MLX4_CQE_CVLAN_PRESENT_MASK)) &&
 			    (dev->features & NETIF_F_HW_VLAN_CTAG_RX)) {
 				u16 vid = be16_to_cpu(cqe->sl_vid);
 
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 				__vlan_hwaccel_put_tag(gro_skb, htons(ETH_P_8021Q), vid);
+#else
+				__vlan_hwaccel_put_tag(gro_skb, vid);
+#endif
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 			} else if ((be32_to_cpu(cqe->vlan_my_qpn) &
 				  MLX4_CQE_SVLAN_PRESENT_MASK) &&
 				 (dev->features & NETIF_F_HW_VLAN_STAG_RX)) {
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 				__vlan_hwaccel_put_tag(gro_skb,
 						       htons(ETH_P_8021AD),
 						       be16_to_cpu(cqe->sl_vid));
+#else
+				__vlan_hwaccel_put_tag(gro_skb,
+						       be16_to_cpu(cqe->sl_vid));
+#endif
+#endif
 			}
 
 			if (dev->features & NETIF_F_RXHASH)
+#ifdef HAVE_SKB_SET_HASH
 				skb_set_hash(gro_skb,
 					     be32_to_cpu(cqe->immed_rss_invalid),
 					     (ip_summed == CHECKSUM_UNNECESSARY) ?
 						PKT_HASH_TYPE_L4 :
 						PKT_HASH_TYPE_L3);
+#else
+				gro_skb->rxhash = be32_to_cpu(cqe->immed_rss_invalid);
+#endif
 
 			skb_record_rx_queue(gro_skb, cq->ring);
 
@@ -1082,25 +1250,58 @@ xdp_drop:
 		skb->protocol = eth_type_trans(skb, dev);
 		skb_record_rx_queue(skb, cq->ring);
 
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
+#ifdef HAVE_SK_BUFF_CSUM_LEVEL
 		if (l2_tunnel && ip_summed == CHECKSUM_UNNECESSARY)
 			skb->csum_level = 1;
+#else
+		if (l2_tunnel)
+			skb->encapsulation = 1;
+#endif
+#endif
 
 		if (dev->features & NETIF_F_RXHASH)
+#ifdef HAVE_SKB_SET_HASH
 			skb_set_hash(skb,
 				     be32_to_cpu(cqe->immed_rss_invalid),
 				     (ip_summed == CHECKSUM_UNNECESSARY) ?
 					PKT_HASH_TYPE_L4 :
 					PKT_HASH_TYPE_L3);
+#else
+			skb->rxhash = be32_to_cpu(cqe->immed_rss_invalid);
+#endif
 
 		if ((be32_to_cpu(cqe->vlan_my_qpn) &
 		    MLX4_CQE_CVLAN_PRESENT_MASK) &&
 		    (dev->features & NETIF_F_HW_VLAN_CTAG_RX))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+		{
+			if (priv->vlgrp) {
+				vlan_gro_receive(&cq->napi, priv->vlgrp,
+						 be16_to_cpu(cqe->sl_vid),
+						 skb);
+				goto next;
+			}
+#endif
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 			__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), be16_to_cpu(cqe->sl_vid));
+#else
+			__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->sl_vid));
+#endif
+#ifdef HAVE_VLAN_GRO_RECEIVE
+		}
+#endif
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		else if ((be32_to_cpu(cqe->vlan_my_qpn) &
 			  MLX4_CQE_SVLAN_PRESENT_MASK) &&
 			 (dev->features & NETIF_F_HW_VLAN_STAG_RX))
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 			__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021AD),
 					       be16_to_cpu(cqe->sl_vid));
+#else
+			__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->sl_vid));
+#endif
+#endif
 
 		if (ring->hwtstamp_rx_filter == HWTSTAMP_FILTER_ALL) {
 			timestamp = mlx4_en_get_cqe_ts(cqe);
@@ -1113,7 +1314,9 @@ next:
 		for (nr = 0; nr < priv->num_frags; nr++)
 			mlx4_en_free_frag(priv, frags, nr);
 
+#ifdef HAVE_XDP_BUFF
 consumed:
+#endif
 		++cq->mcq.cons_index;
 		index = (cq->mcq.cons_index) & ring->size_mask;
 		cqe = mlx4_en_get_cqe(cq->buf, index, priv->cqe_size) + factor;
@@ -1122,11 +1325,17 @@ consumed:
 	}
 
 out:
+#ifdef HAVE_XDP_BUFF
 	rcu_read_unlock();
 	if (doorbell_pending)
 		mlx4_en_xmit_doorbell(priv->tx_ring[tx_index]);
+#endif
 
 	AVG_PERF_COUNTER(priv->pstats.rx_coal_avg, polled);
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	if (dev->features & NETIF_F_LRO)
+		lro_flush_all(&priv->rx_ring[cq->ring]->lro.lro_mgr);
+#endif
 	mlx4_cq_set_ci(&cq->mcq);
 	wmb(); /* ensure HW sees CQ consumer before we post new buffers */
 	ring->cons = cq->mcq.cons_index;
@@ -1155,19 +1364,36 @@ int mlx4_en_poll_rx_cq(struct napi_struc
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	int done;
 
+	if (!mlx4_en_cq_lock_napi(cq))
+		return budget;
+
 	done = mlx4_en_process_rx_cq(dev, cq, budget);
 
+	mlx4_en_cq_unlock_napi(cq);
+
 	/* If we used up all the quota - we're probably not done yet... */
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	cq->tot_rx += done;
+#endif
 	if (done == budget) {
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 		const struct cpumask *aff;
+#ifndef HAVE_IRQ_DATA_AFFINITY
 		struct irq_data *idata;
+#endif
 		int cpu_curr;
+#endif
 
 		INC_PERF_COUNTER(priv->pstats.napi_quota);
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 		cpu_curr = smp_processor_id();
+#ifndef HAVE_IRQ_DATA_AFFINITY
 		idata = irq_desc_get_irq_data(cq->irq_desc);
 		aff = irq_data_get_affinity_mask(idata);
+#else
+		aff = irq_desc_get_irq_data(cq->irq_desc)->affinity;
+#endif
 
 		if (likely(cpumask_test_cpu(cpu_curr, aff)))
 			return budget;
@@ -1177,9 +1403,22 @@ int mlx4_en_poll_rx_cq(struct napi_struc
 		 * poll, and restart it on the right CPU
 		 */
 		done = 0;
+#else
+		if (cq->tot_rx < MLX4_EN_MIN_RX_ARM)
+			return budget;
+
+		cq->tot_rx = 0;
+		done = 0;
+	} else {
+		cq->tot_rx = 0;
+#endif
 	}
 	/* Done for now */
+#ifdef HAVE_NAPI_COMPLETE_DONE
 	napi_complete_done(napi, done);
+#else
+	napi_complete(napi);
+#endif
 	mlx4_en_arm_cq(priv, cq);
 	return done;
 }
@@ -1201,6 +1440,7 @@ void mlx4_en_calc_rx_buf(struct net_devi
 	int buf_size = 0;
 	int i = 0;
 
+#ifdef HAVE_XDP_BUFF
 	/* bpf requires buffers to be set up as 1 packet per page.
 	 * This only works when num_frags == 1.
 	 */
@@ -1212,6 +1452,7 @@ void mlx4_en_calc_rx_buf(struct net_devi
 		align = PAGE_SIZE;
 		order = 0;
 	}
+#endif
 
 	while (buf_size < eff_mtu) {
 		priv->frag_info[i].order = order;
@@ -1272,7 +1513,11 @@ static int mlx4_en_config_rss_qp(struct
 	/* Cancel FCS removal if FW allows */
 	if (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP) {
 		context->param3 |= cpu_to_be32(1 << 29);
+#ifdef HAVE_NETIF_F_RXFCS
 		if (priv->dev->features & NETIF_F_RXFCS)
+#else
+		if (priv->pflags & MLX4_EN_PRIV_FLAGS_RXFCS)
+#endif
 			ring->fcs_del = 0;
 		else
 			ring->fcs_del = ETH_FCS_LEN;
@@ -1389,9 +1634,17 @@ int mlx4_en_config_rss_steer(struct mlx4
 
 	rss_context->flags = rss_mask;
 	rss_context->hash_fn = MLX4_RSS_HASH_TOP;
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (priv->rss_hash_fn == ETH_RSS_HASH_XOR) {
+#else
+	if (priv->pflags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR) {
+#endif
 		rss_context->hash_fn = MLX4_RSS_HASH_XOR;
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	} else if (priv->rss_hash_fn == ETH_RSS_HASH_TOP) {
+#else
+	} else if (!(priv->pflags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR)) {
+#endif
 		rss_context->hash_fn = MLX4_RSS_HASH_TOP;
 		memcpy(rss_context->rss_key, priv->rss_key,
 		       MLX4_EN_RSS_KEY_SIZE);
--- a/drivers/net/ethernet/mellanox/mlx4/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
@@ -340,7 +340,11 @@ u32 mlx4_en_free_tx_desc(struct mlx4_en_
 			}
 		}
 	}
+#ifdef HAVE_NAPI_CONSUME_SKB
 	napi_consume_skb(skb, napi_mode);
+#else
+	dev_kfree_skb(skb);
+#endif
 
 	return tx_info->nr_txbb;
 }
@@ -426,7 +430,13 @@ static bool _mlx4_en_process_tx_cq(struc
 	if (!priv->port_up)
 		return true;
 
+#ifdef HAVE_NETDEV_TXQ_BQL_PREFETCHW
 	netdev_txq_bql_complete_prefetchw(ring->tx_queue);
+#else
+#ifdef CONFIG_BQL
+	prefetchw(&ring->tx_queue->dql.limit);
+#endif
+#endif
 
 	index = cons_index & size_mask;
 	cqe = mlx4_en_get_cqe(buf, index, priv->cqe_size) + factor;
@@ -442,7 +452,11 @@ static bool _mlx4_en_process_tx_cq(struc
 		 * make sure we read the CQE after we read the
 		 * ownership bit
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		if (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==
 			     MLX4_CQE_OPCODE_ERROR)) {
@@ -641,9 +655,11 @@ static int get_real_size(const struct sk
 
 	if (shinfo->gso_size) {
 		*inline_ok = false;
+#ifdef HAVE_SKB_INNER_TRANSPORT_HEADER
 		if (skb->encapsulation)
 			*lso_header_size = (skb_inner_transport_header(skb) - skb->data) + inner_tcp_hdrlen(skb);
 		else
+#endif
 			*lso_header_size = skb_transport_offset(skb) + tcp_hdrlen(skb);
 		real_size = CTRL_SIZE + shinfo->nr_frags * DS_SIZE +
 			ALIGN(*lso_header_size + 4, DS_SIZE);
@@ -717,13 +733,25 @@ static void build_inline_wqe(struct mlx4
 				       skb_frag_size(&shinfo->frags[0]));
 		}
 
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		inl->byte_count = cpu_to_be32(1 << 31 | (skb->len - spc));
 	}
 }
 
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 			 void *accel_priv, select_queue_fallback_t fallback)
+#else
+			 void *accel_priv)
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u16 rings_p_up = priv->num_tx_rings_p_up;
@@ -735,7 +763,12 @@ u16 mlx4_en_select_queue(struct net_devi
 	if (skb_vlan_tag_present(skb))
 		up = skb_vlan_tag_get(skb) >> VLAN_PRIO_SHIFT;
 
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 	return fallback(dev, skb) % rings_p_up + up * rings_p_up;
+#else
+	return __netdev_pick_tx(dev, skb) % rings_p_up + up * rings_p_up;
+#endif
+
 }
 
 static void mlx4_bf_copy(void __iomem *dst, const void *src,
@@ -775,7 +808,11 @@ static void mlx4_en_tx_write_desc(struct
 		/* Ensure new descriptor hits memory
 		 * before setting ownership of this descriptor to HW
 		 */
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		tx_desc->ctrl.owner_opcode = op_own;
 
 		wmb();
@@ -790,12 +827,18 @@ static void mlx4_en_tx_write_desc(struct
 		/* Ensure new descriptor hits memory
 		 * before setting ownership of this descriptor to HW
 		 */
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		tx_desc->ctrl.owner_opcode = op_own;
 		if (send_doorbell)
 			mlx4_en_xmit_doorbell(ring);
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 		else
 			ring->xmit_more++;
+#endif
 	}
 }
 
@@ -816,7 +859,9 @@ static inline netdev_tx_t __mlx4_en_xmit
 	int real_size;
 	u32 index, bf_index;
 	__be32 op_own;
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	u16 vlan_proto = 0;
+#endif
 	int i_frag;
 	int lso_header_size;
 	void *fragptr = NULL;
@@ -850,6 +895,7 @@ static inline netdev_tx_t __mlx4_en_xmit
 	bf_ok = ring->bf_enabled;
 	if (skb_vlan_tag_present(skb)) {
 		qpn_vlan.vlan_tag = cpu_to_be16(skb_vlan_tag_get(skb));
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		vlan_proto = be16_to_cpu(skb->vlan_proto);
 		if (vlan_proto == ETH_P_8021AD)
 			qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_SVLAN;
@@ -857,10 +903,19 @@ static inline netdev_tx_t __mlx4_en_xmit
 			qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_CVLAN;
 		else
 			qpn_vlan.ins_vlan = 0;
+#else
+		qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_CVLAN;
+#endif
 		bf_ok = false;
 	}
 
+#ifdef HAVE_NETDEV_TXQ_BQL_PREFETCHW
 	netdev_txq_bql_enqueue_prefetchw(ring->tx_queue);
+#else
+#ifdef CONFIG_BQL
+	prefetchw(&ring->tx_queue->dql);
+#endif
+#endif
 
 	/* Track current inflight packets for performance analysis */
 	AVG_PERF_COUNTER(priv->pstats.inflight_avg,
@@ -919,7 +974,11 @@ static inline netdev_tx_t __mlx4_en_xmit
 
 			data->addr = cpu_to_be64(dma);
 			data->lkey = ring->mr_key;
+#ifdef dma_wmb
 			dma_wmb();
+#else
+			wmb();
+#endif
 			data->byte_count = cpu_to_be32(byte_count);
 			--data;
 		}
@@ -936,7 +995,11 @@ static inline netdev_tx_t __mlx4_en_xmit
 
 			data->addr = cpu_to_be64(dma);
 			data->lkey = ring->mr_key;
+#ifdef dma_wmb
 			dma_wmb();
+#else
+			wmb();
+#endif
 			data->byte_count = cpu_to_be32(byte_count);
 		}
 		/* tx completion can avoid cache line miss for common cases */
@@ -950,8 +1013,13 @@ static inline netdev_tx_t __mlx4_en_xmit
 	 */
 	tx_info->ts_requested = 0;
 	if (unlikely(ring->hwtstamp_tx_type == HWTSTAMP_TX_ON &&
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 		     shinfo->tx_flags & SKBTX_HW_TSTAMP)) {
 		shinfo->tx_flags |= SKBTX_IN_PROGRESS;
+#else
+		     shinfo->tx_flags.flags & SKBTX_HW_TSTAMP)) {
+		shinfo->tx_flags.flags |= SKBTX_IN_PROGRESS;
+#endif
 		tx_info->ts_requested = 1;
 	}
 
@@ -959,11 +1027,16 @@ static inline netdev_tx_t __mlx4_en_xmit
 	 * whether LSO is used */
 	tx_desc->ctrl.srcrb_flags = priv->ctrl_flags;
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 		if (!skb->encapsulation)
 			tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |
 								 MLX4_WQE_CTRL_TCP_UDP_CSUM);
 		else
 			tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM);
+#else
+		tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |
+							 MLX4_WQE_CTRL_TCP_UDP_CSUM);
+#endif
 		ring->tx_csum++;
 	}
 
@@ -1016,6 +1089,7 @@ static inline netdev_tx_t __mlx4_en_xmit
 	if (tx_info->inl)
 		build_inline_wqe(tx_desc, skb, shinfo, fragptr);
 
+#ifdef HAVE_SKB_INNER_NETWORK_HEADER
 	if (skb->encapsulation) {
 		union {
 			struct iphdr *v4;
@@ -1033,6 +1107,7 @@ static inline netdev_tx_t __mlx4_en_xmit
 		else
 			op_own |= cpu_to_be32(MLX4_WQE_CTRL_IIP);
 	}
+#endif
 
 	ring->prod += nr_txbb;
 
@@ -1048,7 +1123,11 @@ static inline netdev_tx_t __mlx4_en_xmit
 		netif_tx_stop_queue(ring->tx_queue);
 		ring->queue_stopped++;
 	}
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	send_doorbell = !skb->xmit_more || netif_xmit_stopped(ring->tx_queue);
+#else
+	send_doorbell = true;
+#endif
 
 	real_size = (real_size / 16) & 0x3f;
 
@@ -1129,6 +1208,7 @@ tx_drop:
 	return NETDEV_TX_OK;
 }
 
+#ifdef HAVE_XDP_BUFF
 netdev_tx_t mlx4_en_xmit_frame(struct mlx4_en_rx_alloc *frame,
 			       struct net_device *dev, unsigned int length,
 			       int tx_ind, int *doorbell_pending)
@@ -1194,7 +1274,11 @@ netdev_tx_t mlx4_en_xmit_frame(struct ml
 
 	data->addr = cpu_to_be64(dma);
 	data->lkey = ring->mr_key;
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 	data->byte_count = cpu_to_be32(length);
 
 	/* tx completion can avoid cache line miss for common cases */
@@ -1233,3 +1317,4 @@ tx_drop_count:
 tx_drop:
 	return NETDEV_TX_BUSY;
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx4/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/eq.c
@@ -211,7 +211,12 @@ static void slave_event(struct mlx4_dev
 	memcpy(s_eqe, eqe, sizeof(struct mlx4_eqe) - 1);
 	s_eqe->slave_id = slave;
 	/* ensure all information is written before setting the ownersip bit */
+
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 	s_eqe->owner = !!(slave_eq->prod & SLAVE_EVENT_EQ_SIZE) ? 0x0 : 0x80;
 	++slave_eq->prod;
 
@@ -513,7 +518,11 @@ static int mlx4_eq_int(struct mlx4_dev *
 		 * Make sure we read EQ entry contents after we've
 		 * checked the ownership bit.
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		switch (eqe->type) {
 		case MLX4_EVENT_TYPE_COMP:
--- a/drivers/net/ethernet/mellanox/mlx4/fw.c
+++ b/drivers/net/ethernet/mellanox/mlx4/fw.c
@@ -308,9 +308,11 @@ static int mlx4_handle_vst_qinq(struct m
 	vp_admin = &priv->mfunc.master.vf_admin[slave].vport[port];
 	slave_state = &priv->mfunc.master.slave_state[slave];
 
+#ifdef HAVE_ETH_P_8021AD
 	if ((vp_admin->vlan_proto != htons(ETH_P_8021AD)) ||
 	    (!slave_state->active))
 		return 0;
+#endif
 
 	if (vp_oper->state.vlan_proto == vp_admin->vlan_proto &&
 	    vp_oper->state.default_vlan == vp_admin->default_vlan &&
@@ -407,7 +409,9 @@ int mlx4_QUERY_FUNC_CAP_wrapper(struct m
 			mlx4_get_active_ports(dev, slave);
 		int converted_port = mlx4_slave_convert_port(
 				dev, slave, vhcr->in_modifier);
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		struct mlx4_vport_oper_state *vp_oper;
+#endif
 
 		if (converted_port < 0)
 			return -EINVAL;
@@ -448,7 +452,9 @@ int mlx4_QUERY_FUNC_CAP_wrapper(struct m
 		MLX4_PUT(outbox->buf, dev->caps.phys_port_id[vhcr->in_modifier],
 			 QUERY_FUNC_CAP_PHYS_PORT_ID);
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		vp_oper = &priv->mfunc.master.vf_oper[slave].vport[port];
+#endif
 		err = mlx4_handle_vst_qinq(priv, slave, port);
 		if (err)
 			return err;
@@ -456,8 +462,10 @@ int mlx4_QUERY_FUNC_CAP_wrapper(struct m
 		field = 0;
 		if (dev->caps.phv_bit[port])
 			field |= QUERY_FUNC_CAP_PHV_BIT;
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		if (vp_oper->state.vlan_proto == htons(ETH_P_8021AD))
 			field |= QUERY_FUNC_CAP_VLAN_OFFLOAD_DISABLE;
+#endif
 		MLX4_PUT(outbox->buf, field, QUERY_FUNC_CAP_FLAGS0_OFFSET);
 
 	} else if (vhcr->op_modifier == 0) {
@@ -970,10 +978,11 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 	dev_cap->max_sq_sg = field;
 	MLX4_GET(size, outbox, QUERY_DEV_CAP_MAX_DESC_SZ_SQ_OFFSET);
 	dev_cap->max_sq_desc_sz = size;
-
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_SVLAN_BY_QP_OFFSET);
 	if (field & 0x1)
 		dev_cap->flags2 |= MLX4_DEV_CAP_FLAG2_SVLAN_BY_QP;
+#endif
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_QP_MCG_OFFSET);
 	dev_cap->max_qp_per_mcg = 1 << field;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_RSVD_MCG_OFFSET);
@@ -3183,6 +3192,7 @@ int set_phv_bit(struct mlx4_dev *dev, u8
 }
 EXPORT_SYMBOL(set_phv_bit);
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 int mlx4_get_is_vlan_offload_disabled(struct mlx4_dev *dev, u8 port,
 				      bool *vlan_offload_disabled)
 {
@@ -3198,6 +3208,7 @@ int mlx4_get_is_vlan_offload_disabled(st
 	return err;
 }
 EXPORT_SYMBOL(mlx4_get_is_vlan_offload_disabled);
+#endif
 
 void mlx4_replace_zero_macs(struct mlx4_dev *dev)
 {
--- a/drivers/net/ethernet/mellanox/mlx4/intf.c
+++ b/drivers/net/ethernet/mellanox/mlx4/intf.c
@@ -34,7 +34,9 @@
 #include <linux/slab.h>
 #include <linux/export.h>
 #include <linux/errno.h>
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 
 #include "mlx4.h"
 
@@ -267,6 +269,7 @@ void *mlx4_get_protocol_dev(struct mlx4_
 }
 EXPORT_SYMBOL_GPL(mlx4_get_protocol_dev);
 
+#ifdef HAVE_DEVLINK_H
 struct devlink_port *mlx4_get_devlink_port(struct mlx4_dev *dev, int port)
 {
 	struct mlx4_port_info *info = &mlx4_priv(dev)->port[port];
@@ -274,3 +277,4 @@ struct devlink_port *mlx4_get_devlink_po
 	return &info->devlink_port;
 }
 EXPORT_SYMBOL_GPL(mlx4_get_devlink_port);
+#endif
--- a/drivers/net/ethernet/mellanox/mlx4/main.c
+++ b/drivers/net/ethernet/mellanox/mlx4/main.c
@@ -44,7 +44,9 @@
 #include <linux/delay.h>
 #include <linux/kmod.h>
 #include <linux/etherdevice.h>
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 
 #include <linux/mlx4/device.h>
 #include <linux/mlx4/doorbell.h>
@@ -3690,6 +3692,9 @@ static void mlx4_enable_msi_x(struct mlx
 	struct msix_entry *entries;
 	int i;
 	int port = 0;
+#ifndef HAVE_PCI_ENABLE_MSIX_RANGE
+	int err;
+#endif
 
 	if (msi_x) {
 		int nreq = dev->caps.num_ports * num_online_cpus() + 1;
@@ -3712,8 +3717,24 @@ static void mlx4_enable_msi_x(struct mlx
 		for (i = 0; i < nreq; ++i)
 			entries[i].entry = i;
 
+#ifdef HAVE_PCI_ENABLE_MSIX_RANGE
 		nreq = pci_enable_msix_range(dev->persist->pdev, entries, 2,
 					     nreq);
+#else
+retry:
+		err = pci_enable_msix(dev->persist->pdev, entries, nreq);
+		if (err) {
+			/* Try again if at least 2 vectors are available */
+			if (err > 1) {
+				mlx4_info(dev, "Requested %d vectors, "
+					  "but only %d MSI-X vectors available, "
+					  "trying again\n", nreq, err);
+				nreq = err;
+				goto retry;
+			}
+			nreq = -1;
+		}
+#endif
 
 		/* At least 2 vectors are required, one for the ASYNC EQ and
 		 * a completion EQ.
@@ -3789,13 +3810,17 @@ no_msi:
 
 static int mlx4_init_port_info(struct mlx4_dev *dev, int port)
 {
+#ifdef HAVE_DEVLINK_H
 	struct devlink *devlink = priv_to_devlink(mlx4_priv(dev));
+#endif
 	struct mlx4_port_info *info = &mlx4_priv(dev)->port[port];
 	int err;
 
+#ifdef HAVE_DEVLINK_H
 	err = devlink_port_register(devlink, &info->devlink_port, port);
 	if (err)
 		return err;
+#endif
 
 	info->dev = dev;
 	info->port = port;
@@ -3822,7 +3847,9 @@ static int mlx4_init_port_info(struct ml
 	err = device_create_file(&dev->persist->pdev->dev, &info->port_attr);
 	if (err) {
 		mlx4_err(dev, "Failed to create file for port %d\n", port);
+#ifdef HAVE_DEVLINK_H
 		devlink_port_unregister(&info->devlink_port);
+#endif
 		info->port = -1;
 	}
 
@@ -3843,7 +3870,9 @@ static int mlx4_init_port_info(struct ml
 		mlx4_err(dev, "Failed to create mtu file for port %d\n", port);
 		device_remove_file(&info->dev->persist->pdev->dev,
 				   &info->port_attr);
+#ifdef HAVE_DEVLINK_H
 		devlink_port_unregister(&info->devlink_port);
+#endif
 		info->port = -1;
 	}
 
@@ -3858,7 +3887,9 @@ static void mlx4_cleanup_port_info(struc
 	device_remove_file(&info->dev->persist->pdev->dev, &info->port_attr);
 	device_remove_file(&info->dev->persist->pdev->dev,
 			   &info->port_mtu_attr);
+#ifdef HAVE_DEVLINK_H
 	devlink_port_unregister(&info->devlink_port);
+#endif
 
 #ifdef CONFIG_RFS_ACCEL
 	free_irq_cpu_rmap(info->rmap);
@@ -4663,6 +4694,7 @@ err_disable_pdev:
 	return err;
 }
 
+#ifdef HAVE_DEVLINK_H
 static int mlx4_devlink_port_type_set(struct devlink_port *devlink_port,
 				      enum devlink_port_type port_type)
 {
@@ -4691,26 +4723,40 @@ static int mlx4_devlink_port_type_set(st
 static const struct devlink_ops mlx4_devlink_ops = {
 	.port_type_set	= mlx4_devlink_port_type_set,
 };
+#endif
 
 static int mlx4_init_one(struct pci_dev *pdev, const struct pci_device_id *id)
 {
+#ifdef HAVE_DEVLINK_H
 	struct devlink *devlink;
+#endif
 	struct mlx4_priv *priv;
 	struct mlx4_dev *dev;
 	int ret;
 
 	printk_once(KERN_INFO "%s", mlx4_version);
 
+#ifdef HAVE_DEVLINK_H
 	devlink = devlink_alloc(&mlx4_devlink_ops, sizeof(*priv));
 	if (!devlink)
 		return -ENOMEM;
 	priv = devlink_priv(devlink);
+#else
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+#endif
 
 	dev       = &priv->dev;
 	dev->persist = kzalloc(sizeof(*dev->persist), GFP_KERNEL);
 	if (!dev->persist) {
+#ifdef HAVE_DEVLINK_H
 		ret = -ENOMEM;
 		goto err_devlink_free;
+#else
+		kfree(priv);
+		return -ENOMEM;
+#endif
 	}
 	dev->persist->pdev = pdev;
 	dev->persist->dev = dev;
@@ -4720,23 +4766,35 @@ static int mlx4_init_one(struct pci_dev
 	mutex_init(&dev->persist->interface_state_mutex);
 	mutex_init(&dev->persist->pci_status_mutex);
 
+#ifdef HAVE_DEVLINK_H
 	ret = devlink_register(devlink, &pdev->dev);
 	if (ret)
 		goto err_persist_free;
+#endif
 
 	ret =  __mlx4_init_one(pdev, id->driver_data, priv);
+#ifdef HAVE_DEVLINK_H
 	if (ret)
 		goto err_devlink_unregister;
-
 	pci_save_state(pdev);
 	return 0;
+#else
+	if (ret) {
+		kfree(dev->persist);
+		kfree(priv);
+	} else {
+		pci_save_state(pdev);
+	}
+#endif
 
+#ifdef HAVE_DEVLINK_H
 err_devlink_unregister:
 	devlink_unregister(devlink);
 err_persist_free:
 	kfree(dev->persist);
 err_devlink_free:
 	devlink_free(devlink);
+#endif
 	return ret;
 }
 
@@ -4837,7 +4895,9 @@ static void mlx4_remove_one(struct pci_d
 	struct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);
 	struct mlx4_dev  *dev  = persist->dev;
 	struct mlx4_priv *priv = mlx4_priv(dev);
+#ifdef HAVE_DEVLINK_H
 	struct devlink *devlink = priv_to_devlink(priv);
+#endif
 	int active_vfs = 0;
 
 	if (mlx4_is_slave(dev))
@@ -4871,9 +4931,15 @@ static void mlx4_remove_one(struct pci_d
 
 	pci_release_regions(pdev);
 	mlx4_pci_disable_device(dev);
+#ifdef HAVE_DEVLINK_H
 	devlink_unregister(devlink);
+#endif
 	kfree(dev->persist);
+#ifdef HAVE_DEVLINK_H
 	devlink_free(devlink);
+#else
+	kfree(priv);
+#endif
 	pci_set_drvdata(pdev, NULL);
 }
 
@@ -5057,7 +5123,11 @@ static void mlx4_shutdown(struct pci_dev
 	mutex_unlock(&persist->interface_state_mutex);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_PCI_ERROR_HANDLERS
 static const struct pci_error_handlers mlx4_err_handler = {
+#else
+static struct pci_error_handlers mlx4_err_handler = {
+#endif
 	.error_detected = mlx4_pci_err_detected,
 	.slot_reset     = mlx4_pci_slot_reset,
 	.resume		= mlx4_pci_resume,
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4.h
@@ -45,7 +45,9 @@
 #include <linux/workqueue.h>
 #include <linux/interrupt.h>
 #include <linux/spinlock.h>
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 #include <linux/rwsem.h>
 
 #include <linux/mlx4/device.h>
@@ -862,7 +864,9 @@ struct mlx4_port_info {
 	struct mlx4_roce_info	roce;
 	int			base_qpn;
 	struct cpu_rmap		*rmap;
+#ifdef HAVE_DEVLINK_H
 	struct devlink_port	devlink_port;
+#endif
 };
 
 struct mlx4_sense {
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
@@ -44,8 +44,12 @@
 #ifdef CONFIG_MLX4_EN_DCB
 #include <linux/dcbnl.h>
 #endif
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #include <linux/cpu_rmap.h>
+#endif
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 #include <linux/ptp_clock_kernel.h>
+#endif
 
 #include <linux/mlx4/device.h>
 #include <linux/mlx4/qp.h>
@@ -53,6 +57,9 @@
 #include <linux/mlx4/srq.h>
 #include <linux/mlx4/doorbell.h>
 #include <linux/mlx4/cmd.h>
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+#include <linux/inet_lro.h>
+#endif
 
 #include "en_port.h"
 #include "mlx4_stats.h"
@@ -61,6 +68,43 @@
 #define DRV_VERSION	"4.0-2.0.0"
 #define DRV_RELDATE	"28 Mar 2017"
 
+#ifndef CONFIG_COMPAT_DISABLE_DCB
+#ifdef CONFIG_MLX4_EN_DCB
+
+#ifndef HAVE_IEEE_GET_SET_MAXRATE
+#define CONFIG_SYSFS_MAXRATE
+#endif
+
+#ifndef CONFIG_COMPAT_FDB_API_EXISTS
+#define CONFIG_SYSFS_FDB
+#endif
+
+/* make sure to define QCN only when DCB is not disabled
+ * and EN_DCB is defined
+ */
+#ifndef HAVE_IEEE_GETQCN
+#define CONFIG_SYSFS_QCN
+#endif
+
+#endif
+#endif
+
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
+#define CONFIG_SYSFS_MQPRIO
+#endif
+
+#if !defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && !defined(HAVE_GET_SET_RXFH_INDIR)
+#define CONFIG_SYSFS_INDIR_SETTING
+#endif
+
+#if !defined(HAVE_GET_SET_CHANNELS) && !defined(HAVE_GET_SET_CHANNELS_EXT)
+#define CONFIG_SYSFS_NUM_CHANNELS
+#endif
+
+#ifndef HAVE_NDO_SET_FEATURES
+#define CONFIG_SYSFS_LOOPBACK
+#endif
+
 #define MLX4_EN_MSG_LEVEL	(NETIF_MSG_LINK | NETIF_MSG_IFDOWN)
 
 /*
@@ -109,6 +153,13 @@
 #define MLX4_EN_PRIV_FLAGS_FS_EN_TCP		(1 << 6)
 #define MLX4_EN_PRIV_FLAGS_FS_EN_UDP		(1 << 7)
 #define MLX4_EN_PRIV_FLAGS_DISABLE_MC_LOOPBACK	(1 << 8)
+#ifndef HAVE_NETIF_F_RXFCS
+#define MLX4_EN_PRIV_FLAGS_RXFCS		(1 << 9)
+#endif
+#ifndef HAVE_NETIF_F_RXALL
+#define MLX4_EN_PRIV_FLAGS_RXALL		(1 << 10)
+#endif
+#define MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR		(1 << 11)
 
 #define MLX4_EN_WATCHDOG_TIMEOUT	(15 * HZ)
 
@@ -126,6 +177,11 @@ enum {
 	FRAG_SZ3 = MLX4_EN_ALLOC_SIZE
 };
 #define MLX4_EN_MAX_RX_FRAGS	4
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+/* Minimum packet number till arming the CQ */
+#define MLX4_EN_MIN_RX_ARM	2097152
+#endif
+
 
 /* Maximum ring sizes */
 #define MLX4_EN_MAX_TX_SIZE	8192
@@ -210,6 +266,10 @@ enum {
 #define GET_AVG_PERF_COUNTER(cnt)	(0)
 #endif /* MLX4_EN_PERF_STAT */
 
+#if defined(CONFIG_NET_RX_BUSY_POLL) && defined(HAVE_NDO_BUSY_POLL) && !defined(NAPI_STATE_NO_BUSY_POLL)
+#define MLX4_EN_BUSY_POLL
+#endif
+
 /* Constants for TX flow */
 enum {
 	MAX_INLINE = 104, /* 128 - 16 - 4 - 4 */
@@ -272,6 +332,16 @@ struct mlx4_en_tx_desc {
 	};
 };
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+/* LRO defines for MLX4_EN */
+#define MLX4_EN_LRO_MAX_DESC	32
+
+struct mlx4_en_lro {
+	struct net_lro_mgr	lro_mgr;
+	struct net_lro_desc	lro_desc[MLX4_EN_LRO_MAX_DESC];
+};
+#endif
+
 #define MLX4_EN_USE_SRQ		0x01000000
 
 #define MLX4_EN_CX3_LOW_ID	0x1000
@@ -361,16 +431,26 @@ struct mlx4_en_rx_ring {
 	u8  fcs_del;
 	void *buf;
 	void *rx_info;
+#ifdef HAVE_XDP_BUFF
 	struct bpf_prog __rcu *xdp_prog;
+#endif
 	struct mlx4_en_page_cache page_cache;
 	unsigned long bytes;
 	unsigned long packets;
+#ifdef MLX4_EN_BUSY_POLL
+	unsigned long yields;
+	unsigned long misses;
+	unsigned long cleaned;
+#endif
 	unsigned long csum_ok;
 	unsigned long csum_none;
 	unsigned long csum_complete;
 	unsigned long dropped;
 	int hwtstamp_rx_filter;
 	cpumask_var_t affinity_mask;
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	struct mlx4_en_lro lro;
+#endif
 };
 
 struct mlx4_en_cq {
@@ -387,7 +467,22 @@ struct mlx4_en_cq {
 	u16 moder_cnt;
 	struct mlx4_cqe *buf;
 #define MLX4_EN_OPCODE_ERROR	0x1e
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	u32 tot_rx;
+#endif
 
+#ifdef MLX4_EN_BUSY_POLL
+	unsigned int state;
+#define MLX4_EN_CQ_STATE_IDLE     0
+#define MLX4_EN_CQ_STATE_NAPI     1    /* NAPI owns this CQ */
+#define MLX4_EN_CQ_STATE_POLL     2    /* poll owns this CQ */
+#define MLX4_CQ_LOCKED (MLX4_EN_CQ_STATE_NAPI | MLX4_EN_CQ_STATE_POLL)
+#define MLX4_EN_CQ_STATE_NAPI_YIELD  4    /* NAPI yielded this CQ */
+#define MLX4_EN_CQ_STATE_POLL_YIELD  8    /* poll yielded this CQ */
+#define CQ_YIELD (MLX4_EN_CQ_STATE_NAPI_YIELD | MLX4_EN_CQ_STATE_POLL_YIELD)
+#define CQ_USER_PEND (MLX4_EN_CQ_STATE_POLL | MLX4_EN_CQ_STATE_POLL_YIELD)
+	spinlock_t poll_lock; /* protects from LLS/napi conflicts */
+#endif
 	struct irq_desc *irq_desc;
 };
 
@@ -443,8 +538,10 @@ struct mlx4_en_dev {
 	struct timecounter	clock;
 	unsigned long		last_overflow_check;
 	unsigned long		overflow_period;
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	struct ptp_clock	*ptp_clock;
 	struct ptp_clock_info	ptp_clock_info;
+#endif
 	struct notifier_block	nb;
 };
 
@@ -570,7 +667,13 @@ struct mlx4_en_priv {
 	struct mlx4_en_port_profile *prof;
 	struct net_device *dev;
 	struct net_device_ops dev_ops;
+#ifdef HAVE_VLAN_GRO_RECEIVE
+	struct vlan_group *vlgrp;
+#endif
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+#if (!defined(HAVE_NDO_GET_STATS64) || !defined(HAVE_NETDEV_STATS_TO_STATS64))
+	struct net_device_stats ret_stats;
+#endif
 	struct mlx4_en_port_state port_state;
 	spinlock_t stats_lock;
 	struct ethtool_flow_id ethtool_rules[MAX_NUM_OF_FS_RULES];
@@ -622,7 +725,9 @@ struct mlx4_en_priv {
 	struct mlx4_en_frag_info frag_info[MLX4_EN_MAX_RX_FRAGS];
 	u16 num_frags;
 	u16 log_rx_info;
+#ifdef HAVE_XDP_BUFF
 	int xdp_ring_num;
+#endif
 
 	struct mlx4_en_tx_ring **tx_ring;
 	struct mlx4_en_rx_ring *rx_ring[MAX_RX_RINGS];
@@ -659,6 +764,7 @@ struct mlx4_en_priv {
 	u32 counter_index;
 	struct en_port *vf_ports[MLX4_MAX_NUM_VF];
 
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 #define MLX4_EN_DCB_ENABLED	0x3
 	struct ieee_ets ets;
@@ -667,6 +773,7 @@ struct mlx4_en_priv {
 	struct mlx4_en_cee_config cee_config;
 	u8 dcbx_cap;
 #endif
+#endif
 #ifdef CONFIG_RFS_ACCEL
 	spinlock_t filters_lock;
 	int last_filter_id;
@@ -676,6 +783,12 @@ struct mlx4_en_priv {
 	u64 tunnel_reg_id;
 	__be16 vxlan_port;
 
+#ifdef CONFIG_COMPAT_EN_SYSFS
+	int sysfs_group_initialized;
+#endif
+#ifdef CONFIG_SYSFS_FDB
+	int sysfs_fdb_created;
+#endif
 	u32 pflags;
 	u8 rss_key[MLX4_EN_RSS_KEY_SIZE];
 	u8 rss_hash_fn;
@@ -700,9 +813,122 @@ static inline struct mlx4_cqe *mlx4_en_g
 	return buf + idx * cqe_sz;
 }
 
+#ifdef MLX4_EN_BUSY_POLL
+static inline void mlx4_en_cq_init_lock(struct mlx4_en_cq *cq)
+{
+	spin_lock_init(&cq->poll_lock);
+	cq->state = MLX4_EN_CQ_STATE_IDLE;
+}
+
+/* called from the device poll rutine to get ownership of a cq */
+static inline bool mlx4_en_cq_lock_napi(struct mlx4_en_cq *cq)
+{
+	int rc = true;
+	spin_lock(&cq->poll_lock);
+	if (cq->state & MLX4_CQ_LOCKED) {
+		WARN_ON(cq->state & MLX4_EN_CQ_STATE_NAPI);
+		cq->state |= MLX4_EN_CQ_STATE_NAPI_YIELD;
+		rc = false;
+	} else
+		/* we don't care if someone yielded */
+		cq->state = MLX4_EN_CQ_STATE_NAPI;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* returns true is someone tried to get the cq while napi had it */
+static inline bool mlx4_en_cq_unlock_napi(struct mlx4_en_cq *cq)
+{
+	int rc = false;
+	spin_lock(&cq->poll_lock);
+	WARN_ON(cq->state & (MLX4_EN_CQ_STATE_POLL |
+			       MLX4_EN_CQ_STATE_NAPI_YIELD));
+
+	if (cq->state & MLX4_EN_CQ_STATE_POLL_YIELD)
+		rc = true;
+	cq->state = MLX4_EN_CQ_STATE_IDLE;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* called from mlx4_en_low_latency_recv(), BH are disabled */
+static inline bool mlx4_en_cq_lock_poll(struct mlx4_en_cq *cq)
+{
+	int rc = true;
+
+	spin_lock(&cq->poll_lock);
+	if ((cq->state & MLX4_CQ_LOCKED)) {
+		struct net_device *dev = cq->dev;
+		struct mlx4_en_priv *priv = netdev_priv(dev);
+		struct mlx4_en_rx_ring *rx_ring = priv->rx_ring[cq->ring];
+
+		cq->state |= MLX4_EN_CQ_STATE_POLL_YIELD;
+		rc = false;
+		rx_ring->yields++;
+	} else
+		/* preserve yield marks */
+		cq->state |= MLX4_EN_CQ_STATE_POLL;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* returns true if someone tried to get the cq while it was locked */
+static inline bool mlx4_en_cq_unlock_poll(struct mlx4_en_cq *cq)
+{
+	int rc = false;
+
+	spin_lock(&cq->poll_lock);
+	WARN_ON(cq->state & (MLX4_EN_CQ_STATE_NAPI));
+
+	if (cq->state & MLX4_EN_CQ_STATE_POLL_YIELD)
+		rc = true;
+	cq->state = MLX4_EN_CQ_STATE_IDLE;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* true if a socket is polling, even if it did not get the lock */
+static inline bool mlx4_en_cq_busy_polling(struct mlx4_en_cq *cq)
+{
+	WARN_ON(!(cq->state & MLX4_CQ_LOCKED));
+	return cq->state & CQ_USER_PEND;
+}
+#else
+static inline void mlx4_en_cq_init_lock(struct mlx4_en_cq *cq)
+{
+}
+
+static inline bool mlx4_en_cq_lock_napi(struct mlx4_en_cq *cq)
+{
+	return true;
+}
+
+static inline bool mlx4_en_cq_unlock_napi(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+
+static inline bool mlx4_en_cq_lock_poll(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+
+static inline bool mlx4_en_cq_unlock_poll(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+
+static inline bool mlx4_en_cq_busy_polling(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+#endif /* MLX4_EN_BUSY_POLL */
+
 #define MLX4_EN_WOL_DO_MODIFY (1ULL << 63)
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 void mlx4_en_init_ptys2ethtool_map(void);
+#endif
 void mlx4_en_update_loopback_state(struct net_device *dev,
 				   netdev_features_t features);
 
@@ -740,13 +966,23 @@ int mlx4_en_set_cq_moder(struct mlx4_en_
 int mlx4_en_arm_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
 
 void mlx4_en_tx_irq(struct mlx4_cq *mcq);
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 			 void *accel_priv, select_queue_fallback_t fallback);
+#else
+			 void *accel_priv);
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb);
+#endif
 netdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev);
 netdev_tx_t mlx4_en_vgtp_xmit(struct sk_buff *skb, struct net_device *dev);
+#ifdef HAVE_XDP_BUFF
 netdev_tx_t mlx4_en_xmit_frame(struct mlx4_en_rx_alloc *frame,
 			       struct net_device *dev, unsigned int length,
 			       int tx_ind, int *doorbell_pending);
+#endif
 void mlx4_en_xmit_doorbell(struct mlx4_en_tx_ring *ring);
 bool mlx4_en_rx_recycle(struct mlx4_en_rx_ring *ring,
 			struct mlx4_en_rx_alloc *frame);
@@ -811,16 +1047,58 @@ int mlx4_get_vport_ethtool_stats(struct
 int mlx4_en_DUMP_ETH_STATS(struct mlx4_en_dev *mdev, u8 port, u8 reset);
 int mlx4_en_QUERY_PORT(struct mlx4_en_dev *mdev, u8 port);
 
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx4_en_dcbnl_ops;
 extern const struct dcbnl_rtnl_ops mlx4_en_dcbnl_pfc_ops;
 #endif
+#endif
+
+#ifdef CONFIG_SYSFS_QCN
+int mlx4_en_dcbnl_ieee_getqcn(struct net_device *dev, struct ieee_qcn *qcn);
+int mlx4_en_dcbnl_ieee_setqcn(struct net_device *dev, struct ieee_qcn *qcn);
+int mlx4_en_dcbnl_ieee_getqcnstats(struct net_device *dev,
+				   struct ieee_qcn_stats *qcn_stats);
+#endif
+
+#ifdef CONFIG_COMPAT_EN_SYSFS
+int mlx4_en_sysfs_create(struct net_device *dev);
+void mlx4_en_sysfs_remove(struct net_device *dev);
+#endif
+
+#ifdef CONFIG_SYSFS_MAXRATE
+int mlx4_en_dcbnl_ieee_setmaxrate(struct net_device *dev,
+				  struct ieee_maxrate *maxrate);
+int mlx4_en_dcbnl_ieee_getmaxrate(struct net_device *dev,
+				  struct ieee_maxrate *maxrate);
+#endif
+
+#ifdef CONFIG_SYSFS_NUM_CHANNELS
+struct ethtool_channels {
+	__u32   cmd;
+	__u32   max_rx;
+	__u32   max_tx;
+	__u32   max_other;
+	__u32   max_combined;
+	__u32   rx_count;
+	__u32   tx_count;
+	__u32   other_count;
+	__u32   combined_count;
+};
+
+int mlx4_en_set_channels(struct net_device *dev,
+			 struct ethtool_channels *channel);
+void mlx4_en_get_channels(struct net_device *dev,
+			  struct ethtool_channels *channel);
+#endif
 
 int mlx4_en_setup_tc(struct net_device *dev, u8 up);
 
 #ifdef CONFIG_RFS_ACCEL
+#ifdef HAVE_NDO_RX_FLOW_STEER
 void mlx4_en_cleanup_filters(struct mlx4_en_priv *priv);
 #endif
+#endif
 
 #define MLX4_EN_NUM_SELF_TEST	5
 void mlx4_en_ex_selftest(struct net_device *dev, u32 *flags, u64 *buf);
@@ -836,8 +1114,10 @@ void mlx4_en_update_pfc_stats_bitmap(str
 				     struct mlx4_en_stats_bitmap *stats_bitmap,
 				     u8 rx_ppp, u8 rx_pause,
 				     u8 tx_ppp, u8 tx_pause);
+#ifdef HAVE_NETDEV_BONDING_INFO
 int mlx4_en_netdev_event(struct notifier_block *this,
 			 unsigned long event, void *ptr);
+#endif
 
 /*
  * Functions for time stamping
@@ -852,16 +1132,30 @@ void mlx4_en_remove_timestamp(struct mlx
 /* Globals
  */
 extern const struct ethtool_ops mlx4_en_ethtool_ops;
-
-
+#ifdef HAVE_ETHTOOL_OPS_EXT
+extern const struct ethtool_ops_ext mlx4_en_ethtool_ops_ext;
+#endif
 
 /*
  * printk / logging functions
  */
 
+#if !defined(HAVE_VA_FORMAT) || defined CONFIG_X86_XEN
+#define en_print(level, priv, format, arg...)                   \
+        do {                                                    \
+        if ((priv)->registered)                                 \
+                printk(level "%s: %s: " format, DRV_NAME,       \
+                        (priv->dev)->name, ## arg);             \
+        else                                                    \
+                printk(level "%s: %s: Port %d: " format,        \
+                        DRV_NAME, dev_name(&priv->mdev->pdev->dev), \
+                        (priv)->port, ## arg);                  \
+        } while(0) 
+#else
 __printf(3, 4)
 void en_print(const char *level, const struct mlx4_en_priv *priv,
 	      const char *format, ...);
+#endif
 
 #define en_dbg(mlevel, priv, format, ...)				\
 do {									\
@@ -885,4 +1179,23 @@ do {									\
 	pr_warn(DRV_NAME " %s: " format,				\
 		dev_name(&(mdev)->pdev->dev), ##__VA_ARGS__)
 
+#ifdef CONFIG_SYSFS_INDIR_SETTING
+static inline u32 mlx4_en_get_rxfh_indir_size(struct net_device *dev)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+
+	return priv->rx_ring_num;
+}
+
+int mlx4_en_get_rxfh_indir(struct net_device *dev, u32 *ring_index);
+int mlx4_en_set_rxfh_indir(struct net_device *dev, const u32 *ring_index);
+#endif
+#ifdef CONFIG_SYSFS_LOOPBACK
+int mlx4_en_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			 u32 features);
+#else
+			 netdev_features_t features);
+#endif
+#endif
 #endif
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_stats.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_stats.h
@@ -61,6 +61,11 @@ struct mlx4_en_vport_stats {
 };
 
 struct mlx4_en_port_stats {
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	unsigned long lro_aggregated;
+	unsigned long lro_flushed;
+	unsigned long lro_no_desc;
+#endif
 	unsigned long tso_packets;
 	unsigned long xmit_more;
 	unsigned long queue_stopped;
@@ -71,7 +76,11 @@ struct mlx4_en_port_stats {
 	unsigned long rx_chksum_none;
 	unsigned long rx_chksum_complete;
 	unsigned long tx_chksum_offload;
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+#define NUM_PORT_STATS		13
+#else
 #define NUM_PORT_STATS		10
+#endif
 };
 
 struct mlx4_en_perf_stats {
--- a/drivers/net/ethernet/mellanox/mlx4/pd.c
+++ b/drivers/net/ethernet/mellanox/mlx4/pd.c
@@ -205,9 +205,15 @@ int mlx4_bf_alloc(struct mlx4_dev *dev,
 			goto free_uar;
 		}
 
+#ifdef HAVE_IO_MAPPING_MAP_WC_3_PARAMS
 		uar->bf_map = io_mapping_map_wc(priv->bf_mapping,
 						uar->index << PAGE_SHIFT,
 						PAGE_SIZE);
+#else
+		uar->bf_map = io_mapping_map_wc(priv->bf_mapping,
+						uar->index << PAGE_SHIFT);
+#endif
+
 		if (!uar->bf_map) {
 			err = -ENOMEM;
 			goto unamp_uar;
--- a/drivers/net/ethernet/mellanox/mlx4/resource_tracker.c
+++ b/drivers/net/ethernet/mellanox/mlx4/resource_tracker.c
@@ -802,6 +802,7 @@ static int update_vport_qp_param(struct
 				MLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED |
 				MLX4_VLAN_CTRL_ETH_RX_BLOCK_TAGGED;
 		} else if (0 != vp_oper->state.default_vlan) {
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 			if (vp_oper->state.vlan_proto == htons(ETH_P_8021AD)) {
 				/* vst QinQ should block untagged on TX,
 				 * but cvlan is in payload and phv is set so
@@ -813,11 +814,14 @@ static int update_vport_qp_param(struct
 					MLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |
 					MLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED;
 			} else { /* vst 802.1Q */
+#endif
 				qpc->pri_path.vlan_control |=
 					MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |
 					MLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |
 					MLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED;
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 			}
+#endif
 		} else { /* priority tagged */
 			qpc->pri_path.vlan_control |=
 				MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |
@@ -826,9 +830,11 @@ static int update_vport_qp_param(struct
 
 		qpc->pri_path.vlan_index = vp_oper->vlan_idx;
 		qpc->pri_path.fl |= MLX4_FL_ETH_HIDE_CQE_VLAN;
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		if (vp_oper->state.vlan_proto == htons(ETH_P_8021AD))
 			qpc->pri_path.fl |= MLX4_FL_SV;
 		else
+#endif
 			qpc->pri_path.fl |= MLX4_FL_CV;
 		qpc->pri_path.feup |= MLX4_FEUP_FORCE_ETH_UP | MLX4_FVL_FORCE_ETH_VLAN | MLX4_FVL_RX_FORCE_ETH_VLAN;
 		qpc->pri_path.sched_queue &= 0xC7;
@@ -5725,11 +5731,13 @@ void mlx4_vf_immed_vlan_work_handler(str
 	else if (!work->vlan_id)
 		vlan_control = MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |
 			MLX4_VLAN_CTRL_ETH_RX_BLOCK_TAGGED;
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	else if (work->vlan_proto == htons(ETH_P_8021AD))
 		vlan_control = MLX4_VLAN_CTRL_ETH_TX_BLOCK_PRIO_TAGGED |
 			MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |
 			MLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |
 			MLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED;
+#endif
 	else  /* vst 802.1Q */
 		vlan_control = MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |
 			MLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |
@@ -5773,9 +5781,11 @@ void mlx4_vf_immed_vlan_work_handler(str
 				upd_context->qp_context.pri_path.vlan_index = work->vlan_ix;
 				upd_context->qp_context.pri_path.fl =
 					qp->pri_path_fl | MLX4_FL_ETH_HIDE_CQE_VLAN;
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 				if (work->vlan_proto == htons(ETH_P_8021AD))
 					upd_context->qp_context.pri_path.fl |= MLX4_FL_SV;
 				else
+#endif
 					upd_context->qp_context.pri_path.fl |= MLX4_FL_CV;
 				upd_context->qp_context.pri_path.feup =
 					qp->feup | MLX4_FEUP_FORCE_ETH_UP | MLX4_FVL_FORCE_ETH_VLAN | MLX4_FVL_RX_FORCE_ETH_VLAN;
--- a/include/linux/mlx4/cmd.h
+++ b/include/linux/mlx4/cmd.h
@@ -340,6 +340,9 @@ int mlx4_config_dev_retrieval(struct mlx
 void mlx4_cmd_wake_completions(struct mlx4_dev *dev);
 void mlx4_report_internal_err_comm_event(struct mlx4_dev *dev);
 ssize_t mlx4_get_vf_rate(struct mlx4_dev *dev, int port, int vf, char *buf);
+#if (defined(HAVE_NETIF_F_HW_VLAN_STAG_RX) && !defined(HAVE_VF_VLAN_PROTO))
+ssize_t mlx4_get_vf_vlan_info(struct mlx4_dev *dev, int port, int vf, char *buf);
+#endif
 /*
  * mlx4_get_slave_default_vlan -
  * return true if VST ( default vlan)
--- a/include/linux/mlx4/cq.h
+++ b/include/linux/mlx4/cq.h
@@ -34,7 +34,11 @@
 #define MLX4_CQ_H
 
 #include <linux/types.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 
 #include <linux/mlx4/device.h>
 #include <linux/mlx4/doorbell.h>
--- a/include/linux/mlx4/cq_exp.h
+++ b/include/linux/mlx4/cq_exp.h
@@ -2,7 +2,9 @@
 #define MLX4_CQ_EXP_H
 
 #include <linux/types.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#endif
 
 #include <linux/mlx4/device.h>
 #include <linux/mlx4/doorbell.h>
--- a/include/linux/mlx4/device.h
+++ b/include/linux/mlx4/device.h
@@ -37,12 +37,17 @@
 #include <linux/pci.h>
 #include <linux/completion.h>
 #include <linux/radix-tree.h>
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #include <linux/cpu_rmap.h>
+#endif
 #include <linux/crash_dump.h>
 
 #include <linux/atomic.h>
-
+#ifdef HAVE_TIMECOUNTER_H
 #include <linux/timecounter.h>
+#else
+#include <linux/clocksource.h>
+#endif
 
 #define DEFAULT_UAR_PAGE_SHIFT  12
 
@@ -1577,8 +1582,10 @@ int mlx4_SET_PORT_disable_mc_loopback(st
 				      bool disable_mc_loopback);
 int set_phv_bit(struct mlx4_dev *dev, u8 port, int new_val);
 int get_phv_bit(struct mlx4_dev *dev, u8 port, int *phv);
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 int mlx4_get_is_vlan_offload_disabled(struct mlx4_dev *dev, u8 port,
 				      bool *vlan_offload_disabled);
+#endif
 void mlx4_handle_eth_header_mcast_prio(struct mlx4_net_trans_rule_hw_ctrl *ctrl,
 				       struct _rule_hw *eth_header);
 int mlx4_find_cached_mac(struct mlx4_dev *dev, u8 port, u64 mac, int *idx);
--- a/include/linux/mlx4/driver.h
+++ b/include/linux/mlx4/driver.h
@@ -33,7 +33,9 @@
 #ifndef MLX4_DRIVER_H
 #define MLX4_DRIVER_H
 
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 #include <linux/mlx4/device.h>
 
 struct mlx4_dev;
@@ -146,7 +148,9 @@ int mlx4_port_map_set(struct mlx4_dev *d
 
 void *mlx4_get_protocol_dev(struct mlx4_dev *dev, enum mlx4_protocol proto, int port);
 
+#ifdef HAVE_DEVLINK_H
 struct devlink_port *mlx4_get_devlink_port(struct mlx4_dev *dev, int port);
+#endif
 
 static inline u64 mlx4_mac_to_u64(u8 *addr)
 {
