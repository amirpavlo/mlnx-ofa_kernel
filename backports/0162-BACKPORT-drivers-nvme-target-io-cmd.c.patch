From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/nvme/target/io-cmd.c

Change-Id: I55da76d67c65ab0777427dc4830fd28d7c7b630e
---
 drivers/nvme/target/io-cmd.c | 91 +++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 90 insertions(+), 1 deletion(-)

--- a/drivers/nvme/target/io-cmd.c
+++ b/drivers/nvme/target/io-cmd.c
@@ -19,12 +19,22 @@
 #include <linux/module.h>
 #include "nvmet.h"
 
+#ifdef HAVE_BIO_ENDIO_1_PARAM
 static void nvmet_bio_done(struct bio *bio)
+#else
+static void nvmet_bio_done(struct bio *bio, int error)
+#endif
 {
 	struct nvmet_req *req = bio->bi_private;
 
 	nvmet_req_complete(req,
+#ifdef HAVE_BLK_STATUS_T
 		bio->bi_status ? NVME_SC_INTERNAL | NVME_SC_DNR : 0);
+#elif defined(HAVE_STRUCT_BIO_BI_ERROR)
+		bio->bi_error ? NVME_SC_INTERNAL | NVME_SC_DNR : 0);
+#else
+		error ? NVME_SC_INTERNAL | NVME_SC_DNR : 0);
+#endif
 
 	if (bio != &req->inline_bio)
 		bio_put(bio);
@@ -40,7 +50,13 @@ static void nvmet_inline_bio_init(struct
 {
 	struct bio *bio = &req->inline_bio;
 
+#ifdef HAVE_BIO_INIT_3_PARAMS
 	bio_init(bio, req->inline_bvec, NVMET_MAX_INLINE_BIOVEC);
+#else
+	bio_init(bio);
+	bio->bi_max_vecs = NVMET_MAX_INLINE_BIOVEC;
+	bio->bi_io_vec = req->inline_bvec;
+#endif
 }
 
 static void nvmet_execute_rw(struct nvmet_req *req)
@@ -49,7 +65,9 @@ static void nvmet_execute_rw(struct nvme
 	struct scatterlist *sg;
 	struct bio *bio;
 	sector_t sector;
+#ifdef HAVE_SUBMIT_BIO_1_PARAM
 	blk_qc_t cookie;
+#endif
 	int op, op_flags = 0, i;
 
 	if (!req->sg_cnt) {
@@ -59,7 +77,11 @@ static void nvmet_execute_rw(struct nvme
 
 	if (req->cmd->rw.opcode == nvme_cmd_write) {
 		op = REQ_OP_WRITE;
+#ifdef HAVE_REQ_IDLE
 		op_flags = REQ_SYNC | REQ_IDLE;
+#else
+		op_flags = WRITE_ODIRECT;
+#endif
 		if (req->cmd->rw.control & cpu_to_le16(NVME_RW_FUA))
 			op_flags |= REQ_FUA;
 	} else {
@@ -72,11 +94,20 @@ static void nvmet_execute_rw(struct nvme
 	nvmet_inline_bio_init(req);
 	bio = &req->inline_bio;
 	bio->bi_bdev = req->ns->bdev;
+#ifdef HAVE_STRUCT_BIO_BI_ITER
 	bio->bi_iter.bi_sector = sector;
+#else
+	bio->bi_sector = sector;
+#endif
 	bio->bi_private = req;
 	bio->bi_end_io = nvmet_bio_done;
 	bio_set_op_attrs(bio, op, op_flags);
 
+#ifdef HAVE_RH7_STRUCT_BIO_AUX
+	bio->bio_aux = kmalloc(sizeof(struct bio_aux), GFP_KERNEL);
+	bio_init_aux(bio, bio->bio_aux);
+#endif
+
 	for_each_sg(req->sg, sg, req->sg_cnt, i) {
 		while (bio_add_page(bio, sg_page(sg), sg->length, sg->offset)
 				!= sg->length) {
@@ -84,20 +115,37 @@ static void nvmet_execute_rw(struct nvme
 
 			bio = bio_alloc(GFP_KERNEL, min(sg_cnt, BIO_MAX_PAGES));
 			bio->bi_bdev = req->ns->bdev;
+#ifdef HAVE_STRUCT_BIO_BI_ITER
 			bio->bi_iter.bi_sector = sector;
+#else
+			bio->bi_sector = sector;
+#endif
 			bio_set_op_attrs(bio, op, op_flags);
 
 			bio_chain(bio, prev);
+#ifdef HAVE_SUBMIT_BIO_1_PARAM
 			submit_bio(prev);
+#else
+			submit_bio(bio_data_dir(prev), prev);
+#endif
 		}
 
 		sector += sg->length >> 9;
 		sg_cnt--;
 	}
 
+#ifdef HAVE_SUBMIT_BIO_1_PARAM
 	cookie = submit_bio(bio);
-
+#ifdef HAVE_BLK_MQ_POLL
 	blk_mq_poll(bdev_get_queue(req->ns->bdev), cookie);
+#elif defined(HAVE_BLK_POLL)
+	blk_poll(bdev_get_queue(req->ns->bdev), cookie);
+#endif /* HAVE_BLK_MQ_POLL */
+
+#else
+	submit_bio(bio_data_dir(bio), bio);
+#endif /* HAVE_SUBMIT_BIO_1_PARAM */
+
 }
 
 static void nvmet_execute_flush(struct nvmet_req *req)
@@ -110,18 +158,33 @@ static void nvmet_execute_flush(struct n
 	bio->bi_bdev = req->ns->bdev;
 	bio->bi_private = req;
 	bio->bi_end_io = nvmet_bio_done;
+#ifdef HAVE_STRUCT_BIO_BI_OPF
 	bio->bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;
+#else
+	bio_set_op_attrs(bio, REQ_OP_WRITE, WRITE_FLUSH);
+#endif
 
+#ifdef HAVE_SUBMIT_BIO_1_PARAM
 	submit_bio(bio);
+#else
+	submit_bio(bio_data_dir(bio), bio);
+#endif
 }
 
 static u16 nvmet_discard_range(struct nvmet_ns *ns,
 		struct nvme_dsm_range *range, struct bio **bio)
 {
+#ifdef HAVE___BLKDEV_ISSUE_DISCARD
 	if (__blkdev_issue_discard(ns->bdev,
 			le64_to_cpu(range->slba) << (ns->blksize_shift - 9),
 			le32_to_cpu(range->nlb) << (ns->blksize_shift - 9),
 			GFP_KERNEL, 0, bio))
+#else
+	if (blkdev_issue_discard(ns->bdev,
+			le64_to_cpu(range->slba) << (ns->blksize_shift - 9),
+			le32_to_cpu(range->nlb) << (ns->blksize_shift - 9),
+			GFP_KERNEL, 0))
+#endif
 		return NVME_SC_INTERNAL | NVME_SC_DNR;
 	return 0;
 }
@@ -148,10 +211,22 @@ static void nvmet_execute_discard(struct
 		bio->bi_private = req;
 		bio->bi_end_io = nvmet_bio_done;
 		if (status) {
+#ifdef HAVE_BLK_STATUS_T
 			bio->bi_status = BLK_STS_IOERR;
+#elif defined(HAVE_STRUCT_BIO_BI_ERROR)
+			bio->bi_error = -EIO;
+#endif
+#ifdef HAVE_BIO_ENDIO_1_PARAM
 			bio_endio(bio);
+#else
+			bio_endio(bio, -EIO);
+#endif
 		} else {
+#ifdef HAVE_SUBMIT_BIO_1_PARAM
 			submit_bio(bio);
+#else
+			submit_bio(bio_data_dir(bio), bio);
+#endif
 		}
 	} else {
 		nvmet_req_complete(req, status);
@@ -173,6 +248,7 @@ static void nvmet_execute_dsm(struct nvm
 	}
 }
 
+#ifdef HAVE_BLKDEV_ISSUE_ZEROOUT
 static void nvmet_execute_write_zeroes(struct nvmet_req *req)
 {
 	struct nvme_write_zeroes_cmd *write_zeroes = &req->cmd->write_zeroes;
@@ -186,18 +262,29 @@ static void nvmet_execute_write_zeroes(s
 	nr_sector = (((sector_t)le16_to_cpu(write_zeroes->length)) <<
 		(req->ns->blksize_shift - 9)) + 1;
 
+#ifdef CONFIG_COMPAT_IS_BLKDEV_ISSUE_ZEROOUT_HAS_FLAGS
 	if (__blkdev_issue_zeroout(req->ns->bdev, sector, nr_sector,
 				GFP_KERNEL, &bio, 0))
+#else
+	if (__blkdev_issue_zeroout(req->ns->bdev, sector, nr_sector,
+				GFP_KERNEL, &bio, true))
+
+#endif
 		status = NVME_SC_INTERNAL | NVME_SC_DNR;
 
 	if (bio) {
 		bio->bi_private = req;
 		bio->bi_end_io = nvmet_bio_done;
+#ifdef HAVE_SUBMIT_BIO_1_PARAM
 		submit_bio(bio);
+#else
+		submit_bio(bio_data_dir(bio), bio);
+#endif
 	} else {
 		nvmet_req_complete(req, status);
 	}
 }
+#endif
 
 u16 nvmet_parse_io_cmd(struct nvmet_req *req)
 {
@@ -229,9 +316,11 @@ u16 nvmet_parse_io_cmd(struct nvmet_req
 		req->data_len = (le32_to_cpu(cmd->dsm.nr) + 1) *
 			sizeof(struct nvme_dsm_range);
 		return 0;
+#ifdef HAVE_BLKDEV_ISSUE_ZEROOUT
 	case nvme_cmd_write_zeroes:
 		req->execute = nvmet_execute_write_zeroes;
 		return 0;
+#endif
 	default:
 		pr_err("unhandled cmd %d on qid %d\n", cmd->common.opcode,
 		       req->sq->qid);
