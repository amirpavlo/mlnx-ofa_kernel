From: Israel Rukshin <israelr@mellanox.com>
Subject: [PATCH] BACKPORT: ib_srp

Change-Id: Ic90bc43f6bd61818530da7fb700962a8e1ef4aa5
Signed-off-by: Israel Rukshin <israelr@mellanox.com>
---
 drivers/infiniband/ulp/srp/ib_srp.c | 210 ++++++++++++++++++++++++++++++++++++
 drivers/infiniband/ulp/srp/ib_srp.h |   7 ++
 2 files changed, 217 insertions(+)

--- a/drivers/infiniband/ulp/srp/ib_srp.c
+++ b/drivers/infiniband/ulp/srp/ib_srp.c
@@ -77,12 +77,22 @@ module_param(srp_sg_tablesize, uint, 044
 MODULE_PARM_DESC(srp_sg_tablesize, "Deprecated name for cmd_sg_entries");
 
 module_param(cmd_sg_entries, uint, 0444);
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 MODULE_PARM_DESC(cmd_sg_entries,
 		 "Default number of gather/scatter entries in the SRP command (default is 12, max 255)");
+#else
+MODULE_PARM_DESC(cmd_sg_entries,
+                 "Default number of gather/scatter entries in the SRP command (default is 12, max 12)");
+#endif
 
 module_param(indirect_sg_entries, uint, 0444);
+#ifdef HAVE_SG_MAX_SEGMENTS
 MODULE_PARM_DESC(indirect_sg_entries,
 		 "Default max number of gather/scatter entries (default is 12, max is " __stringify(SG_MAX_SEGMENTS) ")");
+#else
+MODULE_PARM_DESC(indirect_sg_entries,
+		 "Default max number of gather/scatter entries (default is 12, max is " __stringify(SCSI_MAX_SG_CHAIN_SEGMENTS) ")");
+#endif
 
 module_param(allow_ext_sg, bool, 0444);
 MODULE_PARM_DESC(allow_ext_sg,
@@ -131,8 +141,12 @@ MODULE_PARM_DESC(dev_loss_tmo,
 
 static unsigned ch_count;
 module_param(ch_count, uint, 0444);
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 MODULE_PARM_DESC(ch_count,
 		 "Number of RDMA channels to use for communication with an SRP target. Using more than one channel improves performance if the HCA supports multiple completion vectors. The default value is the minimum of four times the number of online CPU sockets and the number of completion vectors supported by the HCA.");
+#else
+MODULE_PARM_DESC(ch_count, "Number of RDMA channels to use for communication with an SRP target. [deprecated (using 1 channel)]");
+#endif
 
 static void srp_add_one(struct ib_device *device);
 static void srp_remove_one(struct ib_device *device, void *client_data);
@@ -851,6 +865,9 @@ static int srp_alloc_req_data(struct srp
 	dma_addr_t dma_addr;
 	int i, ret = -ENOMEM;
 
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	INIT_LIST_HEAD(&ch->free_reqs);
+#endif
 	ch->req_ring = kcalloc(target->req_ring_size, sizeof(*ch->req_ring),
 			       GFP_KERNEL);
 	if (!ch->req_ring)
@@ -882,6 +899,10 @@ static int srp_alloc_req_data(struct srp
 			goto out;
 
 		req->indirect_dma_addr = dma_addr;
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+		req->index = i;
+		list_add_tail(&req->list, &ch->free_reqs);
+#endif
 	}
 	ret = 0;
 
@@ -1135,6 +1156,9 @@ static void srp_free_req(struct srp_rdma
 
 	spin_lock_irqsave(&ch->lock, flags);
 	ch->req_lim += req_lim_delta;
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	list_add_tail(&req->list, &ch->free_reqs);
+#endif
 	spin_unlock_irqrestore(&ch->lock, flags);
 }
 
@@ -1889,6 +1913,10 @@ static void srp_process_rsp(struct srp_r
 		}
 		spin_unlock_irqrestore(&ch->lock, flags);
 	} else {
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+		req = &ch->req_ring[rsp->tag];
+		scmnd = srp_claim_req(ch, req, NULL, NULL);
+#else
 		scmnd = scsi_host_find_tag(target->scsi_host, rsp->tag);
 		if (scmnd && scmnd->host_scribble) {
 			req = (void *)scmnd->host_scribble;
@@ -1896,6 +1924,7 @@ static void srp_process_rsp(struct srp_r
 		} else {
 			scmnd = NULL;
 		}
+#endif
 		if (!scmnd) {
 			shost_printk(KERN_ERR, target->scsi_host,
 				     "Null scmnd for RSP w/tag %#016llx received on ch %td / QP %#x\n",
@@ -1991,8 +2020,13 @@ static void srp_process_aer_req(struct s
 	};
 	s32 delta = be32_to_cpu(req->req_lim_delta);
 
+#ifdef HAVE_SCSI_DEVICE_U64_LUN
 	shost_printk(KERN_ERR, target->scsi_host, PFX
 		     "ignoring AER for LUN %llu\n", scsilun_to_int(&req->lun));
+#else
+	shost_printk(KERN_ERR, target->scsi_host, PFX
+		     "ignoring AER for LUN %u\n", scsilun_to_int(&req->lun));
+#endif
 
 	if (srp_response_common(ch, delta, &rsp, sizeof(rsp)))
 		shost_printk(KERN_ERR, target->scsi_host, PFX
@@ -2101,8 +2135,10 @@ static int srp_queuecommand(struct Scsi_
 	struct srp_cmd *cmd;
 	struct ib_device *dev;
 	unsigned long flags;
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	u32 tag;
 	u16 idx;
+#endif
 	int len, ret;
 	const bool in_scsi_eh = !in_interrupt() && current == shost->ehandler;
 
@@ -2119,6 +2155,7 @@ static int srp_queuecommand(struct Scsi_
 	if (unlikely(scmnd->result))
 		goto err;
 
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	WARN_ON_ONCE(scmnd->request->tag < 0);
 	tag = blk_mq_unique_tag(scmnd->request);
 	ch = &target->ch[blk_mq_unique_tag_to_hwq(tag)];
@@ -2126,15 +2163,27 @@ static int srp_queuecommand(struct Scsi_
 	WARN_ONCE(idx >= target->req_ring_size, "%s: tag %#x: idx %d >= %d\n",
 		  dev_name(&shost->shost_gendev), tag, idx,
 		  target->req_ring_size);
+#else
+	ch = &target->ch[0];
+#endif
 
 	spin_lock_irqsave(&ch->lock, flags);
 	iu = __srp_get_tx_iu(ch, SRP_IU_CMD);
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	spin_unlock_irqrestore(&ch->lock, flags);
 
 	if (!iu)
 		goto err;
 
 	req = &ch->req_ring[idx];
+#else
+	if (!iu)
+		goto err_unlock;
+
+	req = list_first_entry(&ch->free_reqs, struct srp_request, list);
+	list_del(&req->list);
+	spin_unlock_irqrestore(&ch->lock, flags);
+#endif
 	dev = target->srp_host->srp_dev->dev;
 	ib_dma_sync_single_for_cpu(dev, iu->dma, target->max_iu_len,
 				   DMA_TO_DEVICE);
@@ -2146,7 +2195,11 @@ static int srp_queuecommand(struct Scsi_
 
 	cmd->opcode = SRP_CMD;
 	int_to_scsilun(scmnd->device->lun, &cmd->lun);
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	cmd->tag    = tag;
+#else
+	cmd->tag    = req->index;
+#endif
 	memcpy(cmd->cdb, scmnd->cmnd, scmnd->cmd_len);
 
 	req->scmnd    = scmnd;
@@ -2195,6 +2248,14 @@ err_iu:
 	 */
 	req->scmnd = NULL;
 
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	spin_lock_irqsave(&ch->lock, flags);
+	list_add(&req->list, &ch->free_reqs);
+
+err_unlock:
+	spin_unlock_irqrestore(&ch->lock, flags);
+
+#endif
 err:
 	if (scmnd->result) {
 		scmnd->scsi_done(scmnd);
@@ -2510,6 +2571,31 @@ static int srp_cm_handler(struct ib_cm_i
 	return 0;
 }
 
+#if defined(HAVE_SCSI_HOST_TEMPLATE_CHANGE_QUEUE_TYPE) && \
+	!defined(HAVE_SCSI_TCQ_SCSI_CHANGE_QUEUE_TYPE)
+/**
+ * srp_change_queue_type - changing device queue tag type
+ * @sdev: scsi device struct
+ * @tag_type: requested tag type
+ *
+ * Returns queue tag type.
+ */
+static int
+srp_change_queue_type(struct scsi_device *sdev, int tag_type)
+{
+	if (sdev->tagged_supported) {
+		scsi_set_tag_type(sdev, tag_type);
+		if (tag_type)
+			scsi_activate_tcq(sdev, sdev->queue_depth);
+		else
+			scsi_deactivate_tcq(sdev, sdev->queue_depth);
+	} else
+		tag_type = 0;
+
+	return tag_type;
+}
+#endif
+
 /**
  * srp_change_queue_depth - setting device queue depth
  * @sdev: scsi device struct
@@ -2517,13 +2603,40 @@ static int srp_cm_handler(struct ib_cm_i
  *
  * Returns queue depth.
  */
+#ifdef HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 static int
 srp_change_queue_depth(struct scsi_device *sdev, int qdepth)
 {
 	if (!sdev->tagged_supported)
 		qdepth = 1;
+#ifdef HAVE_SCSI_CHANGE_QUEUE_DEPTH
 	return scsi_change_queue_depth(sdev, qdepth);
+#else
+	scsi_adjust_queue_depth(sdev, qdepth);
+	return sdev->queue_depth;
+#endif //HAVE_SCSI_CHANGE_QUEUE_DEPTH
+}
+#else
+static int
+srp_change_queue_depth(struct scsi_device *sdev, int qdepth, int reason)
+{
+	struct Scsi_Host *shost = sdev->host;
+	int max_depth;
+	if (reason == SCSI_QDEPTH_DEFAULT || reason == SCSI_QDEPTH_RAMP_UP) {
+		max_depth = shost->can_queue;
+		if (!sdev->tagged_supported)
+			max_depth = 1;
+		if (qdepth > max_depth)
+			qdepth = max_depth;
+		scsi_adjust_queue_depth(sdev, scsi_get_tag_type(sdev), qdepth);
+	} else if (reason == SCSI_QDEPTH_QFULL)
+		scsi_track_queue_full(sdev, qdepth);
+	else
+		return -EOPNOTSUPP;
+
+	return sdev->queue_depth;
 }
+#endif //HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 
 static int srp_send_tsk_mgmt(struct srp_rdma_ch *ch, u64 req_tag, u64 lun,
 			     u8 func, u8 *status)
@@ -2593,8 +2706,10 @@ static int srp_abort(struct scsi_cmnd *s
 {
 	struct srp_target_port *target = host_to_target(scmnd->device->host);
 	struct srp_request *req = (struct srp_request *) scmnd->host_scribble;
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	u32 tag;
 	u16 ch_idx;
+#endif
 	struct srp_rdma_ch *ch;
 	int ret;
 
@@ -2602,6 +2717,7 @@ static int srp_abort(struct scsi_cmnd *s
 
 	if (!req)
 		return SUCCESS;
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	tag = blk_mq_unique_tag(scmnd->request);
 	ch_idx = blk_mq_unique_tag_to_hwq(tag);
 	if (WARN_ON_ONCE(ch_idx >= target->ch_count))
@@ -2613,6 +2729,16 @@ static int srp_abort(struct scsi_cmnd *s
 		     "Sending SRP abort for tag %#x\n", tag);
 	if (srp_send_tsk_mgmt(ch, tag, scmnd->device->lun,
 			      SRP_TSK_ABORT_TASK, NULL) == 0)
+#else
+        ch = &target->ch[0];
+        if (!srp_claim_req(ch, req, NULL, scmnd))
+                return SUCCESS;
+        shost_printk(KERN_ERR, target->scsi_host,
+                     "Sending SRP abort for req index %#x\n", req->index);
+
+	if (srp_send_tsk_mgmt(ch, req->index, scmnd->device->lun,
+			      SRP_TSK_ABORT_TASK, NULL) == 0)
+#endif
 		ret = SUCCESS;
 	else if (target->rport->state == SRP_RPORT_LOST)
 		ret = FAST_IO_FAIL;
@@ -2662,6 +2788,7 @@ static int srp_reset_host(struct scsi_cm
 	return srp_reconnect_rport(target->rport) == 0 ? SUCCESS : FAILED;
 }
 
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 static int srp_slave_alloc(struct scsi_device *sdev)
 {
 	struct Scsi_Host *shost = sdev->host;
@@ -2675,6 +2802,7 @@ static int srp_slave_alloc(struct scsi_d
 
 	return 0;
 }
+#endif
 
 static int srp_slave_configure(struct scsi_device *sdev)
 {
@@ -2867,11 +2995,20 @@ static struct scsi_host_template srp_tem
 	.module				= THIS_MODULE,
 	.name				= "InfiniBand SRP initiator",
 	.proc_name			= DRV_NAME,
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 	.slave_alloc			= srp_slave_alloc,
+#endif
 	.slave_configure		= srp_slave_configure,
 	.info				= srp_target_info,
 	.queuecommand			= srp_queuecommand,
 	.change_queue_depth             = srp_change_queue_depth,
+#ifdef HAVE_SCSI_HOST_TEMPLATE_CHANGE_QUEUE_TYPE
+#ifdef HAVE_SCSI_TCQ_SCSI_CHANGE_QUEUE_TYPE
+	.change_queue_type		= scsi_change_queue_type,
+#else
+	.change_queue_type		= srp_change_queue_type,
+#endif
+#endif
 	.eh_abort_handler		= srp_abort,
 	.eh_device_reset_handler	= srp_reset_device,
 	.eh_host_reset_handler		= srp_reset_host,
@@ -2882,7 +3019,15 @@ static struct scsi_host_template srp_tem
 	.cmd_per_lun			= SRP_DEFAULT_CMD_SQ_SIZE,
 	.use_clustering			= ENABLE_CLUSTERING,
 	.shost_attrs			= srp_host_attrs,
+#ifdef HAVE_SCSI_HOST_TEMPLATE_USE_HOST_WIDE_TAGS
+	.use_host_wide_tags		= 1,
+#endif
+#ifdef HAVE_SCSI_HOST_TEMPLATE_USE_BLK_TAGS
+	.use_blk_tags			= 1,
+#endif
+#ifdef HAVE_SCSI_HOST_TEMPLATE_TRACK_QUEUE_DEPTH
 	.track_queue_depth		= 1,
+#endif
 };
 
 static int srp_sdev_count(struct Scsi_Host *host)
@@ -2931,8 +3076,13 @@ static int srp_add_target(struct srp_hos
 	list_add_tail(&target->list, &host->target_list);
 	spin_unlock(&host->target_lock);
 
+#ifdef HAVE_SCSI_SCAN_INITIAL
 	scsi_scan_target(&target->scsi_host->shost_gendev,
 			 0, target->scsi_id, SCAN_WILD_CARD, SCSI_SCAN_INITIAL);
+#else
+        scsi_scan_target(&target->scsi_host->shost_gendev,
+                         0, target->scsi_id, SCAN_WILD_CARD, 0);
+#endif
 
 	if (srp_connected_ch(target) < target->ch_count ||
 	    target->qp_in_error) {
@@ -3192,11 +3342,19 @@ static int srp_parse_options(const char
 			break;
 
 		case SRP_OPT_CMD_SG_ENTRIES:
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 			if (match_int(args, &token) || token < 1 || token > 255) {
 				pr_warn("bad max cmd_sg_entries parameter '%s'\n",
 					p);
 				goto out;
 			}
+#else
+			if (match_int(args, &token) || token < 1 || token > 12) {
+				pr_warn("bad max cmd_sg_entries parameter '%s'\n",
+					p);
+				goto out;
+			}
+#endif
 			target->cmd_sg_cnt = token;
 			break;
 
@@ -3209,12 +3367,21 @@ static int srp_parse_options(const char
 			break;
 
 		case SRP_OPT_SG_TABLESIZE:
+#ifdef HAVE_SG_MAX_SEGMENTS
 			if (match_int(args, &token) || token < 1 ||
 					token > SG_MAX_SEGMENTS) {
 				pr_warn("bad max sg_tablesize parameter '%s'\n",
 					p);
 				goto out;
 			}
+#else
+			if (match_int(args, &token) || token < 1 ||
+					token > SCSI_MAX_SG_CHAIN_SEGMENTS) {
+				pr_warn("bad max sg_tablesize parameter '%s'\n",
+					p);
+				goto out;
+			}
+#endif
 			target->sg_tablesize = token;
 			break;
 
@@ -3274,7 +3441,11 @@ static ssize_t srp_create_target(struct
 	struct srp_device *srp_dev = host->srp_dev;
 	struct ib_device *ibdev = srp_dev->dev;
 	int ret, node_idx, node, cpu, i;
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 	unsigned int max_sectors_per_mr, mr_per_cmd = 0;
+#else
+	unsigned int mr_per_cmd = 0;
+#endif
 	bool multich = false;
 
 	target_host = scsi_host_alloc(&srp_template,
@@ -3282,6 +3453,11 @@ static ssize_t srp_create_target(struct
 	if (!target_host)
 		return -ENOMEM;
 
+#if defined(HAVE_SCSI_HOST_USE_BLK_MQ) && \
+	defined(HAVE_SCSI_TCQ_SCSI_INIT_SHARED_TAG_MAP) && \
+	!defined(HAVE_SCSI_HOST_TEMPLATE_USE_BLK_TAGS)
+	target_host->use_blk_mq = true;
+#endif
 	target_host->transportt  = ib_srp_transport_template;
 	target_host->max_channel = 0;
 	target_host->max_id      = 1;
@@ -3315,6 +3491,12 @@ static ssize_t srp_create_target(struct
 	if (ret)
 		goto out;
 
+#ifdef HAVE_SCSI_HOST_TEMPLATE_USE_BLK_TAGS
+	ret = scsi_init_shared_tag_map(target_host, target_host->can_queue);
+	if (ret)
+		goto out;
+#endif
+
 	target->req_ring_size = target->queue_size - SRP_TSK_MGMT_SQ_SIZE;
 
 	if (!srp_conn_unique(target->srp_host, target)) {
@@ -3337,6 +3519,7 @@ static ssize_t srp_create_target(struct
 		bool gaps_reg = (ibdev->attrs.device_cap_flags &
 				 IB_DEVICE_SG_GAPS_REG);
 
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 		max_sectors_per_mr = srp_dev->max_pages_per_mr <<
 				  (ilog2(srp_dev->mr_page_size) - 9);
 		if (!gaps_reg) {
@@ -3364,6 +3547,12 @@ static ssize_t srp_create_target(struct
 			 gaps_reg, target->scsi_host->max_sectors,
 			 srp_dev->max_pages_per_mr, srp_dev->mr_page_size,
 			 max_sectors_per_mr, mr_per_cmd);
+#else
+		if (!gaps_reg)
+			mr_per_cmd = target->cmd_sg_cnt;
+		else
+			mr_per_cmd = 1;
+#endif
 	}
 
 	target_host->sg_tablesize = target->sg_tablesize;
@@ -3383,11 +3572,15 @@ static ssize_t srp_create_target(struct
 		goto out;
 
 	ret = -ENOMEM;
+#ifdef HAVE_BLK_MQ_UNIQUE_TAG
 	target->ch_count = max_t(unsigned, num_online_nodes(),
 				 min(ch_count ? :
 				     min(4 * num_online_nodes(),
 					 ibdev->num_comp_vectors),
 				     num_online_cpus()));
+#else
+	target->ch_count = 1;
+#endif
 	target->ch = kcalloc(target->ch_count, sizeof(*target->ch),
 			     GFP_KERNEL);
 	if (!target->ch)
@@ -3454,7 +3647,9 @@ static ssize_t srp_create_target(struct
 	}
 
 connected:
+#ifdef HAVE_SCSI_HOST_NR_HW_QUEUES
 	target->scsi_host->nr_hw_queues = target->ch_count;
+#endif
 
 	ret = srp_add_target(host, target);
 	if (ret)
@@ -3699,10 +3894,17 @@ static int __init srp_init_module(void)
 	if (!cmd_sg_entries)
 		cmd_sg_entries = SRP_DEF_SG_TABLESIZE;
 
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 	if (cmd_sg_entries > 255) {
 		pr_warn("Clamping cmd_sg_entries to 255\n");
 		cmd_sg_entries = 255;
 	}
+#else
+	if (cmd_sg_entries > 12) {
+		pr_warn("Clamping cmd_sg_entries to 12\n");
+		cmd_sg_entries = 12;
+	}
+#endif
 
 	if (!indirect_sg_entries)
 		indirect_sg_entries = cmd_sg_entries;
@@ -3712,11 +3914,19 @@ static int __init srp_init_module(void)
 		indirect_sg_entries = cmd_sg_entries;
 	}
 
+#ifdef HAVE_SG_MAX_SEGMENTS
 	if (indirect_sg_entries > SG_MAX_SEGMENTS) {
 		pr_warn("Clamping indirect_sg_entries to %u\n",
 			SG_MAX_SEGMENTS);
 		indirect_sg_entries = SG_MAX_SEGMENTS;
 	}
+#else
+	if (indirect_sg_entries > SCSI_MAX_SG_CHAIN_SEGMENTS) {
+		pr_warn("Clamping indirect_sg_entries to %u\n",
+			SCSI_MAX_SG_CHAIN_SEGMENTS);
+		indirect_sg_entries = SCSI_MAX_SG_CHAIN_SEGMENTS;
+	}
+#endif
 
 	srp_remove_wq = create_workqueue("srp_remove");
 	if (!srp_remove_wq) {
--- a/drivers/infiniband/ulp/srp/ib_srp.h
+++ b/drivers/infiniband/ulp/srp/ib_srp.h
@@ -112,6 +112,10 @@ struct srp_host {
 };
 
 struct srp_request {
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	struct list_head        list;
+	short                   index;
+#endif
 	struct scsi_cmnd       *scmnd;
 	struct srp_iu	       *cmd;
 	union {
@@ -132,6 +136,9 @@ struct srp_request {
 struct srp_rdma_ch {
 	/* These are RW in the hot path, and commonly used together */
 	struct list_head	free_tx;
+#ifndef HAVE_BLK_MQ_UNIQUE_TAG
+	struct list_head        free_reqs;
+#endif
 	spinlock_t		lock;
 	s32			req_lim;
 
