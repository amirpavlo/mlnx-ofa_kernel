From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: ib-core

Change-Id: Ie209820349292d72897ef4a621c850ee575379c8
Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
---
 drivers/infiniband/core/addr.c             | 193 +++++++++++++++++++++++++-
 drivers/infiniband/core/cm.c               |  27 ++++
 drivers/infiniband/core/cma.c              |  84 ++++++++++-
 drivers/infiniband/core/cma_configfs.c     | 114 ++++++++++++++-
 drivers/infiniband/core/cq.c               |   6 +
 drivers/infiniband/core/device.c           |  59 +++++++-
 drivers/infiniband/core/fmr_pool.c         |   5 +-
 drivers/infiniband/core/iwcm.c             |  22 +++
 drivers/infiniband/core/iwpm_util.c        |  21 ++-
 drivers/infiniband/core/mad.c              |   7 +
 drivers/infiniband/core/netlink.c          |  23 ++++
 drivers/infiniband/core/roce_gid_cache.c   |   4 +
 drivers/infiniband/core/roce_gid_mgmt.c    |  76 +++++++++-
 drivers/infiniband/core/sa_query.c         |  24 +++-
 drivers/infiniband/core/sysfs.c            | 118 +++++++++++++++-
 drivers/infiniband/core/ucm.c              |  38 +++++
 drivers/infiniband/core/ucma.c             |  76 ++++++++++
 drivers/infiniband/core/umem.c             | 214 +++++++++++++++++++++++++++++
 drivers/infiniband/core/user_mad.c         |  19 +++
 drivers/infiniband/core/uverbs_cmd.c       |  45 ++++++
 drivers/infiniband/core/uverbs_main.c      |  42 ++++++
 drivers/infiniband/hw/mlx4/main.c          |  32 +++++
 drivers/infiniband/hw/mlx4/mlx4_ib.h       |   3 +
 drivers/infiniband/hw/mlx4/mr.c            |  18 +++
 drivers/infiniband/hw/mlx4/qp.c            |   4 +
 drivers/infiniband/hw/qib/qib_user_pages.c |   9 +-
 include/rdma/ib_addr.h                     |  12 ++
 include/rdma/ib_pack.h                     |   4 +
 include/rdma/ib_umem.h                     |   2 +
 include/rdma/ib_umem_odp.h                 |   4 +
 include/rdma/ib_verbs.h                    |  40 ++++++
 include/rdma/peer_mem.h                    |   2 +
 32 files changed, 1322 insertions(+), 25 deletions(-)

--- a/drivers/infiniband/core/addr.c
+++ b/drivers/infiniband/core/addr.c
@@ -136,8 +136,16 @@ static int ib_nl_handle_ip_res_resp(stru
 	const struct nlmsghdr *nlh = (struct nlmsghdr *)cb->nlh;
 
 	if ((nlh->nlmsg_flags & NLM_F_REQUEST) ||
+#ifdef HAVE_NETLINK_CAPABLE
+#ifdef HAVE_NETLINK_SKB_PARMS_SK
 	    !(NETLINK_CB(skb).sk) ||
+#else
+	    !(NETLINK_CB(skb).ssk) ||
+#endif
 	    !netlink_capable(skb, CAP_NET_ADMIN))
+#else
+	    sock_net(skb->sk) != &init_net)
+#endif
 		return -EPERM;
 
 	if (ib_nl_is_good_ip_resp(nlh))
@@ -328,30 +336,48 @@ static void queue_req(struct addr_req *r
 	mutex_unlock(&lock);
 }
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
+#ifdef HAVE_DST_NEIGH_LOOKUP
 static int dst_fetch_ha(struct dst_entry *dst, struct rdma_dev_addr *dev_addr,
 			const void *daddr)
+#else
+static int dst_fetch_ha(struct dst_entry *dst, struct rdma_dev_addr *addr)
+#endif
 {
 	struct neighbour *n;
 	int ret;
 
+#ifdef HAVE_DST_NEIGH_LOOKUP
 	n = dst_neigh_lookup(dst, daddr);
+#endif
 
 	rcu_read_lock();
+#ifndef HAVE_DST_NEIGH_LOOKUP
+	n = dst_get_neighbour(dst);
+#endif
 	if (!n || !(n->nud_state & NUD_VALID)) {
 		if (n)
 			neigh_event_send(n, NULL);
 		ret = -ENODATA;
 	} else {
+#ifdef HAVE_DST_NEIGH_LOOKUP
 		ret = rdma_copy_addr(dev_addr, dst->dev, n->ha);
+#else
+		ret = rdma_copy_addr(addr, dst->dev, n->ha);
+#endif
 	}
 	rcu_read_unlock();
 
+#ifdef HAVE_DST_NEIGH_LOOKUP
 	if (n)
 		neigh_release(n);
+#endif
 
 	return ret;
 }
+#endif
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 static int ib_nl_fetch_ha(struct dst_entry *dst, struct rdma_dev_addr *dev_addr,
 			  const void *daddr, u32 seq, u16 family)
 {
@@ -363,14 +389,18 @@ static int ib_nl_fetch_ha(struct dst_ent
 	return ib_nl_ip_send_msg(dev_addr, daddr, seq, family);
 }
 
-static bool has_gateway(struct dst_entry *dst, sa_family_t family)
+static bool has_gateway(struct dst_entry *dst, const void *daddr, sa_family_t family)
 {
 	struct rtable *rt;
 	struct rt6_info *rt6;
 
 	if (family == AF_INET) {
 		rt = container_of(dst, struct rtable, dst);
+#ifdef HAVE_RT_USES_GATEWAY
 		return rt->rt_uses_gateway;
+#else
+		return (rt->rt_gateway != *(__be32 *)daddr);
+#endif
 	}
 
 	rt6 = container_of(dst, struct rt6_info, dst);
@@ -390,11 +420,20 @@ static int fetch_ha(struct dst_entry *ds
 	sa_family_t family = dst_in->sa_family;
 
 	/* Gateway + ARPHRD_INFINIBAND -> IB router */
-	if (has_gateway(dst, family) && dst->dev->type == ARPHRD_INFINIBAND)
+	if (
+#ifndef HAVE_RT_USES_GATEWAY
+	    seq &&
+#endif
+	    has_gateway(dst, daddr, family) && dst->dev->type == ARPHRD_INFINIBAND)
 		return ib_nl_fetch_ha(dst, dev_addr, daddr, seq, family);
 	else
+#ifdef HAVE_DST_NEIGH_LOOKUP
 		return dst_fetch_ha(dst, dev_addr, daddr);
+#else
+		return  dst_fetch_ha(dst, dev_addr);
+#endif
 }
+#endif
 
 static int addr4_resolve(struct sockaddr_in *src_in,
 			 struct sockaddr_in *dst_in,
@@ -404,9 +443,15 @@ static int addr4_resolve(struct sockaddr
 	__be32 src_ip = src_in->sin_addr.s_addr;
 	__be32 dst_ip = dst_in->sin_addr.s_addr;
 	struct rtable *rt;
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	struct flowi4 fl4;
+#else
+	struct flowi fl;
+	struct neighbour *neigh;
+#endif
 	int ret;
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	memset(&fl4, 0, sizeof(fl4));
 	fl4.daddr = dst_ip;
 	fl4.saddr = src_ip;
@@ -416,10 +461,29 @@ static int addr4_resolve(struct sockaddr
 		ret = PTR_ERR(rt);
 		goto out;
 	}
+#else
+	memset(&fl, 0, sizeof(fl));
+	fl.nl_u.ip4_u.daddr = dst_ip;
+	fl.nl_u.ip4_u.saddr = src_ip;
+	fl.oif = addr->bound_dev_if;
+	ret = ip_route_output_key(&init_net, &rt, &fl);
+	if (ret)
+		goto out;
+#endif
 	src_in->sin_family = AF_INET;
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	src_in->sin_addr.s_addr = fl4.saddr;
 
 	if (rt->dst.dev->flags & IFF_LOOPBACK) {
+#else
+	src_in->sin_addr.s_addr = rt->rt_src;
+
+#ifdef HAVE_RTABLE_IDEV
+	if (rt->idev->dev->flags & IFF_LOOPBACK) {
+#else
+	if (rt->dst.dev->flags & IFF_LOOPBACK) {
+#endif
+#endif
 		ret = rdma_translate_ip((struct sockaddr *)dst_in, addr, NULL);
 		if (!ret)
 			memcpy(addr->dst_dev_addr, addr->src_dev_addr, MAX_ADDR_LEN);
@@ -427,15 +491,62 @@ static int addr4_resolve(struct sockaddr
 	}
 
 	/* If the device does ARP internally, return 'done' */
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	if (rt->dst.dev->flags & IFF_NOARP) {
 		ret = rdma_copy_addr(addr, rt->dst.dev, NULL);
 		goto put;
 	}
 
+#ifdef HAVE_RT_USES_GATEWAY
 	if (rt->rt_uses_gateway && rt->dst.dev->type != ARPHRD_INFINIBAND)
 		addr->network = RDMA_NETWORK_IPV4;
-
+#endif
 	ret = fetch_ha(&rt->dst, addr, (struct sockaddr *)dst_in, seq);
+#else
+#ifdef HAVE_RTABLE_IDEV
+	if (rt->idev->dev->flags & IFF_NOARP) {
+		ret = rdma_copy_addr(addr, rt->idev->dev, NULL);
+#else
+	if (rt->dst.dev->flags & IFF_NOARP) {
+		ret = rdma_copy_addr(addr, rt->dst.dev, NULL);
+#endif
+		goto put;
+	}
+
+	if (seq && !ibnl_chk_listeners(RDMA_NL_GROUP_LS)) {
+#ifdef HAVE_RTABLE_IDEV
+		rdma_copy_addr(addr, rt->idev->dev, NULL);
+#else
+		rdma_copy_addr(addr, rt->dst.dev, NULL);
+#endif
+		ib_nl_ip_send_msg(addr,
+				  (void *)&dst_in->sin_addr,
+				  seq, AF_INET);
+		ret  = -ENODATA;
+		goto put;
+	}
+#ifdef HAVE_RTABLE_IDEV
+		neigh = neigh_lookup(&arp_tbl, &rt->rt_gateway, rt->idev->dev);
+#else
+		neigh = neigh_lookup(&arp_tbl, &rt->rt_gateway, rt->dst.dev);
+#endif
+		if (!neigh || !(neigh->nud_state & NUD_VALID)) {
+#ifdef HAVE_RTABLE_IDEV
+			neigh_event_send(rt->u.dst.neighbour, NULL);
+#else
+			neigh_event_send(rt->dst.neighbour, NULL);
+#endif
+			ret = -ENODATA;
+			if (neigh)
+				goto release;
+			goto put;
+		}
+
+		ret = rdma_copy_addr(addr, neigh->dev, neigh->ha);
+release:
+		neigh_release(neigh);
+#endif
+
 put:
 	ip_rt_put(rt);
 out:
@@ -448,11 +559,17 @@ static int addr6_resolve(struct sockaddr
 			 struct rdma_dev_addr *addr,
 			 u32 seq)
 {
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	struct flowi6 fl6;
+#else
+	struct flowi fl;
+	struct neighbour *neigh;
+#endif
 	struct dst_entry *dst;
 	struct rt6_info *rt;
 	int ret;
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	memset(&fl6, 0, sizeof fl6);
 	fl6.daddr = dst_in->sin6_addr;
 	fl6.saddr = src_in->sin6_addr;
@@ -472,6 +589,27 @@ static int addr6_resolve(struct sockaddr
 		src_in->sin6_family = AF_INET6;
 		src_in->sin6_addr = fl6.saddr;
 	}
+#else
+	memset(&fl, 0, sizeof fl);
+	ipv6_addr_copy(&fl.fl6_dst, &dst_in->sin6_addr);
+	ipv6_addr_copy(&fl.fl6_src, &src_in->sin6_addr);
+	fl.oif = addr->bound_dev_if;
+
+	dst = ip6_route_output(&init_net, NULL, &fl);
+	if ((ret = dst->error))
+		goto put;
+
+	rt = (struct rt6_info *)dst;
+	if (ipv6_addr_any(&fl.fl6_src)) {
+		ret = ipv6_dev_get_saddr(&init_net, ip6_dst_idev(dst)->dev,
+					 &fl.fl6_dst, 0, &fl.fl6_src);
+		if (ret)
+			goto put;
+
+		src_in->sin6_family = AF_INET6;
+		ipv6_addr_copy(&src_in->sin6_addr, &fl.fl6_src);
+	}
+#endif
 
 	if (dst->dev->flags & IFF_LOOPBACK) {
 		ret = rdma_translate_ip((struct sockaddr *)dst_in, addr, NULL);
@@ -489,7 +627,33 @@ static int addr6_resolve(struct sockaddr
 	if (rt->rt6i_flags & RTF_GATEWAY && dst->dev->type != ARPHRD_INFINIBAND)
 		addr->network = RDMA_NETWORK_IPV6;
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	ret = fetch_ha(dst, addr, (struct sockaddr *)dst_in, seq);
+#else
+	if (rt->rt6i_flags & RTF_GATEWAY &&
+	    dst->dev->type == ARPHRD_INFINIBAND) {
+		if (!ibnl_chk_listeners(RDMA_NL_GROUP_LS)) {
+			rdma_copy_addr(addr, dst->dev, NULL);
+			ib_nl_ip_send_msg(addr,
+					  (void *)&dst_in->sin6_addr,
+					  seq, AF_INET6);
+			ret = -ENODATA;
+		} else {
+			ret = -EINVAL;
+		}
+		goto put;
+	}
+
+	neigh = dst->neighbour;
+	if (!neigh || !(neigh->nud_state & NUD_VALID)) {
+		neigh_event_send(dst->neighbour, NULL);
+		ret = -ENODATA;
+		goto put;
+	}
+
+	ret = rdma_copy_addr(addr, dst->dev, neigh->ha);
+#endif
+
 put:
 	dst_release(dst);
 	return ret;
@@ -532,9 +696,22 @@ static void process_req(struct work_stru
 			dst_in = (struct sockaddr *) &req->dst_addr;
 			req->status = addr_resolve(src_in, dst_in, req->addr,
 						   req->seq);
-			if (req->status && time_after_eq(jiffies, req->timeout))
-				req->status = -ETIMEDOUT;
-			else if (req->status == -ENODATA)
+			if (req->status && time_after_eq(jiffies, req->timeout)) {
+#ifndef HAVE_RT_USES_GATEWAY
+				if (req->seq &&
+				    !ibnl_chk_listeners(RDMA_NL_GROUP_LS)) {
+					req->seq = 0;
+					req->status = addr_resolve(src_in, dst_in, req->addr,
+								   req->seq);
+					if (req->status) {
+						req->timeout = msecs_to_jiffies(2000) +
+							jiffies;
+						continue;
+					}
+				} else
+#endif
+					req->status = -ETIMEDOUT;
+			} else if (req->status == -ENODATA)
 				continue;
 		}
 		list_move_tail(&req->list, &done_list);
@@ -591,6 +768,10 @@ int rdma_resolve_ip(struct rdma_addr_cli
 	req->client = client;
 	atomic_inc(&client->refcount);
 	req->seq = (u32)atomic_inc_return(&ib_nl_addr_request_seq);
+#ifndef HAVE_RT_USES_GATEWAY
+	if (!req->seq)
+		req->seq = (u32)atomic_inc_return(&ib_nl_addr_request_seq);
+#endif
 
 	req->status = addr_resolve(src_in, dst_in, addr, req->seq);
 	switch (req->status) {
--- a/drivers/infiniband/core/cm.c
+++ b/drivers/infiniband/core/cm.c
@@ -448,6 +448,7 @@ static int cm_init_av_by_path(struct ib_
 
 static int cm_alloc_id(struct cm_id_private *cm_id_priv)
 {
+#ifdef HAVE_IDR_ALLOC_CYCLIC
 	unsigned long flags;
 	int id;
 
@@ -461,6 +462,24 @@ static int cm_alloc_id(struct cm_id_priv
 
 	cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
 	return id < 0 ? id : 0;
+#else
+	unsigned long flags;
+	int ret, id;
+	static int next_id;
+
+	do {
+		spin_lock_irqsave(&cm.lock, flags);
+		ret = idr_get_new_above(&cm.local_id_table, cm_id_priv,
+					next_id, &id);
+		if (!ret)
+			next_id = max(id + 1, 0);
+
+		spin_unlock_irqrestore(&cm.lock, flags);
+	} while( (ret == -EAGAIN) && idr_pre_get(&cm.local_id_table, GFP_KERNEL) );
+
+	cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
+	return ret;
+#endif
 }
 
 static void cm_free_id(__be32 local_id)
@@ -3818,7 +3837,11 @@ static ssize_t cm_show_counter(struct ko
 		       atomic_long_read(&group->counter[cm_attr->index]));
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops cm_counter_ops = {
+#else
+static struct sysfs_ops cm_counter_ops = {
+#endif
 	.show = cm_show_counter
 };
 
@@ -3839,7 +3862,11 @@ static struct kobj_type cm_port_obj_type
 	.release = cm_release_port_obj
 };
 
+#ifdef HAVE_CLASS_DEVNODE_UMODE_T
 static char *cm_devnode(struct device *dev, umode_t *mode)
+#else
+static char *cm_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	if (mode)
 		*mode = 0666;
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -2209,20 +2209,39 @@ static int cma_resolve_iw_route(struct r
 	return 0;
 }
 
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 static u8 iboe_tos_to_sl(struct net_device *ndev, u8 tos)
+#else
+static u8 iboe_tos_to_sl(struct ib_device *ibdev, u8 port_num,
+			 struct net_device *ndev, u8 tos)
+#endif
 {
 	char  skprio;
+#if defined(HAVE_NETDEV_GET_PRIO_TC_MAP)
 	struct net_device *real;
-
-	skprio = rt_tos2priority(tos);
 	real = ndev->priv_flags & IFF_802_1Q_VLAN ?
 		vlan_dev_real_dev(ndev) : ndev;
+#endif
+	skprio = rt_tos2priority(tos);
 
-	if (ndev->priv_flags & IFF_802_1Q_VLAN)
+	if (ndev->priv_flags & IFF_802_1Q_VLAN) {
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 		return (vlan_dev_get_egress_qos_mask(ndev, skprio) &
 			VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
+	}
+#else
+		u8 up;
 
+		if (!ib_get_skprio2up(ibdev, port_num, skprio, &up))
+			return up;
+	}
+#endif
+
+#if defined(HAVE_NETDEV_GET_PRIO_TC_MAP)
 	return netdev_get_prio_tc_map(real, skprio);
+#else
+	return 0;
+#endif
 }
 
 static enum ib_gid_type cma_route_gid_type(enum rdma_network_type
@@ -2300,7 +2319,13 @@ static int cma_resolve_iboe_route(struct
 	route->path_rec->reversible = 1;
 	route->path_rec->pkey = cpu_to_be16(0xffff);
 	route->path_rec->mtu_selector = IB_SA_EQ;
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 	route->path_rec->sl = iboe_tos_to_sl(ndev, id_priv->tos);
+#else
+	route->path_rec->sl = iboe_tos_to_sl(id_priv->id.device,
+					     id_priv->id.port_num,
+					     ndev, id_priv->tos);
+#endif
 	route->path_rec->mtu = iboe_get_mtu(ndev->mtu);
 	route->path_rec->rate_selector = IB_SA_EQ;
 	route->path_rec->rate = iboe_get_rate(ndev);
@@ -2713,6 +2738,7 @@ static int cma_alloc_port(struct idr *ps
 			  unsigned short snum)
 {
 	struct rdma_bind_list *bind_list;
+#ifdef HAVE_IDR_ALLOC
 	int ret;
 
 	bind_list = kzalloc(sizeof *bind_list, GFP_KERNEL);
@@ -2730,6 +2756,35 @@ static int cma_alloc_port(struct idr *ps
 err:
 	kfree(bind_list);
 	return ret == -ENOSPC ? -EADDRNOTAVAIL : ret;
+#else
+	int port, ret;
+
+	bind_list = kzalloc(sizeof *bind_list, GFP_KERNEL);
+	if (!bind_list)
+		return -ENOMEM;
+
+	do {
+		ret = idr_get_new_above(ps, bind_list, snum, &port);
+	} while ((ret == -EAGAIN) && idr_pre_get(ps, GFP_KERNEL));
+
+	if (ret)
+		goto err1;
+
+	if (port != snum) {
+		ret = -EADDRNOTAVAIL;
+		goto err2;
+	}
+
+	bind_list->ps = ps;
+	bind_list->port = (unsigned short) port;
+	cma_bind_port(bind_list, id_priv);
+	return 0;
+err2:
+	idr_remove(ps, port);
+err1:
+	kfree(bind_list);
+	return ret;
+#endif
 }
 
 static int cma_alloc_any_port(struct idr *ps, struct rdma_id_private *id_priv)
@@ -2738,7 +2793,11 @@ static int cma_alloc_any_port(struct idr
 	int low, high, remaining;
 	unsigned int rover;
 
+#ifdef HAVE_INET_GET_LOCAL_PORT_RANGE_3_PARAMS
 	inet_get_local_port_range(&init_net, &low, &high);
+#else
+	inet_get_local_port_range(&low, &high);
+#endif
 	remaining = (high - low) + 1;
 	rover = prandom_u32() % remaining + low;
 retry:
@@ -2774,9 +2833,12 @@ static int cma_check_port(struct rdma_bi
 {
 	struct rdma_id_private *cur_id;
 	struct sockaddr *addr, *cur_addr;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	addr = cma_src_addr(id_priv);
-	hlist_for_each_entry(cur_id, &bind_list->owners, node) {
+	compat_hlist_for_each_entry(cur_id, &bind_list->owners, node) {
 		if (id_priv == cur_id)
 			continue;
 
@@ -3681,7 +3743,11 @@ static int cma_join_ib_multicast(struct
 						id_priv->id.port_num, &rec,
 						comp_mask, GFP_KERNEL,
 						cma_ib_mc_handler, mc);
+#ifdef HAVE_PTR_ERR_OR_ZERO
 	return PTR_ERR_OR_ZERO(mc->multicast.ib);
+#else
+	return PTR_RET(mc->multicast.ib);
+#endif
 }
 
 static void iboe_mcast_work_handler(struct work_struct *work)
@@ -3910,6 +3976,7 @@ static int cma_netdev_change(struct net_
 	struct cma_ndev_work *work;
 	enum rdma_link_layer dev_ll;
 	struct net_device *bounded_dev;
+	struct net_device *real_bounded;
 	work_func_t work_func;
 
 	dev_addr = &id_priv->id.route.addr.dev_addr;
@@ -3942,9 +4009,16 @@ static int cma_netdev_change(struct net_
 		bounded_dev = __dev_get_by_index(&init_net, dev_addr->bound_dev_if);
 		if (!bounded_dev)
 			return 0;
+		real_bounded = rdma_vlan_dev_real_dev(bounded_dev);
+		real_bounded = real_bounded ? real_bounded : bounded_dev;
 
-		if (!netdev_has_upper_dev(ndev, bounded_dev))
+#ifdef HAVE_NETDEV_HAS_UPPER_DEV
+		if (!netdev_has_upper_dev(ndev, real_bounded))
 			return 0;
+#else
+		if (real_bounded != ndev->master)
+			return 0;
+#endif
 
 		work_func = cma_ndev_device_remove_work_handler;
 		break;
--- a/drivers/infiniband/core/cma_configfs.c
+++ b/drivers/infiniband/core/cma_configfs.c
@@ -35,6 +35,10 @@
 #include <rdma/ib_verbs.h>
 #include "core_priv.h"
 
+#ifndef CONFIGFS_ATTR
+#define HAVE_OLD_CONFIGFS_API
+#endif
+
 struct cma_device;
 
 struct cma_dev_group;
@@ -52,6 +56,23 @@ struct cma_dev_group {
 	struct cma_dev_port_group	*ports;
 };
 
+#ifdef HAVE_OLD_CONFIGFS_API
+struct cma_configfs_attr {
+	struct configfs_attribute	attr;
+	ssize_t				(*show)(struct config_item *item,
+						char *buf);
+	ssize_t				(*store)(struct config_item *item,
+						 const char *buf, size_t count);
+};
+#define CONFIGFS_ATTR(dummy, _name)				\
+static struct cma_configfs_attr attr_##_name =	\
+	__CONFIGFS_ATTR(_name, S_IRUGO | S_IWUSR, _name##_show, _name##_store)
+
+#define CONFIGFS_ATTR_ADD(name) &name.attr
+#else
+#define CONFIGFS_ATTR_ADD(name) &name
+#endif /* HAVE_OLD_CONFIGFS_API */
+
 static struct cma_dev_port_group *to_dev_port_group(struct config_item *item)
 {
 	struct config_group *group;
@@ -68,6 +89,34 @@ static bool filter_by_name(struct ib_dev
 	return !strcmp(ib_dev->name, cookie);
 }
 
+#ifdef HAVE_OLD_CONFIGFS_API
+static ssize_t cma_configfs_attr_show(struct config_item *item,
+				      struct configfs_attribute *attr,
+				      char *buf)
+{
+	struct cma_configfs_attr *ca =
+		container_of(attr, struct cma_configfs_attr, attr);
+
+	if (ca->show)
+		return ca->show(item, buf);
+
+	return -EINVAL;
+}
+
+static ssize_t cma_configfs_attr_store(struct config_item *item,
+				       struct configfs_attribute *attr,
+				       const char *buf, size_t count)
+{
+	struct cma_configfs_attr *ca =
+		container_of(attr, struct cma_configfs_attr, attr);
+
+	if (ca->store)
+		return ca->store(item, buf, count);
+
+	return -EINVAL;
+}
+#endif /* HAVE_OLD_CONFIGFS_API */
+
 static int cma_configfs_params_get(struct config_item *item,
 				   struct cma_device **pcma_dev,
 				   struct cma_dev_port_group **pgroup)
@@ -140,12 +189,19 @@ static ssize_t default_roce_mode_store(s
 CONFIGFS_ATTR(, default_roce_mode);
 
 static struct configfs_attribute *cma_configfs_attributes[] = {
-	&attr_default_roce_mode,
+	CONFIGFS_ATTR_ADD(attr_default_roce_mode),
 	NULL,
 };
 
+#ifdef HAVE_OLD_CONFIGFS_API
+static struct configfs_item_operations cma_item_ops = {
+	.show_attribute		= cma_configfs_attr_show,
+	.store_attribute	= cma_configfs_attr_store,
+};
+#else /* HAVE_OLD_CONFIGFS_API */
 static struct configfs_item_operations cma_item_ops = {
 };
+#endif
 
 static struct config_item_type cma_port_group_type = {
 	.ct_item_ops	= &cma_item_ops,
@@ -176,6 +232,13 @@ static int make_cma_ports(struct cma_dev
 		goto free;
 	}
 
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+	cma_dev_group->ports_group.default_groups = kcalloc((num_ports + 1),
+							    sizeof(struct config_group *),
+							    GFP_KERNEL);
+	if (!cma_dev_group->ports_group.default_groups)
+		goto free;
+#endif
 	for (i = 0; i < num_ports; i++) {
 		char port_str[10];
 
@@ -185,9 +248,16 @@ static int make_cma_ports(struct cma_dev
 		config_group_init_type_name(&ports[i].group,
 					    port_str,
 					    &cma_port_group_type);
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
 		configfs_add_default_group(&ports[i].group,
 					   &cma_dev_group->ports_group);
+#else
+		cma_dev_group->ports_group.default_groups[i] = &ports[i].group;
+#endif
 	}
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+	cma_dev_group->ports_group.default_groups[i] = NULL;
+#endif
 	cma_dev_group->ports = ports;
 
 	return 0;
@@ -253,6 +323,14 @@ static struct config_group *make_cma_dev
 		err = -ENOMEM;
 		goto fail;
 	}
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+	cma_dev_group->device_group.default_groups = kzalloc(sizeof(struct config_group *) * 2,
+							     GFP_KERNEL);
+	if (!cma_dev_group->device_group.default_groups) {
+		err = -ENOMEM;
+		goto fail;
+	}
+#endif
 
 	strncpy(cma_dev_group->name, name, sizeof(cma_dev_group->name));
 
@@ -261,16 +339,29 @@ static struct config_group *make_cma_dev
 
 	err = make_cma_ports(cma_dev_group, cma_dev);
 	if (err)
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
 		goto fail;
+#else
+		goto fail_free;
+#endif
 
 	config_group_init_type_name(&cma_dev_group->device_group, name,
 				    &cma_device_group_type);
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
 	configfs_add_default_group(&cma_dev_group->ports_group,
 				   &cma_dev_group->device_group);
+#else
+	cma_dev_group->device_group.default_groups[0] = &cma_dev_group->ports_group;
+	cma_dev_group->device_group.default_groups[1] = NULL;
+#endif
 
 	cma_deref_dev(cma_dev);
 	return &cma_dev_group->device_group;
 
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+fail_free:
+	kfree(cma_dev_group->device_group.default_groups);
+#endif
 fail:
 	if (cma_dev)
 		cma_deref_dev(cma_dev);
@@ -286,8 +377,29 @@ static void drop_cma_dev(struct config_g
 	struct cma_dev_group *cma_dev_group = container_of(group,
 							   struct cma_dev_group,
 							   device_group);
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
 	configfs_remove_default_groups(&cma_dev_group->ports_group);
 	configfs_remove_default_groups(&cma_dev_group->device_group);
+#else
+	struct config_item *temp_item;
+	int i;
+
+	for (i = 0; cma_dev_group->ports_group.default_groups[i]; i++) {
+		temp_item =
+			&cma_dev_group->ports_group.default_groups[i]->cg_item;
+		cma_dev_group->ports_group.default_groups[i] = NULL;
+		config_item_put(temp_item);
+	}
+	kfree(cma_dev_group->ports_group.default_groups);
+
+	for (i = 0; cma_dev_group->device_group.default_groups[i]; i++) {
+		temp_item =
+			&cma_dev_group->device_group.default_groups[i]->cg_item;
+		cma_dev_group->device_group.default_groups[i] = NULL;
+		config_item_put(temp_item);
+	}
+	kfree(cma_dev_group->device_group.default_groups);
+#endif
 	config_item_put(item);
 }
 
--- a/drivers/infiniband/core/cq.c
+++ b/drivers/infiniband/core/cq.c
@@ -74,6 +74,7 @@ static void ib_cq_completion_direct(stru
 	WARN_ONCE(1, "got unsolicited completion for CQ 0x%p\n", cq);
 }
 
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 static int ib_poll_handler(struct irq_poll *iop, int budget)
 {
 	struct ib_cq *cq = container_of(iop, struct ib_cq, iop);
@@ -93,6 +94,7 @@ static void ib_cq_completion_softirq(str
 {
 	irq_poll_sched(&cq->iop);
 }
+#endif
 
 static void ib_cq_poll_work(struct work_struct *work)
 {
@@ -152,12 +154,14 @@ struct ib_cq *ib_alloc_cq(struct ib_devi
 	case IB_POLL_DIRECT:
 		cq->comp_handler = ib_cq_completion_direct;
 		break;
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 	case IB_POLL_SOFTIRQ:
 		cq->comp_handler = ib_cq_completion_softirq;
 
 		irq_poll_init(&cq->iop, IB_POLL_BUDGET_IRQ, ib_poll_handler);
 		ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);
 		break;
+#endif
 	case IB_POLL_WORKQUEUE:
 		cq->comp_handler = ib_cq_completion_workqueue;
 		INIT_WORK(&cq->work, ib_cq_poll_work);
@@ -192,9 +196,11 @@ void ib_free_cq(struct ib_cq *cq)
 	switch (cq->poll_ctx) {
 	case IB_POLL_DIRECT:
 		break;
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 	case IB_POLL_SOFTIRQ:
 		irq_poll_disable(&cq->iop);
 		break;
+#endif
 	case IB_POLL_WORKQUEUE:
 		flush_work(&cq->work);
 		break;
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -313,6 +313,9 @@ int ib_register_device(struct ib_device
 		goto out;
 	}
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	mutex_init(&device->skprio2up.lock);
+#endif
 	INIT_LIST_HEAD(&device->event_handler_list);
 	INIT_LIST_HEAD(&device->client_data_list);
 	spin_lock_init(&device->event_handler_lock);
@@ -885,6 +888,46 @@ int ib_find_pkey(struct ib_device *devic
 }
 EXPORT_SYMBOL(ib_find_pkey);
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+int ib_set_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    up >= NUM_UP ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	device->skprio2up.map[port_num - 1][prio] = up;
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_set_skprio2up);
+
+int ib_get_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 *up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    !up ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	*up = device->skprio2up.map[port_num - 1][prio];
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_get_skprio2up);
+#endif
+
 static int __init ib_core_init(void)
 {
 	int ret;
@@ -894,8 +937,22 @@ static int __init ib_core_init(void)
 		return -ENOMEM;
 
 	ib_comp_wq = alloc_workqueue("ib-comp-wq",
-			WQ_UNBOUND | WQ_HIGHPRI | WQ_MEM_RECLAIM,
+			0
+#if defined(HAVE_WQ_UNBOUND)
+			| WQ_UNBOUND
+#endif
+#if defined(HAVE_WQ_HIGHPRI)
+			| WQ_HIGHPRI
+#endif
+#if defined(HAVE_WQ_MEM_RECLAIM)
+			| WQ_MEM_RECLAIM
+#endif
+			,
+#if defined(HAVE_WQ_UNBOUND_MAX_ACTIVE)
 			WQ_UNBOUND_MAX_ACTIVE);
+#else
+			0);
+#endif
 	if (!ib_comp_wq) {
 		ret = -ENOMEM;
 		goto err;
--- a/drivers/infiniband/core/fmr_pool.c
+++ b/drivers/infiniband/core/fmr_pool.c
@@ -118,13 +118,16 @@ static inline struct ib_pool_fmr *ib_fmr
 {
 	struct hlist_head *bucket;
 	struct ib_pool_fmr *fmr;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	if (!pool->cache_bucket)
 		return NULL;
 
 	bucket = pool->cache_bucket + ib_fmr_hash(*page_list);
 
-	hlist_for_each_entry(fmr, bucket, cache_node)
+	compat_hlist_for_each_entry(fmr, bucket, cache_node)
 		if (io_virtual_address == fmr->io_virtual_address &&
 		    page_list_len      == fmr->page_list_len      &&
 		    !memcmp(page_list, fmr->page_list,
--- a/drivers/infiniband/core/iwcm.c
+++ b/drivers/infiniband/core/iwcm.c
@@ -68,6 +68,7 @@ struct iwcm_work {
 
 static unsigned int default_backlog = 256;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table_header *iwcm_ctl_table_hdr;
 static struct ctl_table iwcm_ctl_table[] = {
 	{
@@ -80,6 +81,15 @@ static struct ctl_table iwcm_ctl_table[]
 	{ }
 };
 
+#ifndef HAVE_REGISTER_NET_SYSCTL
+static struct ctl_path iwcm_ctl_path[] = {
+    { .procname = "net" },
+    { .procname = "iw_cm" },
+    { }
+};
+#endif
+#endif
+
 /*
  * The following services provide a mechanism for pre-allocating iwcm_work
  * elements.  The design pre-allocates them  based on the cm_id type:
@@ -1048,20 +1058,32 @@ static int __init iw_cm_init(void)
 	if (!iwcm_wq)
 		return -ENOMEM;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	iwcm_ctl_table_hdr = register_net_sysctl(&init_net, "net/iw_cm",
 						 iwcm_ctl_table);
+#else
+	iwcm_ctl_table_hdr = register_sysctl_paths(iwcm_ctl_path, iwcm_ctl_table);
+#endif
 	if (!iwcm_ctl_table_hdr) {
 		pr_err("iw_cm: couldn't register sysctl paths\n");
 		destroy_workqueue(iwcm_wq);
 		return -ENOMEM;
 	}
+#endif
 
 	return 0;
 }
 
 static void __exit iw_cm_cleanup(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	unregister_net_sysctl_table(iwcm_ctl_table_hdr);
+#else
+	unregister_sysctl_table(iwcm_ctl_table_hdr);
+#endif
+#endif
 	destroy_workqueue(iwcm_wq);
 }
 
--- a/drivers/infiniband/core/iwpm_util.c
+++ b/drivers/infiniband/core/iwpm_util.c
@@ -131,6 +131,9 @@ int iwpm_remove_mapinfo(struct sockaddr_
 	struct hlist_node *tmp_hlist_node;
 	struct hlist_head *hash_bucket_head;
 	struct iwpm_mapping_info *map_info = NULL;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	unsigned long flags;
 	int ret = -EINVAL;
 
@@ -139,9 +142,8 @@ int iwpm_remove_mapinfo(struct sockaddr_
 		hash_bucket_head = get_hash_bucket_head(
 					local_sockaddr,
 					mapped_local_addr);
-		hlist_for_each_entry_safe(map_info, tmp_hlist_node,
+		compat_hlist_for_each_entry_safe(map_info, tmp_hlist_node,
 					hash_bucket_head, hlist_node) {
-
 			if (!iwpm_compare_sockaddr(&map_info->mapped_sockaddr,
 						mapped_local_addr)) {
 
@@ -161,15 +163,17 @@ static void free_hash_bucket(void)
 {
 	struct hlist_node *tmp_hlist_node;
 	struct iwpm_mapping_info *map_info;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	unsigned long flags;
 	int i;
 
 	/* remove all the mapinfo data from the list */
 	spin_lock_irqsave(&iwpm_mapinfo_lock, flags);
 	for (i = 0; i < IWPM_HASH_BUCKET_SIZE; i++) {
-		hlist_for_each_entry_safe(map_info, tmp_hlist_node,
+		compat_hlist_for_each_entry_safe(map_info, tmp_hlist_node,
 			&iwpm_hash_bucket[i], hlist_node) {
-
 				hlist_del_init(&map_info->hlist_node);
 				kfree(map_info);
 			}
@@ -348,7 +352,11 @@ int iwpm_parse_nlmsg(struct netlink_call
 	int ret;
 	const char *err_str = "";
 
+#ifndef CONFIG_COMPAT_IS_NLMSG_VALIDATE_NOT_CONST_NLMSGHDR
 	ret = nlmsg_validate(cb->nlh, nlh_len, policy_max-1, nlmsg_policy);
+#else
+	ret = nlmsg_validate((struct nlmsghdr *)cb->nlh, nlh_len, policy_max-1, nlmsg_policy);
+#endif
 	if (ret) {
 		err_str = "Invalid attribute";
 		goto parse_nlmsg_error;
@@ -498,6 +506,9 @@ int iwpm_send_mapinfo(u8 nl_client, int
 	struct iwpm_mapping_info *map_info;
 	struct sk_buff *skb = NULL;
 	struct nlmsghdr *nlh;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	int skb_num = 0, mapping_num = 0;
 	int i = 0, nlmsg_bytes = 0;
 	unsigned long flags;
@@ -513,7 +524,7 @@ int iwpm_send_mapinfo(u8 nl_client, int
 	skb_num++;
 	spin_lock_irqsave(&iwpm_mapinfo_lock, flags);
 	for (i = 0; i < IWPM_HASH_BUCKET_SIZE; i++) {
-		hlist_for_each_entry(map_info, &iwpm_hash_bucket[i],
+		compat_hlist_for_each_entry(map_info, &iwpm_hash_bucket[i],
 				     hlist_node) {
 			if (map_info->nl_client != nl_client)
 				continue;
--- a/drivers/infiniband/core/mad.c
+++ b/drivers/infiniband/core/mad.c
@@ -34,6 +34,9 @@
  *
  */
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/dma-mapping.h>
@@ -3940,7 +3943,11 @@ static ssize_t sa_cc_attr_show(struct ko
 	return sa->show(cc_obj, buf);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops sa_cc_sysfs_ops = {
+#else
+static struct sysfs_ops sa_cc_sysfs_ops = {
+#endif
 	.show = sa_cc_attr_show,
 	.store = sa_cc_attr_store,
 };
--- a/drivers/infiniband/core/netlink.c
+++ b/drivers/infiniband/core/netlink.c
@@ -30,6 +30,9 @@
  * SOFTWARE.
  */
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) "%s:%s: " fmt, KBUILD_MODNAME, __func__
 
 #include <linux/export.h>
@@ -189,18 +192,29 @@ static int ibnl_rcv_msg(struct sk_buff *
 					.skb = skb,
 					.nlh = nlh,
 					.dump = client->cb_table[op].dump,
+#if defined(HAVE_NETLINK_CALLBACK_MODULE)
 					.module = client->cb_table[op].module,
+#endif
 				};
 
 				return cb.dump(skb, &cb);
 			}
 
 			{
+#if (defined(HAVE_NETLINK_DUMP_CONTROL_DUMP) || \
+     defined(HAVE_NETLINK_DUMP_CONTROL_MODULE))
 				struct netlink_dump_control c = {
 					.dump = client->cb_table[op].dump,
+#if defined(HAVE_NETLINK_DUMP_CONTROL_MODULE)
 					.module = client->cb_table[op].module,
+#endif
 				};
 				return netlink_dump_start(nls, skb, nlh, &c);
+#else /* have netlink_dump_control */
+				return netlink_dump_start(nls, skb, nlh,
+							  client->cb_table[op].dump,
+							  NULL, 0);
+#endif
 			}
 		}
 	}
@@ -262,11 +276,20 @@ EXPORT_SYMBOL(ibnl_multicast);
 
 int __init ibnl_init(void)
 {
+#ifdef HAVE_NETLINK_KERNEL_CFG_INPUT
 	struct netlink_kernel_cfg cfg = {
 		.input	= ibnl_rcv,
 	};
 
+#ifdef HAVE_NETLINK_KERNEL_CREATE_3_PARAMS
 	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, &cfg);
+#else
+	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, THIS_MODULE, &cfg);
+#endif
+#else /* HAVE_NETLINK_KERNEL_CFG_INPUT */
+	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, 0, ibnl_rcv,
+				    NULL, THIS_MODULE);
+#endif /* HAVE_NETLINK_KERNEL_CFG_INPUT */
 	if (!nls) {
 		pr_warn("Failed to create netlink socket\n");
 		return -ENOMEM;
--- a/drivers/infiniband/core/roce_gid_cache.c
+++ b/drivers/infiniband/core/roce_gid_cache.c
@@ -435,7 +435,11 @@ static int get_netdev_from_ifindex(struc
 {
 	if (if_index && net) {
 		rcu_read_lock();
+#ifdef HAVE_DEV_GET_BY_INDEX_RCU
 		gid_attr_val->ndev = dev_get_by_index_rcu(net, if_index);
+#else
+		gid_attr_val->ndev = __dev_get_by_index(net, if_index);
+#endif
 		rcu_read_unlock();
 		if (gid_attr_val->ndev)
 			return GID_ATTR_FIND_MASK_NETDEV;
--- a/drivers/infiniband/core/roce_gid_mgmt.c
+++ b/drivers/infiniband/core/roce_gid_mgmt.c
@@ -152,6 +152,7 @@ static enum bonding_slave_state is_eth_a
 							       struct net_device *upper)
 {
 	if (upper && IS_NETDEV_BONDING_MASTER(upper)) {
+#ifdef HAVE_BONDING_H
 		struct net_device *pdev;
 
 		rcu_read_lock();
@@ -160,6 +161,10 @@ static enum bonding_slave_state is_eth_a
 		if (pdev)
 			return idev == pdev ? BONDING_SLAVE_STATE_ACTIVE :
 				BONDING_SLAVE_STATE_INACTIVE;
+#else
+	return memcmp(upper->dev_addr, idev->dev_addr, ETH_ALEN) ?
+		BONDING_SLAVE_STATE_INACTIVE : BONDING_SLAVE_STATE_ACTIVE;
+#endif
 	}
 
 	return BONDING_SLAVE_STATE_NA;
@@ -167,6 +172,8 @@ static enum bonding_slave_state is_eth_a
 
 static bool is_upper_dev_rcu(struct net_device *dev, struct net_device *upper)
 {
+	bool ret;
+#ifdef HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU
 	struct net_device *_upper = NULL;
 	struct list_head *iter;
 
@@ -175,9 +182,30 @@ static bool is_upper_dev_rcu(struct net_
 		if (_upper == upper)
 			break;
 	}
+	ret = (_upper == upper);
+	rcu_read_unlock();
+#else
+	{
+		struct net_device *rdev_upper = rdma_vlan_dev_real_dev(upper);
+		struct net_device *master;
 
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU
+	rcu_read_lock();
+#endif
+	master = netdev_master_upper_dev_get_rcu(dev);
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU
 	rcu_read_unlock();
-	return _upper == upper;
+#endif
+		if (!upper || !dev)
+			ret = false;
+		else
+			ret = (upper == master) ||
+			      (rdev_upper && (rdev_upper == master)) ||
+			      (rdev_upper == dev);
+	}
+
+#endif
+	return ret;
 }
 
 static int _is_eth_port_of_netdev(struct ib_device *ib_dev, u8 port,
@@ -221,11 +249,15 @@ static int is_eth_port_inactive_slave(st
 	if (!idev)
 		return 0;
 
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU
 	rcu_read_lock();
+#endif
 	mdev = netdev_master_upper_dev_get_rcu(idev);
 	res = is_eth_active_slave_of_bonding(idev, mdev) ==
 		BONDING_SLAVE_STATE_INACTIVE;
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU
 	rcu_read_unlock();
+#endif
 
 	return res;
 }
@@ -407,7 +439,11 @@ static void enum_netdev_ipv6_ips(struct
 		return;
 
 	read_lock_bh(&in6_dev->lock);
+#ifdef HAVE_INET6_IF_LIST
 	list_for_each_entry(ifp, &in6_dev->addr_list, if_list) {
+#else
+	for (ifp=in6_dev->addr_list; ifp; ifp=ifp->if_next) {
+#endif
 		struct sin6_list *entry = kzalloc(sizeof(*entry), GFP_ATOMIC);
 
 		if (!entry) {
@@ -454,9 +490,11 @@ static void del_netdev_ips(struct ib_dev
 	roce_del_all_netdev_gids(ib_dev, port, ndev);
 }
 
+#ifdef HAVE_NETDEV_CHANGEUPPER
 static void del_netdev_upper_ips(struct ib_device *ib_dev, u8 port,
 				 struct net_device *idev, void *cookie)
 {
+#if defined(HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU) || defined(HAVE_NETDEV_HAS_UPPER_DEV)
 	struct net_device *ndev = (struct net_device *)cookie;
 	struct net_device *rdev = rdma_vlan_dev_real_dev(ndev);
 
@@ -464,8 +502,10 @@ static void del_netdev_upper_ips(struct
 		rdev = ndev;
 
 	if (idev == rdev) {
-		struct net_device *upper;
+		struct net_device *upper = NULL;
+#ifdef HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU
 		struct list_head *iter;
+#endif
 		struct roce_netdev_list *upper_iter;
 		struct roce_netdev_list *upper_temp;
 		LIST_HEAD(upper_list);
@@ -479,6 +519,7 @@ static void del_netdev_upper_ips(struct
 			e->ndev = idev;
 			dev_hold(idev);
 		}
+#ifdef HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU
 		rcu_read_lock();
 		netdev_for_each_all_upper_dev_rcu(idev, upper, iter) {
 			struct roce_netdev_list *entry = kmalloc(sizeof(*entry),
@@ -494,7 +535,26 @@ static void del_netdev_upper_ips(struct
 			entry->ndev = upper;
 		}
 		rcu_read_unlock();
-
+#else
+		rtnl_lock();
+		for_each_netdev(&init_net, upper) {
+
+			if (is_upper_dev_rcu(idev, upper)) {
+				struct roce_netdev_list *entry = kmalloc(sizeof(*entry),
+						GFP_ATOMIC);
+
+				if (!entry) {
+					pr_info("roce_gid_mgmt: couldn't allocate entry to delete ndev\n");
+					continue;
+				}
+
+				list_add_tail(&entry->list, &upper_list);
+				dev_hold(upper);
+				entry->ndev = upper;
+			}
+		}
+		rtnl_unlock();
+#endif
 		roce_sync_all_netdev_gids(ib_dev, port, &upper_list);
 
 		list_for_each_entry_safe(upper_iter, upper_temp, &upper_list,
@@ -504,7 +564,9 @@ static void del_netdev_upper_ips(struct
 			kfree(upper_iter);
 		}
 	}
+#endif
 }
+#endif
 
 static void del_netdev_default_ips(struct ib_device *ib_dev, u8 port,
 				   struct net_device *idev, void *cookie)
@@ -546,8 +608,10 @@ static int netdevice_event(struct notifi
 		.cb = del_netdev_default_ips, .filter = is_eth_port_inactive_slave};
 	static const struct netdev_event_work_cmd bonding_event_ips_del_cmd = {
 		.cb = del_netdev_ips, .filter = bonding_slaves_filter};
+#ifdef HAVE_NETDEV_CHANGEUPPER
 	static const struct netdev_event_work_cmd upper_ips_del_cmd = {
 		.cb = del_netdev_upper_ips, .filter = pass_all_filter};
+#endif
 	struct net_device *ndev = netdev_notifier_info_to_dev(ptr);
 	struct netdev_event_work *ndev_work;
 	struct netdev_event_work_cmd cmds[ROCE_NETDEV_CALLBACK_SZ] = { {NULL} };
@@ -576,10 +640,12 @@ static int netdevice_event(struct notifi
 		cmds[1] = add_cmd;
 		break;
 
+#ifdef HAVE_NETDEV_CHANGEUPPER
 	case NETDEV_CHANGEUPPER:
 		cmds[0] = upper_ips_del_cmd;
 		cmds[1] = add_cmd;
 		break;
+#endif
 
 	case NETDEV_BONDING_FAILOVER:
 		cmds[0] = bonding_event_ips_del_cmd;
@@ -807,6 +873,10 @@ void __exit roce_gid_mgmt_cleanup(void)
 	 * so no issue with remaining hardware contexts.
 	 */
 	synchronize_rcu();
+#ifdef HAVE_DRAIN_WORKQUEUE
 	drain_workqueue(roce_gid_mgmt_wq);
+#endif
+	/* Old implementaion of destroy_workqueue will drain the queue and
+	 * destroy it. */
 	destroy_workqueue(roce_gid_mgmt_wq);
 }
--- a/drivers/infiniband/core/sa_query.c
+++ b/drivers/infiniband/core/sa_query.c
@@ -42,7 +42,11 @@
 #include <linux/kref.h>
 #include <linux/idr.h>
 #include <linux/workqueue.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 #include <rdma/ib_pack.h>
 #include <rdma/ib_cache.h>
 #include "sa.h"
@@ -630,13 +634,19 @@ static void init_mad(struct ib_sa_mad *m
 		cpu_to_be64(((u64) agent->hi_tid) << 32 | tid++);
 	spin_unlock_irqrestore(&tid_lock, flags);
 }
-
 static int send_mad(struct ib_sa_query *query, int timeout_ms, int retries, gfp_t gfp_mask)
 {
+#ifdef HAVE_IDR_ALLOC
+#ifdef __GFP_WAIT
 	bool preload = !!(gfp_mask & __GFP_WAIT);
+#else
+	bool preload = gfpflags_allow_blocking(gfp_mask);
+#endif
+#endif
 	unsigned long flags;
 	int ret, id;
 
+#ifdef HAVE_IDR_ALLOC
 	if (preload)
 		idr_preload(gfp_mask);
 	spin_lock_irqsave(&idr_lock, flags);
@@ -648,6 +658,18 @@ static int send_mad(struct ib_sa_query *
 		idr_preload_end();
 	if (id < 0)
 		return id;
+#else
+retry:
+	if (!idr_pre_get(&query_idr, gfp_mask))
+		return -ENOMEM;
+	spin_lock_irqsave(&idr_lock, flags);
+	ret = idr_get_new(&query_idr, query, &id);
+	spin_unlock_irqrestore(&idr_lock, flags);
+	if (ret == -EAGAIN)
+		goto retry;
+	if (ret)
+		return ret;
+#endif
 
 	query->mad_buf->timeout_ms  = timeout_ms;
 	query->mad_buf->retries = retries;
--- a/drivers/infiniband/core/sysfs.c
+++ b/drivers/infiniband/core/sysfs.c
@@ -91,8 +91,30 @@ static ssize_t port_attr_show(struct kob
 	return port_attr->show(p, port_attr, buf);
 }
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+static ssize_t port_attr_store(struct kobject *kobj,
+			      struct attribute *attr, const char *buf, size_t count)
+{
+	struct port_attribute *port_attr =
+		container_of(attr, struct port_attribute, attr);
+	struct ib_port *p = container_of(kobj, struct ib_port, kobj);
+
+	if (!port_attr->store)
+		return -EIO;
+
+	return port_attr->store(p, port_attr, buf, count);
+}
+#endif
+
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops port_sysfs_ops = {
-	.show = port_attr_show
+#else
+static struct sysfs_ops port_sysfs_ops = {
+#endif
+	.show = port_attr_show,
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	.store = port_attr_store
+#endif
 };
 
 static ssize_t gid_attr_show(struct kobject *kobj,
@@ -109,7 +131,11 @@ static ssize_t gid_attr_show(struct kobj
 	return port_attr->show(p, port_attr, buf);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops gid_attr_sysfs_ops = {
+#else
+static struct sysfs_ops gid_attr_sysfs_ops = {
+#endif
 	.show = gid_attr_show
 };
 
@@ -190,6 +216,90 @@ static ssize_t sm_sl_show(struct ib_port
 	return sprintf(buf, "%d\n", attr.sm_sl);
 }
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+static ssize_t skprio2up_show(struct ib_port *p, struct port_attribute *unused,
+			  char *buf)
+{
+	int ret = 0;
+	int i;
+	u8 port_num = p->port_num;
+	struct ib_device *ibdev = p->ibdev;
+
+	for (i = 0; i < NUM_SKPRIO; ++i) {
+		int res;
+		u8 up;
+
+		res = ib_get_skprio2up(ibdev, port_num, i, &up);
+		if (res) {
+			pr_err("failed to get skprio2up (%d)\n", res);
+			ret = res;
+			goto out;
+		}
+		res = sprintf(buf + ret, "%d ", up);
+		if (res < 0) {
+			pr_err("failed to copy skprio2up (%d)\n", res);
+			ret = res;
+			goto out;
+		}
+		ret += res;
+	}
+	sprintf(buf + ret -1, "\n");
+out:
+	return ret;
+}
+
+static ssize_t skprio2up_store(struct ib_port *p, struct port_attribute *unused,
+		const char *buf, size_t count)
+{
+	int ret = count;
+	char save;
+	int i = 0;
+	u8 port_num = p->port_num;
+	struct ib_device *ibdev = p->ibdev;
+	u8 map[NUM_SKPRIO];
+
+	do {
+		int len;
+		int new_value;
+
+		if (i >= NUM_SKPRIO) {
+			pr_err("bad number of elemets in skprio2up array\n");
+			goto out;
+		}
+
+		len = strcspn(buf, " ");
+
+		/* nul-terminate and parse */
+		save = buf[len];
+		((char *)buf)[len] = '\0';
+
+		if (sscanf(buf, "%d", &new_value) != 1 ||
+				new_value >= NUM_UP || new_value < 0) {
+			pr_err( "bad user priority: '%s'\n", buf);
+			goto out;
+		}
+		map[i] = new_value;
+
+		buf += len+1;
+		i++;
+	} while (save == ' ');
+
+	if (i != NUM_SKPRIO) {
+		pr_err("bad number of elemets in skprio2up array\n");
+		goto out;
+	}
+	for (i = 0; i < NUM_SKPRIO; ++i) {
+		int res = ib_set_skprio2up(ibdev, port_num, i, map[i]);
+		if (res)
+			return res;
+	}
+	return ret;
+
+out:
+	return -EINVAL;
+}
+#endif
+
 static ssize_t cap_mask_show(struct ib_port *p, struct port_attribute *unused,
 			     char *buf)
 {
@@ -274,6 +384,9 @@ static PORT_ATTR_RO(lid);
 static PORT_ATTR_RO(lid_mask_count);
 static PORT_ATTR_RO(sm_lid);
 static PORT_ATTR_RO(sm_sl);
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+static PORT_ATTR(skprio2up, S_IRUGO | S_IWUSR, skprio2up_show, skprio2up_store);
+#endif
 static PORT_ATTR_RO(cap_mask);
 static PORT_ATTR_RO(rate);
 static PORT_ATTR_RO(phys_state);
@@ -285,6 +398,9 @@ static struct attribute *port_default_at
 	&port_attr_lid_mask_count.attr,
 	&port_attr_sm_lid.attr,
 	&port_attr_sm_sl.attr,
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	&port_attr_skprio2up.attr,
+#endif
 	&port_attr_cap_mask.attr,
 	&port_attr_rate.attr,
 	&port_attr_phys_state.attr,
--- a/drivers/infiniband/core/ucm.c
+++ b/drivers/infiniband/core/ucm.c
@@ -180,6 +180,9 @@ static void ib_ucm_cleanup_events(struct
 static struct ib_ucm_context *ib_ucm_ctx_alloc(struct ib_ucm_file *file)
 {
 	struct ib_ucm_context *ctx;
+#ifndef HAVE_IDR_ALLOC
+	int result;
+#endif
 
 	ctx = kzalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
@@ -190,11 +193,26 @@ static struct ib_ucm_context *ib_ucm_ctx
 	ctx->file = file;
 	INIT_LIST_HEAD(&ctx->events);
 
+#ifdef HAVE_IDR_ALLOC
 	mutex_lock(&ctx_id_mutex);
 	ctx->id = idr_alloc(&ctx_id_table, ctx, 0, 0, GFP_KERNEL);
 	mutex_unlock(&ctx_id_mutex);
 	if (ctx->id < 0)
 		goto error;
+#else
+	do {
+		result = idr_pre_get(&ctx_id_table, GFP_KERNEL);
+		if (!result)
+			goto error;
+
+		mutex_lock(&ctx_id_mutex);
+		result = idr_get_new(&ctx_id_table, ctx, &ctx->id);
+		mutex_unlock(&ctx_id_mutex);
+	} while (result == -EAGAIN);
+
+	if (result)
+		goto error;
+#endif
 
 	list_add_tail(&ctx->file_list, &file->ctxs);
 	return ctx;
@@ -1326,8 +1344,16 @@ static void ib_ucm_remove_one(struct ib_
 	device_unregister(&ucm_dev->dev);
 }
 
+#ifdef HAVE_CLASS_ATTR_STRING
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_CM_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_CM_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static int __init ib_ucm_init(void)
 {
@@ -1340,7 +1366,11 @@ static int __init ib_ucm_init(void)
 		goto error1;
 	}
 
+#ifdef HAVE_CLASS_ATTR_STRING
 	ret = class_create_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(&cm_class, &class_attr_abi_version);
+#endif
 	if (ret) {
 		printk(KERN_ERR "ucm: couldn't create abi_version attribute\n");
 		goto error2;
@@ -1354,7 +1384,11 @@ static int __init ib_ucm_init(void)
 	return 0;
 
 error3:
+#ifdef HAVE_CLASS_ATTR_STRING
 	class_remove_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	class_remove_file(&cm_class, &class_attr_abi_version);
+#endif
 error2:
 	unregister_chrdev_region(IB_UCM_BASE_DEV, IB_UCM_MAX_DEVICES);
 error1:
@@ -1364,7 +1398,11 @@ error1:
 static void __exit ib_ucm_cleanup(void)
 {
 	ib_unregister_client(&ucm_client);
+#ifdef HAVE_CLASS_ATTR_STRING
 	class_remove_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	class_remove_file(&cm_class, &class_attr_abi_version);
+#endif
 	unregister_chrdev_region(IB_UCM_BASE_DEV, IB_UCM_MAX_DEVICES);
 	if (overflow_maj)
 		unregister_chrdev_region(overflow_maj, IB_UCM_MAX_DEVICES);
--- a/drivers/infiniband/core/ucma.c
+++ b/drivers/infiniband/core/ucma.c
@@ -56,6 +56,7 @@ MODULE_LICENSE("Dual BSD/GPL");
 
 static unsigned int max_backlog = 1024;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table_header *ucma_ctl_table_hdr;
 static struct ctl_table ucma_ctl_table[] = {
 	{
@@ -67,6 +68,14 @@ static struct ctl_table ucma_ctl_table[]
 	},
 	{ }
 };
+#ifndef HAVE_REGISTER_NET_SYSCTL
+static struct ctl_path ucma_ctl_path[] = {
+	{ .procname = "net" },
+	{ .procname = "rdma_ucm" },
+	{ }
+};
+#endif
+#endif
 
 struct ucma_file {
 	struct mutex		mut;
@@ -184,6 +193,9 @@ static void ucma_close_id(struct work_st
 static struct ucma_context *ucma_alloc_ctx(struct ucma_file *file)
 {
 	struct ucma_context *ctx;
+#ifndef HAVE_IDR_ALLOC
+	int ret;
+#endif
 
 	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
 	if (!ctx)
@@ -195,11 +207,26 @@ static struct ucma_context *ucma_alloc_c
 	INIT_LIST_HEAD(&ctx->mc_list);
 	ctx->file = file;
 
+#ifndef HAVE_IDR_ALLOC
+	do {
+		ret = idr_pre_get(&ctx_idr, GFP_KERNEL);
+		if (!ret)
+			goto error;
+
+		mutex_lock(&mut);
+		ret = idr_get_new(&ctx_idr, ctx, &ctx->id);
+		mutex_unlock(&mut);
+	} while (ret == -EAGAIN);
+
+	if (ret)
+		goto error;
+#else
 	mutex_lock(&mut);
 	ctx->id = idr_alloc(&ctx_idr, ctx, 0, 0, GFP_KERNEL);
 	mutex_unlock(&mut);
 	if (ctx->id < 0)
 		goto error;
+#endif
 
 	list_add_tail(&ctx->list, &file->ctx_list);
 	return ctx;
@@ -212,16 +239,34 @@ error:
 static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)
 {
 	struct ucma_multicast *mc;
+#ifndef HAVE_IDR_ALLOC
+	int ret;
+#endif
 
 	mc = kzalloc(sizeof(*mc), GFP_KERNEL);
 	if (!mc)
 		return NULL;
 
+#ifndef HAVE_IDR_ALLOC
+	do {
+		ret = idr_pre_get(&multicast_idr, GFP_KERNEL);
+		if (!ret)
+			goto error;
+
+		mutex_lock(&mut);
+		ret = idr_get_new(&multicast_idr, mc, &mc->id);
+		mutex_unlock(&mut);
+	} while (ret == -EAGAIN);
+
+	if (ret)
+		goto error;
+#else
 	mutex_lock(&mut);
 	mc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);
 	mutex_unlock(&mut);
 	if (mc->id < 0)
 		goto error;
+#endif
 
 	mc->ctx = ctx;
 	list_add_tail(&mc->list, &ctx->mc_list);
@@ -1508,7 +1553,11 @@ static ssize_t ucma_migrate_id(struct uc
 	struct rdma_ucm_migrate_id cmd;
 	struct rdma_ucm_migrate_resp resp;
 	struct ucma_context *ctx;
+#ifdef HAVE_FDGET
 	struct fd f;
+#else
+	struct file *filp;
+#endif
 	struct ucma_file *cur_file;
 	int ret = 0;
 
@@ -1516,12 +1565,21 @@ static ssize_t ucma_migrate_id(struct uc
 		return -EFAULT;
 
 	/* Get current fd to protect against it being closed */
+#ifdef HAVE_FDGET
 	f = fdget(cmd.fd);
 	if (!f.file)
+#else
+	filp = fget(cmd.fd);
+	if (!filp)
+#endif
 		return -ENOENT;
 
 	/* Validate current fd and prevent destruction of id. */
+#ifdef HAVE_FDGET
 	ctx = ucma_get_ctx(f.file->private_data, cmd.id);
+#else
+	ctx = ucma_get_ctx(filp->private_data, cmd.id);
+#endif
 	if (IS_ERR(ctx)) {
 		ret = PTR_ERR(ctx);
 		goto file_put;
@@ -1555,7 +1613,11 @@ response:
 
 	ucma_put_ctx(ctx);
 file_put:
+#ifdef HAVE_FDGET
 	fdput(f);
+#else
+	fput(filp);
+#endif
 	return ret;
 }
 
@@ -1732,15 +1794,23 @@ static int __init ucma_init(void)
 		goto err1;
 	}
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table);
+#else
+	ucma_ctl_table_hdr = register_sysctl_paths(ucma_ctl_path, ucma_ctl_table);
+#endif
 	if (!ucma_ctl_table_hdr) {
 		printk(KERN_ERR "rdma_ucm: couldn't register sysctl paths\n");
 		ret = -ENOMEM;
 		goto err2;
 	}
+#endif
 	return 0;
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 err2:
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
+#endif
 err1:
 	misc_deregister(&ucma_misc);
 	return ret;
@@ -1748,7 +1818,13 @@ err1:
 
 static void __exit ucma_cleanup(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	unregister_net_sysctl_table(ucma_ctl_table_hdr);
+#else
+	unregister_sysctl_table(ucma_ctl_table_hdr);
+#endif
+#endif
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
 	misc_deregister(&ucma_misc);
 	idr_destroy(&ctx_idr);
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@ -37,7 +37,9 @@
 #include <linux/sched.h>
 #include <linux/export.h>
 #include <linux/hugetlb.h>
+#ifdef HAVE_STRUCT_DMA_ATTRS
 #include <linux/dma-attrs.h>
+#endif
 #include <linux/slab.h>
 #include <rdma/ib_umem_odp.h>
 
@@ -61,7 +63,11 @@ static void umem_vma_open(struct vm_area
 	with mm->mmap_sem held for writing.
 	*/
 	if (current->mm)
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm += ntotal_pages;
+#else
+		current->mm->locked_vm += ntotal_pages;
+#endif
 	return;
 }
 
@@ -81,7 +87,11 @@ static void umem_vma_close(struct vm_are
 	with mm->mmap_sem held for writing.
 	*/
 	if (current->mm)
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= ntotal_pages;
+#else
+		current->mm->locked_vm -= ntotal_pages;
+#endif
 	return;
 
 }
@@ -116,7 +126,11 @@ int ib_umem_map_to_vma(struct ib_umem *u
 	with mm->mmap_sem held for writing.
 	No need to lock.
 	*/
+#ifdef HAVE_PINNED_VM
 	locked = ntotal_pages + current->mm->pinned_vm;
+#else
+	locked = ntotal_pages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK))
@@ -145,7 +159,11 @@ int ib_umem_map_to_vma(struct ib_umem *u
 end:
 	/* We expect to have enough pages   */
 	if (vma_entry_number >= ntotal_pages) {
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm = locked;
+#else
+		current->mm->locked_vm = locked;
+#endif
 		vma->vm_ops =  &umem_vm_ops;
 		return 0;
 	}
@@ -186,7 +204,11 @@ static void ib_cmem_release(struct kref
 	counter not relevant any more.*/
 	if (current->mm) {
 		ntotal_pages = PAGE_ALIGN(cmem->length) >> PAGE_SHIFT;
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= ntotal_pages;
+#else
+		current->mm->locked_vm -= ntotal_pages;
+#endif
 	}
 	kfree(cmem);
 
@@ -347,7 +369,11 @@ struct ib_cmem *ib_cmem_alloc_contiguous
 	with mm->mmap_sem held for writing.
 	No need to lock
 	 */
+#ifdef HAVE_PINNED_VM
 	locked     = ntotal_pages + current->mm->pinned_vm;
+#else
+	locked     = ntotal_pages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK))
@@ -397,7 +423,11 @@ struct ib_cmem *ib_cmem_alloc_contiguous
 
 	cmem->length = total_size;
 
+#ifdef HAVE_PINNED_VM
 	current->mm->pinned_vm = locked;
+#else
+	current->mm->locked_vm = locked;
+#endif
 	return cmem;
 
 err_alloc:
@@ -582,6 +612,133 @@ void ib_umem_activate_invalidation_notif
 }
 EXPORT_SYMBOL(ib_umem_activate_invalidation_notifier);
 
+#if !defined(ARCH_HAS_SG_CHAIN) && defined(__AARCH64EL__) && LINUX_VERSION_CODE <= KERNEL_VERSION(3, 16, 0)
+static void arm_sg_kfree(struct scatterlist *sg, unsigned int nents)
+{
+	if (nents == SG_MAX_SINGLE_ALLOC) {
+		kmemleak_free(sg);
+		free_page((unsigned long) sg);
+	} else {
+		kfree(sg);
+	}
+}
+
+static inline void arm_sg_chain(struct scatterlist *prv, unsigned int prv_nents,
+				struct scatterlist *sgl)
+{
+	/*
+	 * offset and length are unused for chain entry.  Clear them.
+	 */
+	prv[prv_nents - 1].offset = 0;
+	prv[prv_nents - 1].length = 0;
+
+	/*
+	 * Set lowest bit to indicate a link pointer, and make sure to clear
+	 * the termination bit if it happens to be set.
+	 */
+	prv[prv_nents - 1].page_link = ((unsigned long) sgl | 0x01) & ~0x02;
+}
+
+static struct scatterlist *arm_sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
+{
+	if (nents == SG_MAX_SINGLE_ALLOC) {
+		/*
+		 * Kmemleak doesn't track page allocations as they are not
+		 * commonly used (in a raw form) for kernel data structures.
+		 * As we chain together a list of pages and then a normal
+		 * kmalloc (tracked by kmemleak), in order to for that last
+		 * allocation not to become decoupled (and thus a
+		 * false-positive) we need to inform kmemleak of all the
+		 * intermediate allocations.
+		 */
+		void *ptr = (void *)__get_free_page(gfp_mask);
+		kmemleak_alloc(ptr, PAGE_SIZE, 1, gfp_mask);
+		return ptr;
+	} else {
+		return kmalloc(nents * sizeof(struct scatterlist), gfp_mask);
+	}
+}
+
+int __arm_sg_alloc_table(struct sg_table *table, unsigned int nents,
+		     unsigned int max_ents, gfp_t gfp_mask,
+		     sg_alloc_fn *alloc_fn)
+{
+	struct scatterlist *sg, *prv;
+	unsigned int left;
+
+	memset(table, 0, sizeof(*table));
+
+	if (nents == 0)
+		return -EINVAL;
+
+	left = nents;
+	prv = NULL;
+	do {
+		unsigned int sg_size, alloc_size = left;
+
+		if (alloc_size > max_ents) {
+			alloc_size = max_ents;
+			sg_size = alloc_size - 1;
+		} else {
+			sg_size = alloc_size;
+		}
+
+		left -= sg_size;
+
+		sg = alloc_fn(alloc_size, gfp_mask);
+
+		if (unlikely(!sg)) {
+			/*
+			 * Adjust entry count to reflect that the last
+			 * entry of the previous table won't be used for
+			 * linkage.  Without this, sg_kfree() may get
+			 * confused.
+			 */
+			if (prv)
+				table->nents = ++table->orig_nents;
+
+			return -ENOMEM;
+		}
+		sg_init_table(sg, alloc_size);
+		table->nents = table->orig_nents += sg_size;
+
+		/*
+		 * If this is the first mapping, assign the sg table header.
+		 * If this is not the first mapping, chain previous part.
+		 */
+		if (prv)
+			arm_sg_chain(prv, max_ents, sg);
+		else
+			table->sgl = sg;
+
+		/*
+		 * If no more entries after this one, mark the end
+		 */
+		if (!left)
+			sg_mark_end(&sg[sg_size - 1]);
+
+		prv = sg;
+	} while (left);
+
+	return 0;
+}
+EXPORT_SYMBOL(__arm_sg_alloc_table);
+
+int arm_sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
+{
+	int ret;
+
+	ret = __arm_sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
+			       gfp_mask, arm_sg_kmalloc);
+	if (unlikely(ret))
+		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, arm_sg_kfree);
+
+	return ret;
+}
+EXPORT_SYMBOL(arm_sg_alloc_table);
+
+#endif
+
 /**
  * ib_umem_get - Pin and DMA map userspace memory.
  *
@@ -607,12 +764,20 @@ struct ib_umem *ib_umem_get_ex(struct ib
 	unsigned long npages;
 	int ret;
 	int i;
+#ifdef HAVE_STRUCT_DMA_ATTRS
 	DEFINE_DMA_ATTRS(attrs);
+#else
+	unsigned long dma_attrs = 0;
+#endif
 	struct scatterlist *sg, *sg_list_start;
 	int need_release = 0;
 
 	if (dmasync)
+#ifdef HAVE_STRUCT_DMA_ATTRS
 		dma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);
+#else
+		dma_attrs |= DMA_ATTR_WRITE_BARRIER;
+#endif
 
 	if (!size)
 		return ERR_PTR(-EINVAL);
@@ -636,7 +801,9 @@ struct ib_umem *ib_umem_get_ex(struct ib
 	umem->length    = size;
 	umem->address   = addr;
 	umem->page_size = PAGE_SIZE;
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	umem->pid       = get_task_pid(current, PIDTYPE_PID);
+#endif
 	/*
 	 * We ask for writable memory if any of the following
 	 * access flags are set.  "Local write" and "remote write"
@@ -692,7 +859,11 @@ struct ib_umem *ib_umem_get_ex(struct ib
 
 	down_write(&current->mm->mmap_sem);
 
+#ifdef HAVE_PINNED_VM
 	locked     = npages + current->mm->pinned_vm;
+#else
+	locked     = npages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {
@@ -707,7 +878,12 @@ struct ib_umem *ib_umem_get_ex(struct ib
 		goto out;
 	}
 
+
+#if !defined(ARCH_HAS_SG_CHAIN) && defined(__AARCH64EL__) && LINUX_VERSION_CODE <= KERNEL_VERSION(3, 16, 0)
+	ret = arm_sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL);
+#else
 	ret = sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL);
+#endif
 	if (ret)
 		goto out;
 
@@ -742,7 +918,11 @@ struct ib_umem *ib_umem_get_ex(struct ib
 				  umem->sg_head.sgl,
 				  umem->npages,
 				  DMA_BIDIRECTIONAL,
+#ifdef HAVE_STRUCT_DMA_ATTRS
 				  &attrs);
+#else
+				  dma_attrs);
+#endif
 
 	if (umem->nmap <= 0) {
 		ret = -ENOMEM;
@@ -755,10 +935,16 @@ out:
 	if (ret < 0) {
 		if (need_release)
 			__ib_umem_release(context->device, umem, 0);
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 		put_pid(umem->pid);
+#endif
 		kfree(umem);
 	} else
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm = locked;
+#else
+		current->mm->locked_vm = locked;
+#endif
 
 	up_write(&current->mm->mmap_sem);
 	if (vma_list)
@@ -781,7 +967,11 @@ static void ib_umem_account(struct work_
 	struct ib_umem *umem = container_of(work, struct ib_umem, work);
 
 	down_write(&umem->mm->mmap_sem);
+#ifdef HAVE_PINNED_VM
 	umem->mm->pinned_vm -= umem->diff;
+#else
+	umem->mm->locked_vm -= umem->diff;
+#endif
 	up_write(&umem->mm->mmap_sem);
 	mmput(umem->mm);
 	kfree(umem);
@@ -795,7 +985,9 @@ void ib_umem_release(struct ib_umem *ume
 {
 	struct ib_ucontext *context = umem->context;
 	struct mm_struct *mm;
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	struct task_struct *task;
+#endif
 	unsigned long diff;
 
 	if (umem->ib_peer_mem) {
@@ -810,6 +1002,7 @@ void ib_umem_release(struct ib_umem *ume
 
 	__ib_umem_release(umem->context->device, umem, 1);
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	task = get_pid_task(umem->pid, PIDTYPE_PID);
 	put_pid(umem->pid);
 	if (!task)
@@ -818,6 +1011,13 @@ void ib_umem_release(struct ib_umem *ume
 	put_task_struct(task);
 	if (!mm)
 		goto out;
+#else
+	mm = get_task_mm(current);
+	if (!mm) {
+		kfree(umem);
+		return;
+	}
+#endif
 
 	diff = ib_umem_num_pages(umem);
 
@@ -841,10 +1041,24 @@ void ib_umem_release(struct ib_umem *ume
 	} else
 		down_write(&mm->mmap_sem);
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
+#ifdef HAVE_PINNED_VM
 	mm->pinned_vm -= diff;
+#else
+	mm->locked_vm -= diff;
+#endif
+#else
+#ifdef HAVE_PINNED_VM
+	current->mm->pinned_vm -= diff;
+#else
+	current->mm->locked_vm -= diff;
+#endif
+#endif
 	up_write(&mm->mmap_sem);
 	mmput(mm);
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 out:
+#endif
 	kfree(umem);
 }
 EXPORT_SYMBOL(ib_umem_release);
--- a/drivers/infiniband/core/user_mad.c
+++ b/drivers/infiniband/core/user_mad.c
@@ -33,6 +33,9 @@
  * SOFTWARE.
  */
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) "user_mad: " fmt
 
 #include <linux/module.h>
@@ -1248,8 +1251,16 @@ static ssize_t show_port(struct device *
 }
 static DEVICE_ATTR(port, S_IRUGO, show_port, NULL);
 
+#ifdef HAVE_CLASS_ATTR_STRING
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_MAD_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_MAD_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UMAD_MAX_PORTS);
@@ -1459,7 +1470,11 @@ static void ib_umad_remove_one(struct ib
 	kobject_put(&umad_dev->kobj);
 }
 
+#ifdef HAVE_CLASS_DEVNODE_UMODE_T
 static char *umad_devnode(struct device *dev, umode_t *mode)
+#else
+static char *umad_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	return kasprintf(GFP_KERNEL, "infiniband/%s", dev_name(dev));
 }
@@ -1484,7 +1499,11 @@ static int __init ib_umad_init(void)
 
 	umad_class->devnode = umad_devnode;
 
+#ifdef HAVE_CLASS_ATTR_STRING
 	ret = class_create_file(umad_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(umad_class, &class_attr_abi_version);
+#endif
 	if (ret) {
 		pr_err("couldn't create abi_version attribute\n");
 		goto out_class;
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@ -165,6 +165,7 @@ static int idr_add_uobj(struct idr *idr,
 {
 	int ret;
 
+#ifdef HAVE_IDR_ALLOC
 	idr_preload(GFP_KERNEL);
 	spin_lock(&ib_uverbs_idr_lock);
 
@@ -176,6 +177,20 @@ static int idr_add_uobj(struct idr *idr,
 	idr_preload_end();
 
 	return ret < 0 ? ret : 0;
+#else
+retry:
+	if (!idr_pre_get(idr, GFP_KERNEL))
+		return -ENOMEM;
+
+	spin_lock(&ib_uverbs_idr_lock);
+	ret = idr_get_new(idr, uobj, &uobj->id);
+	spin_unlock(&ib_uverbs_idr_lock);
+
+	if (ret == -EAGAIN)
+		goto retry;
+
+	return ret;
+#endif
 }
 
 void idr_remove_uobj(struct idr *idr, struct ib_uobject *uobj)
@@ -419,9 +434,11 @@ ssize_t ib_uverbs_get_context(struct ib_
 	INIT_LIST_HEAD(&ucontext->xrcd_list);
 	INIT_LIST_HEAD(&ucontext->rule_list);
 	INIT_LIST_HEAD(&ucontext->dct_list);
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	rcu_read_lock();
 	ucontext->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);
 	rcu_read_unlock();
+#endif
 	ucontext->closing = 0;
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
@@ -831,7 +848,11 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 	struct ib_udata			udata;
 	struct ib_uxrcd_object         *obj;
 	struct ib_xrcd                 *xrcd = NULL;
+#ifdef HAVE_FDGET
 	struct fd			f = {NULL, 0};
+#else
+	struct file                    *f = NULL;
+#endif
 	struct inode                   *inode = NULL;
 	int				ret = 0;
 	int				new_xrcd = 0;
@@ -850,6 +871,7 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 
 	if (cmd.fd != -1) {
 		/* search for file descriptor */
+#ifdef HAVE_FDGET
 		f = fdget(cmd.fd);
 		if (!f.file) {
 			ret = -EBADF;
@@ -857,6 +879,19 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 		}
 
 		inode = file_inode(f.file);
+#else
+		f = fget(cmd.fd);
+		if (!f) {
+			ret = -EBADF;
+			goto err_tree_mutex_unlock;
+		}
+
+		inode = f->f_dentry->d_inode;
+		if (!inode) {
+			ret = -EBADF;
+			goto err_tree_mutex_unlock;
+		}
+#endif
 		xrcd = find_xrcd(file->device, inode);
 		if (!xrcd && !(cmd.oflags & O_CREAT)) {
 			/* no file descriptor. Need CREATE flag */
@@ -921,8 +956,13 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 		goto err_copy;
 	}
 
+#ifdef HAVE_FDGET
 	if (f.file)
 		fdput(f);
+#else
+	if (f)
+		fput(f);
+#endif
 
 	mutex_lock(&file->mutex);
 	list_add_tail(&obj->uobject.list, &file->ucontext->xrcd_list);
@@ -951,8 +991,13 @@ err:
 	put_uobj_write(&obj->uobject);
 
 err_tree_mutex_unlock:
+#ifdef HAVE_FDGET
 	if (f.file)
 		fdput(f);
+#else
+	if (f)
+		fput(f);
+#endif
 
 	mutex_unlock(&file->device->xrcd_tree_mutex);
 
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@ -409,7 +409,9 @@ static int ib_uverbs_cleanup_ucontext(st
 		kfree(uobj);
 	}
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	put_pid(context->tgid);
+#endif
 
 	return context->device->dealloc_ucontext(context);
 }
@@ -727,6 +729,7 @@ struct file *ib_uverbs_alloc_event_file(
 struct ib_uverbs_event_file *ib_uverbs_lookup_comp_file(int fd)
 {
 	struct ib_uverbs_event_file *ev_file = NULL;
+#ifdef HAVE_FDGET
 	struct fd f = fdget(fd);
 
 	if (!f.file)
@@ -746,6 +749,29 @@ struct ib_uverbs_event_file *ib_uverbs_l
 out:
 	fdput(f);
 	return ev_file;
+#else
+	struct file *filp;
+	int fput_needed;
+
+	filp = fget_light(fd, &fput_needed);
+	if (!filp)
+		return NULL;
+
+	if (filp->f_op != &uverbs_event_fops)
+		goto out;
+
+	ev_file = filp->private_data;
+	if (ev_file->is_async) {
+		ev_file = NULL;
+		goto out;
+	}
+
+	kref_get(&ev_file->ref);
+
+out:
+	fput_light(filp, fput_needed);
+	return ev_file;
+#endif
 }
 
 enum {
@@ -1182,8 +1208,16 @@ static ssize_t show_dev_abi_version(stru
 }
 static DEVICE_ATTR(abi_version, S_IRUGO, show_dev_abi_version, NULL);
 
+#ifdef HAVE_CLASS_ATTR_STRING
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_VERBS_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_VERBS_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UVERBS_MAX_DEVICES);
@@ -1403,7 +1437,11 @@ static void ib_uverbs_remove_one(struct
 	}
 }
 
+#ifdef HAVE_CLASS_DEVNODE_UMODE_T
 static char *uverbs_devnode(struct device *dev, umode_t *mode)
+#else
+static char *uverbs_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	if (mode)
 		*mode = 0666;
@@ -1430,7 +1468,11 @@ static int __init ib_uverbs_init(void)
 
 	uverbs_class->devnode = uverbs_devnode;
 
+#ifdef HAVE_CLASS_ATTR_STRING
 	ret = class_create_file(uverbs_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(uverbs_class, &class_attr_abi_version);
+#endif
 	if (ret) {
 		printk(KERN_ERR "user_verbs: couldn't create abi_version attribute\n");
 		goto out_class;
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@ -61,6 +61,16 @@
 #include "user.h"
 #include "mlx4_exp.h"
 
+#ifdef DRV_NAME
+#undef DRV_NAME
+#endif
+#ifdef DRV_VERSION
+#undef DRV_VERSION
+#endif
+#ifdef DRV_RELDATE
+#undef DRV_RELDATE
+#endif
+
 #define DRV_NAME	MLX4_IB_DRV_NAME
 #define DRV_VERSION	"3.4-1.0.0"
 #define DRV_RELDATE	"25 Sep 2016"
@@ -865,6 +875,7 @@ static int mlx4_ib_dealloc_ucontext(stru
 	return 0;
 }
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 static void  mlx4_ib_vma_open(struct vm_area_struct *area)
 {
 	/* vma_open is called when a new VMA is created on top of our VMA.
@@ -990,6 +1001,7 @@ static void mlx4_ib_set_vma_data(struct
 	vma->vm_private_data = vma_private_data;
 	vma->vm_ops =  &mlx4_ib_vm_ops;
 }
+#endif
 
 static unsigned long mlx4_ib_get_unmapped_area(struct file *file,
 			unsigned long addr,
@@ -1079,7 +1091,9 @@ static int mlx4_ib_mmap(struct ib_uconte
 				       PAGE_SIZE, vma->vm_page_prot))
 			return -EAGAIN;
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 		mlx4_ib_set_vma_data(vma, &mucontext->hw_bar_info[HW_BAR_DB]);
+#endif
 
 	} else if (command == MLX4_IB_MMAP_BLUE_FLAME_PAGE &&
 			dev->dev->caps.bf_reg_size != 0) {
@@ -1095,7 +1109,9 @@ static int mlx4_ib_mmap(struct ib_uconte
 				       PAGE_SIZE, vma->vm_page_prot))
 			return -EAGAIN;
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 		mlx4_ib_set_vma_data(vma, &mucontext->hw_bar_info[HW_BAR_BF]);
+#endif
 
 	} else if (command == MLX4_IB_MMAP_GET_HW_CLOCK) {
 		struct mlx4_clock_params params;
@@ -1118,7 +1134,9 @@ static int mlx4_ib_mmap(struct ib_uconte
 				       PAGE_SIZE, vma->vm_page_prot))
 			return -EAGAIN;
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 		mlx4_ib_set_vma_data(vma, &mucontext->hw_bar_info[HW_BAR_CLOCK]);
+#endif
 
 	} else if (command == MLX4_IB_MMAP_GET_CONTIGUOUS_PAGES ||
 		   command == MLX4_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_CPU_NUMA ||
@@ -1184,7 +1202,11 @@ static int mlx4_ib_mmap(struct ib_uconte
 			return -EAGAIN;
 		}
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 		mlx4_ib_set_vma_data(vma, &uar->hw_bar_info[HW_BAR_DB]);
+#else
+		uar->uar_virt_addr = vma->vm_start;
+#endif
 		mutex_lock(&mucontext->user_uar_mutex);
 		list_add(&uar->list, &mucontext->user_uar_list);
 		mutex_unlock(&mucontext->user_uar_mutex);
@@ -1219,7 +1241,9 @@ static int mlx4_ib_mmap(struct ib_uconte
 				       PAGE_SIZE, vma->vm_page_prot))
 			return -EAGAIN;
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 		mlx4_ib_set_vma_data(vma, &uar->hw_bar_info[HW_BAR_BF]);
+#endif
 	} else
 		return -EINVAL;
 
@@ -2068,7 +2092,11 @@ static int del_gid_entry(struct ib_qp *i
 		rdma_get_mcast_mac((struct in6_addr *)gid, mac);
 		if (ndev) {
 			rtnl_lock();
+#ifdef HAVE_DEV_MC_DEL
 			dev_mc_del(ndev, mac);
+#else
+			dev_mc_delete(ndev, mac, 6, 0);
+#endif
 			rtnl_unlock();
 			dev_put(ndev);
 		}
@@ -2322,6 +2350,7 @@ static struct net_device *mlx4_ib_get_ne
 {
 	struct mlx4_ib_dev *ibdev = to_mdev(device);
 
+#ifdef HAVE_BONDING_H
 	if (mlx4_is_bonded(ibdev->dev)) {
 		struct net_device *dev;
 		struct net_device *upper = NULL;
@@ -2345,6 +2374,7 @@ unlock:
 
 		return dev;
 	}
+#endif
 
 	return mlx4_get_protocol_dev(ibdev->dev, MLX4_PROT_ETH, port_num);
 }
@@ -3020,7 +3050,9 @@ static void *mlx4_ib_add(struct mlx4_dev
 	ibdev->ib_dev.process_mad	= mlx4_ib_process_mad;
 	ibdev->ib_dev.get_netdev	= mlx4_ib_get_netdev;
 	ibdev->ib_dev.modify_gid	= mlx4_ib_modify_gid;
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	ibdev->ib_dev.disassociate_ucontext = mlx4_ib_disassociate_ucontext;
+#endif
 
 	if (!mlx4_is_slave(ibdev->dev)) {
 		ibdev->ib_dev.alloc_fmr		= mlx4_ib_fmr_alloc;
--- a/drivers/infiniband/hw/mlx4/mlx4_ib.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib.h
@@ -105,6 +105,9 @@ struct mlx4_ib_user_uar {
 	struct mlx4_ib_vma_private_data hw_bar_info[HW_BAR_COUNT];
 	struct mlx4_uar		uar;
 	int			user_idx;
+#if !defined(HAVE_PUT_TASK_STRUCT_EXPORTED) || !defined (HAVE_GET_TASK_PID_EXPORTED) || !defined(HAVE_GET_PID_TASK_EXPORTED)
+	unsigned long		uar_virt_addr;
+#endif
 	struct list_head	list;
 };
 
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@ -76,8 +76,14 @@ static ssize_t shared_mr_proc_write(stru
 static int shared_mr_mmap(struct file *filep, struct vm_area_struct *vma)
 {
 
+#ifdef HAVE_PDE_DATA
 	struct mlx4_shared_mr_info *smr_info =
 		(struct mlx4_shared_mr_info *)PDE_DATA(filep->f_path.dentry->d_inode);
+#else
+	struct proc_dir_entry *pde = PDE(filep->f_path.dentry->d_inode);
+	struct mlx4_shared_mr_info *smr_info =
+		(struct mlx4_shared_mr_info *)pde->data;
+#endif
 
 	/* Prevent any mapping not on start of area */
 	if (vma->vm_pgoff != 0)
@@ -449,8 +455,10 @@ static int prepare_shared_mr(struct mlx4
 	struct proc_dir_entry *mr_proc_entry;
 	mode_t mode = S_IFREG;
 	char name_buff[128];
+#ifdef HAVE_PROC_SET_USER
 	kuid_t uid;
 	kgid_t gid;
+#endif
 
 	mode |= convert_shared_access(access_flags);
 	sprintf(name_buff, "%X", mr_id);
@@ -469,9 +477,14 @@ static int prepare_shared_mr(struct mlx4
 		return -ENODEV;
 	}
 
+#ifdef HAVE_PROC_SET_USER
 	current_uid_gid(&uid, &gid);
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	/* now creating an extra entry having a uniqe suffix counter */
 	mr->smr_info->counter = atomic64_inc_return(&shared_mr_count);
@@ -487,8 +500,13 @@ static int prepare_shared_mr(struct mlx4
 	}
 
 	mr->smr_info->counter_used = 1;
+#ifdef HAVE_PROC_SET_USER
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	return 0;
 
--- a/drivers/infiniband/hw/mlx4/qp.c
+++ b/drivers/infiniband/hw/mlx4/qp.c
@@ -983,8 +983,12 @@ static struct mlx4_uar *find_user_uar(st
 
 	mutex_lock(&uctx->user_uar_mutex);
 	list_for_each_entry(uar, &uctx->user_uar_list, list)
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 		if (uar->hw_bar_info[HW_BAR_DB].vma &&
 		    uar->hw_bar_info[HW_BAR_DB].vma->vm_start == uar_virt_add) {
+#else
+		if (uar->uar_virt_addr == uar_virt_add) {
+#endif
 			mutex_unlock(&uctx->user_uar_mutex);
 			return &uar->uar;
 		}
--- a/drivers/infiniband/hw/qib/qib_user_pages.c
+++ b/drivers/infiniband/hw/qib/qib_user_pages.c
@@ -73,8 +73,11 @@ static int __qib_get_user_pages(unsigned
 		if (ret < 0)
 			goto bail_release;
 	}
-
+#ifdef HAVE_PINNED_VM
 	current->mm->pinned_vm += num_pages;
+#else
+	current->mm->locked_vm += num_pages;
+#endif
 
 	ret = 0;
 	goto bail;
@@ -151,7 +154,11 @@ void qib_release_user_pages(struct page
 	__qib_release_user_pages(p, num_pages, 1);
 
 	if (current->mm) {
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= num_pages;
+#else
+		current->mm->locked_vm -= num_pages;
+#endif
 		up_write(&current->mm->mmap_sem);
 	}
 }
--- a/include/rdma/ib_addr.h
+++ b/include/rdma/ib_addr.h
@@ -243,17 +243,29 @@ static inline enum ib_mtu iboe_get_mtu(i
 
 static inline int iboe_get_rate(struct net_device *dev)
 {
+#ifndef HAVE___ETHTOOL_GET_LINK_KSETTINGS
 	struct ethtool_cmd cmd;
+#else
+	struct ethtool_link_ksettings cmd;
+#endif
 	u32 speed;
 	int err;
 
 	rtnl_lock();
+#ifndef HAVE___ETHTOOL_GET_LINK_KSETTINGS
 	err = __ethtool_get_settings(dev, &cmd);
+#else
+	err = __ethtool_get_link_ksettings(dev, &cmd);
+#endif
 	rtnl_unlock();
 	if (err)
 		return IB_RATE_PORT_CURRENT;
 
+#ifndef HAVE___ETHTOOL_GET_LINK_KSETTINGS
 	speed = ethtool_cmd_speed(&cmd);
+#else
+	speed = cmd.base.speed;
+#endif
 	if (speed >= 40000)
 		return IB_RATE_40_GBPS;
 	else if (speed >= 30000)
--- a/include/rdma/ib_pack.h
+++ b/include/rdma/ib_pack.h
@@ -34,7 +34,11 @@
 #define IB_PACK_H
 
 #include <rdma/ib_verbs.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 
 enum {
 	IB_LRH_BYTES  = 8,
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -65,7 +65,9 @@ struct ib_umem {
 	int                     writable;
 	int                     hugetlb;
 	struct work_struct	work;
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	struct pid             *pid;
+#endif
 	struct mm_struct       *mm;
 	unsigned long		diff;
 	struct ib_umem_odp     *odp_data;
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@ -35,12 +35,14 @@
 
 #include <rdma/ib_umem.h>
 #include <rdma/ib_verbs.h>
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 #include <linux/interval_tree.h>
 
 struct umem_odp_node {
 	u64 __subtree_last;
 	struct rb_node rb;
 };
+#endif
 
 struct ib_umem_odp {
 	/*
@@ -72,6 +74,7 @@ struct ib_umem_odp {
 	/* A linked list of umems that don't have private mmu notifier
 	 * counters yet. */
 	struct list_head no_private_counters;
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct ib_umem		*umem;
 
 	/* Tree tracking */
@@ -79,6 +82,7 @@ struct ib_umem_odp {
 
 	struct completion	notifier_completion;
 	int			dying;
+#endif
 };
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -39,6 +39,11 @@
 #if !defined(IB_VERBS_H)
 #define IB_VERBS_H
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
+#define pr_fmt(fmt) fmt
+
 #include <linux/types.h>
 #include <linux/device.h>
 #include <linux/mm.h>
@@ -1534,7 +1539,9 @@ struct ib_cq {
 	enum ib_poll_context	poll_ctx;
 	struct ib_wc		*wc;
 	union {
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 		struct irq_poll		iop;
+#endif
 		struct work_struct	work;
 	};
 };
@@ -2380,6 +2387,15 @@ struct ib_device {
 						   u32 flags);
 
 	u64			uverbs_exp_cmd_mask;
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+#define NUM_SKPRIO 16
+#define NUM_UP	   8
+#define MAX_PORTS  2
+	struct {
+		u8  map[MAX_PORTS][NUM_SKPRIO];
+		struct mutex lock;
+	} skprio2up;
+#endif
 };
 
 struct ib_client {
@@ -2917,7 +2933,11 @@ static inline void ib_dma_unmap_single(s
 static inline u64 ib_dma_map_single_attrs(struct ib_device *dev,
 					  void *cpu_addr, size_t size,
 					  enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
 					  struct dma_attrs *attrs)
+#else
+					  unsigned long attrs)
+#endif
 {
 	return dma_map_single_attrs(dev->dma_device, cpu_addr, size,
 				    direction, attrs);
@@ -2926,7 +2946,11 @@ static inline u64 ib_dma_map_single_attr
 static inline void ib_dma_unmap_single_attrs(struct ib_device *dev,
 					     u64 addr, size_t size,
 					     enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
 					     struct dma_attrs *attrs)
+#else
+					     unsigned long attrs)
+#endif
 {
 	return dma_unmap_single_attrs(dev->dma_device, addr, size,
 				      direction, attrs);
@@ -3004,7 +3028,11 @@ static inline void ib_dma_unmap_sg(struc
 static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
 				      struct scatterlist *sg, int nents,
 				      enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
 				      struct dma_attrs *attrs)
+#else
+				      unsigned long attrs)
+#endif
 {
 	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction, attrs);
 }
@@ -3012,7 +3040,11 @@ static inline int ib_dma_map_sg_attrs(st
 static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 					 struct scatterlist *sg, int nents,
 					 enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
 					 struct dma_attrs *attrs)
+#else
+					 unsigned long attrs)
+#endif
 {
 	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, attrs);
 }
@@ -3626,4 +3658,12 @@ void ib_drain_rq(struct ib_qp *qp);
 void ib_drain_sq(struct ib_qp *qp);
 void ib_drain_qp(struct ib_qp *qp);
 
+
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+int ib_set_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 up);
+
+int ib_get_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 *up);
+#endif
 #endif /* IB_VERBS_H */
--- a/include/rdma/peer_mem.h
+++ b/include/rdma/peer_mem.h
@@ -35,9 +35,11 @@
 
 #include <linux/module.h>
 #include <linux/init.h>
+#include <linux/device.h>
 #include <linux/slab.h>
 #include <linux/errno.h>
 #include <linux/export.h>
+#include <linux/scatterlist.h>
 
 
 #define IB_PEER_MEMORY_NAME_MAX 64
